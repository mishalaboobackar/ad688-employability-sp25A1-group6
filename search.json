[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies supervised ML models to the employability dataset.\nWe evaluate baseline, linear, and ensemble methods, and compare performance."
  },
  {
    "objectID": "ml_methods.html#loading-the-dataset-and-initial-inspection",
    "href": "ml_methods.html#loading-the-dataset-and-initial-inspection",
    "title": "Machine Learning Methods",
    "section": "Loading the Dataset and Initial Inspection",
    "text": "Loading the Dataset and Initial Inspection\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n    f1_score,\n    mean_squared_error,\n    r2_score,\n)\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n\n\n\n\nCode\n# Step 1 — Load Dataset and Initial Inspection\nimport pandas as pd\ndf = pd.read_csv(\"data/data_science_job_posts_2025.csv\")\ndf.columns\n\n\nIndex(['job_title', 'seniority_level', 'status', 'company', 'location',\n       'post_date', 'headquarter', 'industry', 'ownership', 'company_size',\n       'revenue', 'salary', 'skills'],\n      dtype='object')\n\n\n\n\nCode\ndf.info()\ndf.isna().mean().sort_values(ascending=False)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 944 entries, 0 to 943\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   job_title        941 non-null    object\n 1   seniority_level  884 non-null    object\n 2   status           688 non-null    object\n 3   company          944 non-null    object\n 4   location         942 non-null    object\n 5   post_date        944 non-null    object\n 6   headquarter      944 non-null    object\n 7   industry         944 non-null    object\n 8   ownership        897 non-null    object\n 9   company_size     944 non-null    object\n 10  revenue          929 non-null    object\n 11  salary           944 non-null    object\n 12  skills           944 non-null    object\ndtypes: object(13)\nmemory usage: 96.0+ KB\n\n\nstatus             0.271186\nseniority_level    0.063559\nownership          0.049788\nrevenue            0.015890\njob_title          0.003178\nlocation           0.002119\ncompany            0.000000\nheadquarter        0.000000\npost_date          0.000000\nindustry           0.000000\ncompany_size       0.000000\nsalary             0.000000\nskills             0.000000\ndtype: float64\n\n\n\n\nCode\ndf.head(10)\n\n\n\n\n\n\n\n\n\njob_title\nseniority_level\nstatus\ncompany\nlocation\npost_date\nheadquarter\nindustry\nownership\ncompany_size\nrevenue\nsalary\nskills\n\n\n\n\n0\ndata scientist\nsenior\nhybrid\ncompany_003\nGrapevine, TX . Hybrid\n17 days ago\nBentonville, AR, US\nRetail\nPublic\n€352.44B\nPublic\n€100,472 - €200,938\n['spark', 'r', 'python', 'scala', 'machine lea...\n\n\n1\ndata scientist\nlead\nhybrid\ncompany_005\nFort Worth, TX . Hybrid\n15 days ago\nDetroit, MI, US\nManufacturing\nPublic\n155,030\n€51.10B\n€118,733\n['spark', 'r', 'python', 'sql', 'machine learn...\n\n\n2\ndata scientist\nsenior\non-site\ncompany_007\nAustin, TX . Toronto, Ontario, Canada . Kirkla...\na month ago\nRedwood City, CA, US\nTechnology\nPublic\n25,930\n€33.80B\n€94,987 - €159,559\n['aws', 'git', 'python', 'docker', 'sql', 'mac...\n\n\n3\ndata scientist\nsenior\nhybrid\ncompany_008\nChicago, IL . Scottsdale, AZ . Austin, TX . Hy...\n8 days ago\nSan Jose, CA, US\nTechnology\nPublic\n34,690\n€81.71B\n€112,797 - €194,402\n['sql', 'r', 'python']\n\n\n4\ndata scientist\nNaN\non-site\ncompany_009\nOn-site\n3 days ago\nStamford, CT, US\nFinance\nPrivate\n1,800\nPrivate\n€114,172 - €228,337\n[]\n\n\n5\ndata scientist\nlead\nNaN\ncompany_013\nNew York, NY\n3 months ago\nNew York, NY, US\nTechnology\nPrivate\n150\n€2.16B\n€196,371 - €251,170\n['scikit-learn', 'python', 'scala', 'sql', 'ma...\n\n\n6\ndata scientist\njunior\nNaN\ncompany_014\nBerkeley, CA\n15 days ago\nBerkeley, CA, US\nEducation\nNaN\n17,471\nEducation\n€51,330 - €70,144\n[]\n\n\n7\nmachine learning engineer\nsenior\non-site\ncompany_015\nMenlo Park, CA\n9 days ago\nMenlo Park, CA, US\nTechnology\nPublic\n900\nPublic\n€121,480 - €132,440\n['machine learning']\n\n\n8\ndata scientist\nsenior\nremote\ncompany_019\nFully Remote\n6 days ago\nBoston, MA, US\nTechnology\nPrivate\n126\nPrivate\n€207,331\n[]\n\n\n9\ndata scientist\nsenior\non-site\ncompany_021\nOn-site\na day ago\nSan Francisco, CA, US\nTechnology\nPublic\n5,520\n€61.06B\n€219,201\n[]\n\n\n\n\n\n\n\n\n\nCode\n# Step 2 – Create binary skill features (python, sql, spark, etc.)\n\nskill_keywords = [\n    \"python\", \"sql\", \"spark\", \"r\", \"scala\", \"docker\",\n    \"tableau\", \"powerbi\",\n    \"tensorflow\", \"pytorch\", \"machine learning\", \"deep learning\",\n    \"aws\", \"azure\", \"gcp\"\n]\n\nfor kw in skill_keywords:\n    col = \"skill_\" + kw.lower().replace(\" \", \"_\")\n    df[col] = df[\"skills\"].str.contains(kw, case=False, na=False).astype(int)\n\n\n\n\n\nCode\n# Check how many postings mention each skill\n\ndf.filter(like=\"skill_\").sum()\n\n\nskill_python              640\nskill_sql                 442\nskill_spark               161\nskill_r                   694\nskill_scala                85\nskill_docker               54\nskill_tableau             116\nskill_powerbi              25\nskill_tensorflow          165\nskill_pytorch             148\nskill_machine_learning    580\nskill_deep_learning       178\nskill_aws                 218\nskill_azure               155\nskill_gcp                 106\ndtype: int64\n\n\n\n\nCode\n# Step 3 — Create Binary Target: Is This an ML Role?\n\nml_keywords = [\n    \"machine learning\", \"deep learning\", \"neural network\",\n    \"tensorflow\", \"pytorch\", \"ai\", \"ml\"\n]\n\npattern = \"|\".join(ml_keywords)\n\ndf[\"is_ml_role\"] = df[\"skills\"].str.contains(pattern, case=False, na=False).astype(int)\n\ndf[\"is_ml_role\"].value_counts()\n\n\nis_ml_role\n1    603\n0    341\nName: count, dtype: int64\n\n\n\n\nCode\n# Step 4 – Clean salary column\n\nimport re\n\ndef parse_salary_range(s):\n    if pd.isna(s):\n        return (None, None)\n    \n    # Remove € and commas\n    s = s.replace(\"€\", \"\").replace(\",\", \"\").strip()\n    \n    # Match patterns like \"100000 – 200000\"\n    match = re.findall(r\"(\\d+)\", s)\n    \n    if len(match) == 2:\n        low, high = map(int, match)\n        return low, high\n    else:\n        return (None, None)\n\n# Apply to dataset\ndf[\"salary_min\"], df[\"salary_max\"] = zip(*df[\"salary\"].apply(parse_salary_range))\n\n# Average salary\ndf[\"salary_avg\"] = df[[\"salary_min\", \"salary_max\"]].mean(axis=1)\n\ndf[[\"salary_min\", \"salary_max\", \"salary_avg\"]].head()\n\n\n\n\n\n\n\n\n\nsalary_min\nsalary_max\nsalary_avg\n\n\n\n\n0\n100472.0\n200938.0\n150705.0\n\n\n1\nNaN\nNaN\nNaN\n\n\n2\n94987.0\n159559.0\n127273.0\n\n\n3\n112797.0\n194402.0\n153599.5\n\n\n4\n114172.0\n228337.0\n171254.5\n\n\n\n\n\n\n\n\n\nCode\n# Check missing salary values\ndf[[\"salary_min\", \"salary_max\", \"salary_avg\"]].isna().sum()\n\n\nsalary_min    350\nsalary_max    350\nsalary_avg    350\ndtype: int64\n\n\nThe dataset contained missing values across several fields such as seniority_level, status, salary, revenue, and ownership. Missing salary values were particularly common because many job postings did not disclose salary ranges. After transforming the salary column into numeric values (min, max, avg), these missing entries became explicit as NaN\n\n\nCode\ndf[\"seniority_level\"].value_counts(dropna=False)\ndf[\"status\"].value_counts(dropna=False)\ndf[\"industry\"].value_counts().head(20)\n\n\nindustry\nTechnology       582\nFinance          127\nRetail           110\nHealthcare        83\nEducation         19\nEnergy            12\nManufacturing      7\nLogistics          4\nName: count, dtype: int64"
  },
  {
    "objectID": "ml_methods.html#step-5b-encode-status-remote-hybrid-on-site",
    "href": "ml_methods.html#step-5b-encode-status-remote-hybrid-on-site",
    "title": "Machine Learning Methods",
    "section": "STEP 5B – Encode status (remote / hybrid / on-site)",
    "text": "STEP 5B – Encode status (remote / hybrid / on-site)\n\n\nCode\ndf[\"status\"].value_counts(dropna=False)\n\n\nstatus\non-site    363\nNaN        256\nhybrid     207\nremote     118\nName: count, dtype: int64\n\n\n\n\nCode\n\n# Replace missing with \"unknown\"\ndf[\"status\"] = df[\"status\"].fillna(\"unknown\").str.lower()\n\nstatus_dummies = pd.get_dummies(df[\"status\"], prefix=\"status\")\n\ndf = pd.concat([df, status_dummies], axis=1)\n\ndf[status_dummies.columns].head()\n\n\n\n\n\n\n\n\n\nstatus_hybrid\nstatus_on-site\nstatus_remote\nstatus_unknown\n\n\n\n\n0\nTrue\nFalse\nFalse\nFalse\n\n\n1\nTrue\nFalse\nFalse\nFalse\n\n\n2\nFalse\nTrue\nFalse\nFalse\n\n\n3\nTrue\nFalse\nFalse\nFalse\n\n\n4\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\n\nCode\ndf[status_dummies.columns].sum()\n\n\nstatus_hybrid     207\nstatus_on-site    363\nstatus_remote     118\nstatus_unknown    256\ndtype: int64"
  },
  {
    "objectID": "ml_methods.html#step-5c-encode-industry",
    "href": "ml_methods.html#step-5c-encode-industry",
    "title": "Machine Learning Methods",
    "section": "STEP 5C — Encode Industry",
    "text": "STEP 5C — Encode Industry\n\n\nCode\n# Step 5C – One-hot encode industry\n\ndf[\"industry\"] = df[\"industry\"].str.lower()\n\nindustry_dummies = pd.get_dummies(df[\"industry\"], prefix=\"industry\")\n\ndf = pd.concat([df, industry_dummies], axis=1)\n\ndf[industry_dummies.columns].head()\n\n\n\n\n\n\n\n\n\nindustry_education\nindustry_energy\nindustry_finance\nindustry_healthcare\nindustry_logistics\nindustry_manufacturing\nindustry_retail\nindustry_technology\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\nCode\nindustry_dummies.sum()\n\n\nindustry_education         19\nindustry_energy            12\nindustry_finance          127\nindustry_healthcare        83\nindustry_logistics          4\nindustry_manufacturing      7\nindustry_retail           110\nindustry_technology       582\ndtype: int64"
  },
  {
    "objectID": "ml_methods.html#step-6-build-feature-matrix-x-for-clustering-and-regression",
    "href": "ml_methods.html#step-6-build-feature-matrix-x-for-clustering-and-regression",
    "title": "Machine Learning Methods",
    "section": "Step 6 – Build feature matrix X for clustering and regression",
    "text": "Step 6 – Build feature matrix X for clustering and regression\n\n\nCode\n## Step 6 – Build feature matrix X for clustering and regression\n\n# 1) Skill features\nskill_cols = [c for c in df.columns if c.startswith(\"skill_\")]\n\n# 2) Seniority dummies\nseniority_cols = [\n    c for c in df.columns\n    if c.startswith(\"seniority_\") and c != \"seniority_level\"\n]\n\n# 3) Status dummies\nstatus_cols = [c for c in df.columns if c.startswith(\"status_\")]\n\n# 4) Industry dummies\nindustry_cols = [c for c in df.columns if c.startswith(\"industry_\")]\n\n# Combine all features\nfeature_cols = skill_cols + seniority_cols + status_cols + industry_cols\n\nX = df[feature_cols]\n\nX.dtypes.head(), X.shape\n\n\n(skill_python    int64\n skill_sql       int64\n skill_spark     int64\n skill_r         int64\n skill_scala     int64\n dtype: object,\n (944, 32))"
  },
  {
    "objectID": "ml_methods.html#step-7-unsupervised-learning-k-means-clustering",
    "href": "ml_methods.html#step-7-unsupervised-learning-k-means-clustering",
    "title": "Machine Learning Methods",
    "section": "Step 7 – Unsupervised Learning: K-Means Clustering",
    "text": "Step 7 – Unsupervised Learning: K-Means Clustering\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled.shape\n\n\n(944, 32)\n\n\n\n\nCode\n#Elbow plot to determine optimal k\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ninertias = []\nK = range(2, 11)\n\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    km.fit(X_scaled)\n    inertias.append(km.inertia_)\n\nplt.figure(figsize=(6, 4))\nplt.plot(K, inertias, marker=\"o\")\nplt.xlabel(\"Number of clusters k\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Plot for K-Means\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe selected k = 5 because the reduction in inertia begins to slow after five clusters. While the elbow is not very sharp, k = 5 marks the point where additional clusters provide smaller incremental improvements, making it a reasonable balance between simplicity and segmentation quality.\n\n\nCode\n#Fit K-means with chosen k\n\nk = 5\nkmeans = KMeans(n_clusters=k, random_state=42)\ndf[\"cluster\"] = kmeans.fit_predict(X_scaled)\n\ndf[\"cluster\"].value_counts()\n\n\ncluster\n2    229\n1    218\n4    218\n0    166\n3    113\nName: count, dtype: int64\n\n\nAlthough the inertia curve suggested both 4 and 5 as reasonable options, the 5-cluster solution produced a very small cluster (7 observations), which limits interpretability. Therefore, we selected k = 4, ensuring all clusters are large, stable, and meaningful for analysis.\n\n\nCode\n#Fit K-means with  chosen k=4\n\nk = 4\n\nkmeans = KMeans(n_clusters=k, random_state=42)\ndf[\"cluster\"] = kmeans.fit_predict(X_scaled)\n\ndf[\"cluster\"].value_counts()\n\n\ncluster\n1    309\n2    236\n3    228\n0    171\nName: count, dtype: int64\n\n\n\n\nCode\n#Inspect Cluster Centers (Interpretation)\ncluster_centers = pd.DataFrame(\n    kmeans.cluster_centers_,\n    columns=X.columns\n)\ncluster_centers\n\n\n\n\n\n\n\n\n\nskill_python\nskill_sql\nskill_spark\nskill_r\nskill_scala\nskill_docker\nskill_tableau\nskill_powerbi\nskill_tensorflow\nskill_pytorch\n...\nstatus_remote\nstatus_unknown\nindustry_education\nindustry_energy\nindustry_finance\nindustry_healthcare\nindustry_logistics\nindustry_manufacturing\nindustry_retail\nindustry_technology\n\n\n\n\n0\n0.463923\n-0.094527\n0.479443\n0.600192\n0.298335\n0.760944\n-0.124916\n-0.019252\n1.772486\n1.836619\n...\n-0.130409\n0.139790\n-0.101678\n-0.009069\n0.051324\n-0.021373\n2.048539e-01\n-0.086433\n0.001352\n-0.005122\n\n\n1\n0.495273\n0.501466\n0.166063\n0.578189\n0.284649\n-0.162708\n0.158013\n0.117237\n-0.323888\n-0.395593\n...\n-0.094185\n-0.544479\n-0.051142\n-0.084583\n0.032522\n-0.001925\n-6.523281e-02\n0.064455\n0.433649\n-0.276256\n\n\n2\n-1.287720\n-0.827945\n-0.453453\n-1.598912\n-0.314567\n-0.246321\n-0.296855\n-0.164935\n-0.460228\n-0.431196\n...\n0.339527\n-0.428902\n-0.082975\n0.037823\n-0.071403\n-0.071074\n9.067605e-17\n0.012348\n-0.257523\n0.274508\n\n\n3\n0.313737\n0.248272\n-0.115278\n0.421272\n-0.283922\n-0.095231\n0.186808\n0.026274\n-0.414034\n-0.395007\n...\n-0.125988\n1.077021\n0.231455\n0.082283\n-0.008660\n0.092206\n-6.523281e-02\n-0.035310\n-0.322164\n0.094102\n\n\n\n\n4 rows × 32 columns\n\n\n\n\n\nCode\n#lets make a table \ncluster_centers = pd.DataFrame(\n    kmeans.cluster_centers_,\n    columns=X.columns,\n    index=[f\"cluster_{i}\" for i in range(k)]\n)\n\ncluster_centers.style.background_gradient(\n    cmap=\"coolwarm\"  # red = high, blue = low\n)\n\n\n\n\n\n\n\n \nskill_python\nskill_sql\nskill_spark\nskill_r\nskill_scala\nskill_docker\nskill_tableau\nskill_powerbi\nskill_tensorflow\nskill_pytorch\nskill_machine_learning\nskill_deep_learning\nskill_aws\nskill_azure\nskill_gcp\nseniority_junior\nseniority_lead\nseniority_midlevel\nseniority_senior\nseniority_unknown\nstatus_hybrid\nstatus_on-site\nstatus_remote\nstatus_unknown\nindustry_education\nindustry_energy\nindustry_finance\nindustry_healthcare\nindustry_logistics\nindustry_manufacturing\nindustry_retail\nindustry_technology\n\n\n\n\ncluster_0\n0.463923\n-0.094527\n0.479443\n0.600192\n0.298335\n0.760944\n-0.124916\n-0.019252\n1.772486\n1.836619\n0.623998\n1.147535\n0.687033\n0.772292\n0.792742\n0.090010\n0.017586\n-0.206620\n0.234329\n-0.260525\n0.091914\n-0.117267\n-0.130409\n0.139790\n-0.101678\n-0.009069\n0.051324\n-0.021373\n0.204854\n-0.086433\n0.001352\n-0.005122\n\n\ncluster_1\n0.495273\n0.501466\n0.166063\n0.578189\n0.284649\n-0.162708\n0.158013\n0.117237\n-0.323888\n-0.395593\n0.419866\n-0.101473\n0.051005\n-0.058847\n-0.140399\n-0.124625\n0.079153\n-0.229183\n0.280120\n-0.260525\n-0.013746\n0.573289\n-0.094185\n-0.544479\n-0.051142\n-0.084583\n0.032522\n-0.001925\n-0.065233\n0.064455\n0.433649\n-0.276256\n\n\ncluster_2\n-1.287720\n-0.827945\n-0.453453\n-1.598912\n-0.314567\n-0.246321\n-0.296855\n-0.164935\n-0.460228\n-0.431196\n-1.236186\n-0.482054\n-0.487647\n-0.431790\n-0.328815\n-0.112156\n-0.025813\n-0.264329\n-0.166378\n0.781575\n0.320030\n-0.111053\n0.339527\n-0.428902\n-0.082975\n0.037823\n-0.071403\n-0.071074\n0.000000\n0.012348\n-0.257523\n0.274508\n\n\ncluster_3\n0.313737\n0.248272\n-0.115278\n0.421272\n-0.283922\n-0.095231\n0.186808\n0.026274\n-0.414034\n-0.395007\n0.242533\n-0.224160\n-0.079643\n-0.052525\n-0.063926\n0.217482\n-0.093744\n0.739172\n-0.383167\n-0.260525\n-0.381566\n-0.574058\n-0.125988\n1.077021\n0.231455\n0.082283\n-0.008660\n0.092206\n-0.065233\n-0.035310\n-0.322164\n0.094102\n\n\n\n\n\nCluster 0: represents Machine Learning Engineers/AI Engineers, characterized by the strongest deep learning stack (TensorFlow, PyTorch, ML, DL), the broadest cloud expertise (AWS/Azure/GCP), and robust programming (Python, R, Docker). They are more common in logistics and rarely appear in manufacturing, and they tend to work in hybrid settings rather than fully on-site roles.\nCluster 1 represents senior Data Engineering and Analytics Engineering roles. These jobs show the highest proficiency in Python, SQL, Spark, Scala, and Tableau, and appear predominantly in finance, healthcare, manufacturing, and retail. The strong presence of lead-level roles and on-site work further indicates a traditional enterprise engineering environment rather than modern remote ML roles.\nCluster 2 consists of low-information or non-technical data roles. These job postings show the lowest levels of programming, analytics, cloud, and machine learning skills across all clusters. They also contain the highest proportion of unknown seniority and are most frequently remote or hybrid. The absence of clear technical requirements and the inconsistent industry pattern suggest that these positions are general data-related or support roles rather than specialized data engineering, analytics, or machine learning positions.\nCluster 3 represents mid-level generalist data roles, including Data Analysts and Data Scientists who work across education, energy, and technology sectors. These jobs show strong Python, SQL, and Tableau usage, moderate R, and only limited machine learning depth. The seniority pattern is centered on mid-level positions, indicating roles that require solid technical competence without the advanced ML or engineering specialization seen in Clusters 0 and 1.\nThe purpose of K-means clustering is to uncover natural job families based on shared skill profiles, seniority, and industry characteristics. By grouping similar postings together without using salary, we can identify distinct job types and then compare their compensation and skill requirements in subsequent analysis.\n\n\nCode\ndf[\"cluster\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Number of Job Postings per Cluster\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.boxplot(column=\"salary_avg\", by=\"cluster\", figsize=(7,5))\nplt.title(\"Salary Distribution by Cluster\")\nplt.suptitle(\"\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Salary\")\nplt.show()"
  },
  {
    "objectID": "ml_methods.html#supervised-regression-modeling-for-salary-prediction",
    "href": "ml_methods.html#supervised-regression-modeling-for-salary-prediction",
    "title": "Machine Learning Methods",
    "section": "Supervised Regression Modeling for Salary Prediction",
    "text": "Supervised Regression Modeling for Salary Prediction\n\n\nCode\n# ===== Block 1 – Clean skill columns =====\n\n# 1) Define the *valid* skills you actually care about\nskill_keywords = [\n    \"python\", \"sql\", \"spark\", \"r\", \"scala\", \"docker\",\n    \"excel\", \"tableau\", \"power bi\",\n    \"tensorflow\", \"pytorch\", \"machine learning\", \"deep learning\",\n    \"aws\", \"azure\", \"gcp\"\n]\n\nvalid_skill_cols = [\n    \"skill_\" + kw.lower().replace(\" \", \"_\") for kw in skill_keywords\n]\n\n# 2) Find ALL columns that start with \"skill_\"\nall_skill_cols = [c for c in df.columns if c.startswith(\"skill_\")]\n\n# 3) Anything that starts with \"skill_\" but is not in our valid list is garbage\nextra_skill_cols = [c for c in all_skill_cols if c not in valid_skill_cols]\n\nprint(\"Extra / invalid skill columns to drop:\")\nprint(extra_skill_cols)\n\n# 4) Drop the garbage columns\ndf = df.drop(columns=extra_skill_cols)\n\n\nExtra / invalid skill columns to drop:\n['skill_powerbi']\n\n\n\n\nCode\n# ===== Block 2 – Build X and y for salary regression (robust) =====\n\n# Make sure salary_avg exists and is numeric\ndf[\"salary_avg\"] = df[\"salary_avg\"].astype(float)\n\n# Drop rows with missing salary\nreg_df = df.dropna(subset=[\"salary_avg\"]).copy()\n\n# Skills (already cleaned)\nskill_cols = valid_skill_cols\n\n# Seniority / status / industry dummies\nseniority_cols = [c for c in reg_df.columns \n                  if c.startswith(\"seniority_\") and c != \"seniority_level\"]\nstatus_cols    = [c for c in reg_df.columns if c.startswith(\"status_\")]\nindustry_cols  = [c for c in reg_df.columns if c.startswith(\"industry_\")]\n\n# All desired features\nfeature_cols = skill_cols + seniority_cols + status_cols + industry_cols\n\n# Keep only columns that actually exist in reg_df\navailable_features = [c for c in feature_cols if c in reg_df.columns]\nmissing_features   = sorted(set(feature_cols) - set(available_features))\n\nprint(\"Number of desired features:\", len(feature_cols))\nprint(\"Number of available features:\", len(available_features))\n\nif missing_features:\n    print(\"\\nThese expected features were NOT found in reg_df and will be ignored:\")\n    for c in missing_features:\n        print(\"  -\", c)\n\n# Build X and y\nX = reg_df[available_features].astype(float)\ny = reg_df[\"salary_avg\"].astype(float)\n\n\nNumber of desired features: 33\nNumber of available features: 31\n\nThese expected features were NOT found in reg_df and will be ignored:\n  - skill_excel\n  - skill_power_bi\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.3,\n    random_state=42\n)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n\n((415, 31), (179, 31), (415,), (179,))\n\n\n\n\nCode\nimport statsmodels.api as sm\nimport numpy as np\n\n# 1. Make sure X_train is purely numeric (float)\nX_train_sm = X_train.astype(float)\n\n# 2. Add intercept term\nX_train_sm = sm.add_constant(X_train_sm)\n\n# 3. Ensure y is numeric 1-D array\ny_train_sm = y_train.astype(float)\n\n# 4. Fit OLS model\nols_model = sm.OLS(y_train_sm, X_train_sm).fit()\n\n# 5. Show R-style summary: coefficients, p-values, R², etc.\nprint(ols_model.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             salary_avg   R-squared:                       0.368\nModel:                            OLS   Adj. R-squared:                  0.322\nMethod:                 Least Squares   F-statistic:                     8.011\nDate:                Sun, 07 Dec 2025   Prob (F-statistic):           1.31e-24\nTime:                        05:37:13   Log-Likelihood:                -5019.9\nNo. Observations:                 415   AIC:                         1.010e+04\nDf Residuals:                     386   BIC:                         1.021e+04\nDf Model:                          28                                         \nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nconst                   9.455e+04   5203.625     18.170      0.000    8.43e+04    1.05e+05\nskill_python            7541.9936   8348.852      0.903      0.367   -8872.924     2.4e+04\nskill_sql              -9547.7980   6461.924     -1.478      0.140   -2.23e+04    3157.177\nskill_spark            -4222.9763   7531.012     -0.561      0.575    -1.9e+04    1.06e+04\nskill_r                 3661.1136   9535.023      0.384      0.701   -1.51e+04    2.24e+04\nskill_scala             1.722e+04   1.01e+04      1.706      0.089   -2627.626    3.71e+04\nskill_docker            6289.2099   1.23e+04      0.510      0.611    -1.8e+04    3.05e+04\nskill_tableau          -4514.2138   7401.219     -0.610      0.542   -1.91e+04       1e+04\nskill_tensorflow        9381.2722   1.06e+04      0.886      0.376   -1.14e+04    3.02e+04\nskill_pytorch          -1.032e+04   1.19e+04     -0.866      0.387   -3.38e+04    1.31e+04\nskill_machine_learning -2.715e+04   7416.228     -3.660      0.000   -4.17e+04   -1.26e+04\nskill_deep_learning     8436.1008   7252.748      1.163      0.245   -5823.736    2.27e+04\nskill_aws              -7315.6652   7080.626     -1.033      0.302   -2.12e+04    6605.757\nskill_azure            -1.731e+04   8697.296     -1.990      0.047   -3.44e+04    -208.468\nskill_gcp               5846.3622   9966.892      0.587      0.558   -1.37e+04    2.54e+04\nseniority_junior        1.045e+04   1.16e+04      0.901      0.368   -1.24e+04    3.33e+04\nseniority_lead           6.41e+04   7120.742      9.001      0.000    5.01e+04    7.81e+04\nseniority_midlevel      6504.6449   6607.781      0.984      0.326   -6487.103    1.95e+04\nseniority_senior        2.147e+04   4356.793      4.927      0.000    1.29e+04       3e+04\nseniority_unknown      -7967.8666   8342.888     -0.955      0.340   -2.44e+04    8435.324\nstatus_hybrid           2.512e+04   4363.442      5.756      0.000    1.65e+04    3.37e+04\nstatus_on-site          4.884e+04   3907.818     12.498      0.000    4.12e+04    5.65e+04\nstatus_remote           3.483e+04   5588.138      6.233      0.000    2.38e+04    4.58e+04\nstatus_unknown         -1.424e+04   4809.358     -2.961      0.003   -2.37e+04   -4783.127\nindustry_education     -3451.9383   1.39e+04     -0.248      0.804   -3.08e+04    2.39e+04\nindustry_energy         2.739e+04   1.78e+04      1.542      0.124   -7534.737    6.23e+04\nindustry_finance        2.389e+04   9049.384      2.640      0.009    6095.236    4.17e+04\nindustry_healthcare     1.192e+04   9242.597      1.290      0.198   -6250.346    3.01e+04\nindustry_logistics      4.458e+04   4.09e+04      1.089      0.277   -3.59e+04    1.25e+05\nindustry_manufacturing -3.774e+04   1.91e+04     -1.972      0.049   -7.54e+04    -105.124\nindustry_retail         1.624e+04   1.02e+04      1.592      0.112   -3811.470    3.63e+04\nindustry_technology     1.173e+04   7046.224      1.664      0.097   -2126.708    2.56e+04\n==============================================================================\nOmnibus:                        5.510   Durbin-Watson:                   1.973\nProb(Omnibus):                  0.064   Jarque-Bera (JB):                5.609\nSkew:                          -0.281   Prob(JB):                       0.0605\nKurtosis:                       2.908   Cond. No.                     1.09e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.55e-29. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport numpy as np\n\n# ---------- Multiple Linear Regression ----------\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\ny_pred_lin = lin_reg.predict(X_test)\n\nlin_r2 = r2_score(y_test, y_pred_lin)\nlin_mse = mean_squared_error(y_test, y_pred_lin)\nlin_rmse = np.sqrt(lin_mse)\n\nprint(\"=== Multiple Linear Regression ===\")\nprint(f\"R²:   {lin_r2:.3f}\")\nprint(f\"RMSE: {lin_rmse:,.0f}\")\n\n# ---------- Random Forest Regression ----------\nrf_reg = RandomForestRegressor(\n    n_estimators=300,\n    random_state=42,\n    n_jobs=-1\n)\nrf_reg.fit(X_train, y_train)\n\ny_pred_rf = rf_reg.predict(X_test)\n\nrf_r2 = r2_score(y_test, y_pred_rf)\nrf_mse = mean_squared_error(y_test, y_pred_rf)\nrf_rmse = np.sqrt(rf_mse)\n\nprint(\"\\n=== Random Forest Regression ===\")\nprint(f\"R²:   {rf_r2:.3f}\")\nprint(f\"RMSE: {rf_rmse:,.0f}\")\n\n\n=== Multiple Linear Regression ===\nR²:   0.010\nRMSE: 57,183\n\n=== Random Forest Regression ===\nR²:   0.210\nRMSE: 51,108"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "",
    "text": "Code\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook\"\nResearch Question: What does the job market look like for Business Analytics, Data Science, and ML professionals in 2024?\nWhat we’re Looking into: 1. Who’s hiring? → Top industries (barplot) and companies (treemap) 2. What roles exist? → Job titles within our occupation categories 3. What do they say? → Word cloud from job descriptions 4. What skills do they want? → Radar charts for each occupation + software skills barplot 5. What drives salary? → Salary by remote work type 6. Where are jobs posted? → Source types analysis\nEach insight builds toward our ML modeling decisions.\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport re\n\npd.set_option('display.max_columns', None)\n\n# Load data\ndf = pd.read_csv('data/lightcast_cleaned.csv')\ndf['POSTED'] = pd.to_datetime(df['POSTED'])\n\n# Exclude unclassified/unknown values for cleaner analysis\ndf_clean = df[~df['NAICS_2022_2_NAME'].str.contains('Unclassified', na=False)].copy()\ndf_clean = df_clean[~df_clean['COMPANY_NAME'].str.contains('Unclassified', case=False, na=False)].copy()\ndf_clean = df_clean[~df_clean['TITLE_NAME'].str.contains('Unclassified', case=False, na=False)].copy()\n\nprint(f\"Dataset: {len(df):,} job postings\")\nprint(f\"After filtering unclassified: {len(df_clean):,} job postings\")\nprint(f\"Date range: {df['POSTED'].min().strftime('%b %Y')} - {df['POSTED'].max().strftime('%b %Y')}\")\nprint(f\"Occupations: {df['LOT_V6_OCCUPATION_NAME'].nunique()}\")\n\n\nDataset: 55,917 job postings\nAfter filtering unclassified: 46,713 job postings\nDate range: May 2024 - Sep 2024\nOccupations: 4"
  },
  {
    "objectID": "eda.html#whos-hiring",
    "href": "eda.html#whos-hiring",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Who’s Hiring?",
    "text": "Who’s Hiring?\n\n\nCode\n# Top 10 Industries - Horizontal Bar Plot\nindustry_counts = df_clean['NAICS_2022_2_NAME'].value_counts().head(10)\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    y=industry_counts.index,\n    x=industry_counts.values,\n    orientation='h',\n    marker_color='steelblue',\n    text=industry_counts.values,\n    textposition='outside'\n))\n\nfig.update_layout(\n    title='Top 10 Industries Hiring Data Professionals',\n    xaxis_title='Number of Job Postings',\n    yaxis={'categoryorder': 'total ascending'},\n    template='plotly_white',\n    height=450,\n    margin=dict(r=80)\n)\nfig.write_image('figures/top_industries.png', scale=2)\nfig.show()\n\n# Top 10 Companies - Treemap visualization\ncompany_counts = df_clean['COMPANY_NAME'].value_counts().head(10).reset_index()\ncompany_counts.columns = ['Company', 'Postings']\n\nfig2 = px.treemap(\n    company_counts,\n    path=['Company'],\n    values='Postings',\n    title='Top 10 Companies Hiring Data Professionals',\n    color='Postings',\n    color_continuous_scale='Viridis'\n)\nfig2.update_layout(height=500, template='plotly_white')\nfig2.update_traces(textinfo='label+value+percent root')\nfig2.write_image('figures/top_companies_treemap.png', scale=2)\nfig2.show()\n\nprint(\"\\nTop 10 Companies by Job Postings:\")\nfor i, row in company_counts.iterrows():\n    pct = row['Postings'] / len(df_clean) * 100\n    print(f\"  {i+1}. {row['Company']}: {row['Postings']:,} postings ({pct:.1f}%)\")\n\n\n                                                    \n\n\n                                                    \n\n\n\nTop 10 Companies by Job Postings:\n  1. Deloitte: 2,271 postings (4.9%)\n  2. Accenture: 1,316 postings (2.8%)\n  3. PricewaterhouseCoopers: 697 postings (1.5%)\n  4. Insight Global: 355 postings (0.8%)\n  5. Cardinal Health: 346 postings (0.7%)\n  6. Smx Corporation Limited: 317 postings (0.7%)\n  7. Oracle: 311 postings (0.7%)\n  8. Robert Half: 308 postings (0.7%)\n  9. Randstad: 285 postings (0.6%)\n  10. Lumen Technologies: 267 postings (0.6%)"
  },
  {
    "objectID": "eda.html#what-roles-can-you-go-for",
    "href": "eda.html#what-roles-can-you-go-for",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "What Roles can you Go for?",
    "text": "What Roles can you Go for?\n\n\nCode\noccupations = df_clean['LOT_V6_OCCUPATION_NAME'].unique()\ncolors = {'Data / Data Mining Analyst': '#1f77b4', \n          'Business Intelligence Analyst': '#ff7f0e', \n          'Business / Management Analyst': '#2ca02c', \n          'Market Research Analyst': '#d62728'}\n\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=[f\"{occ}\" for occ in occupations],\n    horizontal_spacing=0.15,\n    vertical_spacing=0.25\n)\n\nfor idx, occ in enumerate(occupations):\n    row = idx // 2 + 1\n    col = idx % 2 + 1\n    \n    df_occ = df_clean[df_clean['LOT_V6_OCCUPATION_NAME'] == occ]\n    df_occ = df_occ[~df_occ['TITLE_NAME'].str.contains('Unclassified', case=False, na=False)]\n    top_titles = df_occ['TITLE_NAME'].value_counts().head(8)\n    \n    truncated_titles = [t[:28] + '...' if len(t) &gt; 28 else t for t in top_titles.index]\n    \n    fig.add_trace(\n        go.Bar(\n            y=truncated_titles,\n            x=top_titles.values,\n            orientation='h',\n            marker_color=colors.get(occ, '#636EFA'),\n            text=top_titles.values,\n            textposition='outside',\n            name=occ,\n            showlegend=False\n        ),\n        row=row, col=col\n    )\n    \n    fig.update_xaxes(title_text=\"Count\", row=row, col=col)\n    fig.update_yaxes(categoryorder='total ascending', row=row, col=col)\n\nfig.update_layout(\n    title_text='Top 8 Job Titles per Occupation Category (Excl. Unclassified)',\n    height=800,\n    template='plotly_white',\n    margin=dict(l=20, r=80, t=80, b=20)\n)\nfig.write_image('figures/job_titles_by_occupation.png', scale=2)\nfig.show()\n\nprint(\"\\nPostings per Occupation:\")\nfor occ, count in df_clean['LOT_V6_OCCUPATION_NAME'].value_counts().items():\n    print(f\"  • {occ}: {count:,} ({count/len(df_clean)*100:.1f}%)\")\n\n\n                                                    \n\n\n\nPostings per Occupation:\n  • Data / Data Mining Analyst: 22,352 (47.8%)\n  • Business Intelligence Analyst: 21,244 (45.5%)\n  • Business / Management Analyst: 2,999 (6.4%)\n  • Market Research Analyst: 118 (0.3%)"
  },
  {
    "objectID": "eda.html#wordcloud-of-job-descriptions",
    "href": "eda.html#wordcloud-of-job-descriptions",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Wordcloud of Job Descriptions",
    "text": "Wordcloud of Job Descriptions\n\n\nCode\n# Using a sample of 3000 rows to prevent crashes\n\ncustom_stopwords = {\n    'the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this',\n    'with', 'are', 'be', 'as', 'at', 'from', 'or', 'an', 'will', 'your', 'you',\n    'we', 'our', 'have', 'has', 'it', 'their', 'all', 'can', 'been', 'would',\n    'who', 'more', 'if', 'about', 'which', 'when', 'what', 'into', 'also',\n    'may', 'other', 'its', 'than', 'should', 'such', 'any', 'these', 'only',\n    'new', 'well', 'them', 'they', 'but', 'not', 'do', 'up', 'out', 'so',\n    'job', 'position', 'apply', 'applicant', 'employer', 'employment',\n    'equal', 'opportunity', 'eeo', 'affirmative', 'action', 'disability',\n    'race', 'color', 'religion', 'sex', 'national', 'origin', 'age',\n    'status', 'protected', 'discrimination', 'including', 'without', 'regard',\n    'com', 'www', 'http', 'https', 'click', 'here', 'learn', 'please', 'contact',\n    'must', 'work', 'working', 'experience', 'years', 'year', 'required',\n    'requirements', 'skills', 'ability', 'strong', 'excellent', 'good',\n    'team', 'company', 'business', 'including', 'within', 'across', 'using'\n}\n\ndf_sample = df_clean.sample(n=min(3000, len(df_clean)), random_state=42)\nbody_text = ' '.join(df_sample['BODY'].dropna().astype(str).tolist())\n\nbody_text = re.sub(r'[^a-zA-Z\\s]', ' ', body_text.lower())\nbody_text = re.sub(r'\\s+', ' ', body_text)\n\nwordcloud = WordCloud(\n    width=1200, \n    height=600,\n    background_color='white',\n    stopwords=custom_stopwords,\n    max_words=100,\n    colormap='viridis',\n    collocations=False,\n    random_state=42\n).generate(body_text)\n\nplt.figure(figsize=(14, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud: Common Terms in Job Descriptions', fontsize=18, fontweight='bold', pad=15)\nplt.tight_layout()\nplt.savefig('figures/job_description_wordcloud.png', dpi=200, bbox_inches='tight')\nplt.show()\n\nprint(f\"Word cloud generated from {len(df_sample):,} sampled job postings\")\n\n\n\n\n\n\n\n\n\nWord cloud generated from 3,000 sampled job postings"
  },
  {
    "objectID": "eda.html#skills-to-look-for",
    "href": "eda.html#skills-to-look-for",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Skills to Look For",
    "text": "Skills to Look For\n\n\nCode\ndef extract_skills(skills_series):\n    all_skills = []\n    for skills in skills_series.dropna():\n        if isinstance(skills, str) and skills not in ['Not Listed', '']:\n            all_skills.extend([s.strip() for s in skills.split(',')])\n    return Counter(all_skills)\n\n# Shorten skill names for better display\ndef shorten_skill(skill):\n    replacements = {\n        'SQL (Programming Language)': 'SQL',\n        'Microsoft Excel': 'Excel',\n        'Microsoft Power BI': 'Power BI',\n        'Computer Science': 'Computer Sci',\n        'Problem Solving': 'Problem Solving',\n        'Data Visualization': 'Data Viz',\n        'Business Intelligence': 'Business Intel',\n        'Project Management': 'Project Mgmt',\n        'Data Management': 'Data Mgmt',\n        'Business Development': 'Business Dev',\n        'Customer Service': 'Customer Svc',\n        'Marketing Strategy': 'Mktg Strategy',\n        'Market Research': 'Market Research',\n        'Statistical Analysis': 'Statistics'\n    }\n    return replacements.get(skill, skill[:14] + '..' if len(skill) &gt; 14 else skill)\n\nocc_short = {\n    'Business Intelligence Analyst': 'Business Intelligence',\n    'Data / Data Mining Analyst': 'Data/Data Mining',\n    'Business / Management Analyst': 'Business/Management',\n    'Market Research Analyst': 'Market Research'\n}\n\ncolors_list = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\noccupations = df_clean['LOT_V6_OCCUPATION_NAME'].unique().tolist()\n\n# Get top 8 skills for EACH occupation separately\nskill_data = {}\nfor occ in occupations:\n    df_occ = df_clean[df_clean['LOT_V6_OCCUPATION_NAME'] == occ]\n    occ_skills = extract_skills(df_occ['SKILLS_NAME'])\n    top_8 = occ_skills.most_common(8)\n    total = len(df_occ)\n    skill_data[occ] = {\n        'skills': [s[0] for s in top_8],\n        'values': [(s[1] / total * 100) for s in top_8]\n    }\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{'type': 'polar'}, {'type': 'polar'}],\n           [{'type': 'polar'}, {'type': 'polar'}]],\n    subplot_titles=[occ_short.get(occ, occ) for occ in occupations],\n    vertical_spacing=0.18,\n    horizontal_spacing=0.12\n)\n\nfor idx, occ in enumerate(occupations):\n    row = idx // 2 + 1\n    col = idx % 2 + 1\n    \n    skills = skill_data[occ]['skills']\n    values = skill_data[occ]['values']\n    \n    # Shorten labels and close the polygon\n    short_labels = [shorten_skill(s) for s in skills]\n    values_closed = values + [values[0]]\n    labels_closed = short_labels + [short_labels[0]]\n    \n    fig.add_trace(\n        go.Scatterpolar(\n            r=values_closed,\n            theta=labels_closed,\n            fill='toself',\n            name=occ_short.get(occ, occ),\n            line_color=colors_list[idx],\n            fillcolor=colors_list[idx],\n            opacity=0.5,\n            showlegend=False\n        ),\n        row=row, col=col\n    )\n\n# Update polar subplots\nfig.update_polars(\n    radialaxis=dict(\n        visible=True,\n        tickfont=dict(size=9),\n        range=[0, 85]\n    ),\n    angularaxis=dict(\n        tickfont=dict(size=10),\n        rotation=90,\n        direction='clockwise'\n    )\n)\n\nfig.update_layout(\n    title=dict(\n        text='Top 8 Skills by Occupation (Each with Own Top Skills)',\n        font=dict(size=16),\n        y=0.98\n    ),\n    height=750,\n    template='plotly_white',\n    margin=dict(t=80, b=30, l=60, r=60)\n)\n\n# Adjust subplot title positions\nfor annotation in fig['layout']['annotations']:\n    annotation['font'] = dict(size=12, color='#333')\n    annotation['y'] = annotation['y'] + 0.02\n\nfig.write_image('figures/skills_radar_by_occupation.png', scale=2)\nfig.show()\n\n# Print the top skills for each occupation\nprint(\"\\nTop 8 Skills by Occupation:\")\nfor occ in occupations:\n    print(f\"\\n{occ_short.get(occ, occ)}:\")\n    for skill, val in zip(skill_data[occ]['skills'], skill_data[occ]['values']):\n        print(f\"  • {skill}: {val:.1f}%\")\n\n# Software skills - Horizontal Bar Chart\nsoftware_counts = extract_skills(df_clean['SOFTWARE_SKILLS_NAME'])\ntop_software = pd.DataFrame(software_counts.most_common(12), columns=['Software', 'Count'])\ntop_software['Percentage'] = (top_software['Count'] / len(df_clean) * 100).round(1)\n\nfig2 = go.Figure()\nfig2.add_trace(go.Bar(\n    y=top_software['Software'],\n    x=top_software['Percentage'],\n    orientation='h',\n    marker_color='mediumpurple',\n    text=[f\"{p}%\" for p in top_software['Percentage']],\n    textposition='outside'\n))\n\nfig2.update_layout(\n    title='Top 12 Software/Technical Skills (% of Postings)',\n    xaxis_title='% of Job Postings',\n    yaxis={'categoryorder': 'total ascending'},\n    template='plotly_white',\n    height=450,\n    margin=dict(r=80)\n)\nfig2.write_image('figures/software_skills.png', scale=2)\nfig2.show()\n\n\n                                                    \n\n\n\nTop 8 Skills by Occupation:\n\nBusiness Intelligence:\n  • Communication: 43.7%\n  • SAP Applications: 35.8%\n  • Management: 35.7%\n  • Business Process: 29.5%\n  • Business Requirements: 26.7%\n  • Problem Solving: 25.4%\n  • Finance: 23.4%\n  • Consulting: 23.1%\n\nData/Data Mining:\n  • Data Analysis: 76.5%\n  • SQL (Programming Language): 51.6%\n  • Communication: 45.4%\n  • Management: 33.8%\n  • Python (Programming Language): 31.2%\n  • Tableau (Business Intelligence Software): 30.7%\n  • Microsoft Excel: 28.3%\n  • Dashboard: 27.9%\n\nBusiness/Management:\n  • Communication: 51.2%\n  • Management: 39.2%\n  • Leadership: 34.2%\n  • Operations: 33.8%\n  • Microsoft Excel: 32.0%\n  • Problem Solving: 30.9%\n  • Project Management: 29.1%\n  • Presentations: 27.5%\n\nMarket Research:\n  • Customer Relationship Management: 83.1%\n  • Business Process: 45.8%\n  • Communication: 41.5%\n  • Business Requirements: 41.5%\n  • Salesforce: 40.7%\n  • Project Management: 40.7%\n  • Sales: 39.8%\n  • Problem Solving: 38.1%"
  },
  {
    "objectID": "eda.html#how-does-remote-work-type-affect-salary",
    "href": "eda.html#how-does-remote-work-type-affect-salary",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "How does Remote Work Type affect Salary?",
    "text": "How does Remote Work Type affect Salary?\n\n\nCode\n# Salary Analysis - Violin Plot by Remote Work Type and Occupation\ndf_remote = df_clean[df_clean['REMOTE_TYPE_NAME'] != 'Not Specified'].copy()\n\nfig = px.violin(\n    df_remote,\n    x='REMOTE_TYPE_NAME',\n    y='SALARY',\n    color='LOT_V6_OCCUPATION_NAME',\n    box=True,\n    title='Salary Distribution by Remote Work Type and Occupation',\n    labels={\n        'REMOTE_TYPE_NAME': 'Remote Work Type',\n        'SALARY': 'Annual Salary ($)',\n        'LOT_V6_OCCUPATION_NAME': 'Occupation'\n    }\n)\nfig.update_layout(\n    template='plotly_white',\n    height=500,\n    legend=dict(orientation='h', yanchor='bottom', y=-0.3)\n)\nfig.write_image('figures/salary_by_remote.png', scale=2)\nfig.show()\n\nprint(\"\\nSalary Statistics by Remote Type:\")\nsalary_stats = df_remote.groupby('REMOTE_TYPE_NAME')['SALARY'].agg(['count', 'median', 'mean', 'std']).round(0)\nsalary_stats.columns = ['Count', 'Median', 'Mean', 'Std Dev']\nsalary_stats['Median'] = salary_stats['Median'].apply(lambda x: f\"${x:,.0f}\")\nsalary_stats['Mean'] = salary_stats['Mean'].apply(lambda x: f\"${x:,.0f}\")\nsalary_stats['Std Dev'] = salary_stats['Std Dev'].apply(lambda x: f\"${x:,.0f}\")\nprint(salary_stats)\n\n\n                                                    \n\n\n\nSalary Statistics by Remote Type:\n                  Count   Median      Mean  Std Dev\nREMOTE_TYPE_NAME                                   \nHybrid Remote      1482  $95,300  $104,846  $28,957\nNot Remote          688  $97,250  $104,763  $27,970\nRemote             7875  $98,800  $109,097  $29,321"
  },
  {
    "objectID": "eda.html#where-can-you-apply-for-these-jobs",
    "href": "eda.html#where-can-you-apply-for-these-jobs",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Where can you apply for these Jobs?",
    "text": "Where can you apply for these Jobs?\n\n\nCode\ndef extract_sources(source_series):\n    all_sources = []\n    for sources in source_series.dropna():\n        if isinstance(sources, str):\n            all_sources.extend([s.strip() for s in sources.split(',') if s.strip()])\n    return Counter(all_sources)\n\nsource_counts = extract_sources(df_clean['SOURCE_TYPES'])\n\ncleaned_counts = {}\nfor source, count in source_counts.items():\n    if source == 'NONE' or source == '':\n        continue\n    elif source == 'Job intermediary':\n        cleaned_counts['Recruiter'] = cleaned_counts.get('Recruiter', 0) + count\n    elif source =='FreeJobBoard':\n        cleaned_counts['Job Board'] = cleaned_counts.get('Job Board', 0) + count\n    else:\n        cleaned_counts[source] = cleaned_counts.get(source, 0) + count\n\ntop_sources = pd.DataFrame(list(cleaned_counts.items()), columns=['Source', 'Count'])\ntop_sources = top_sources.sort_values('Count', ascending=False).head(10).reset_index(drop=True)\ntop_sources['Percentage'] = (top_sources['Count'] / len(df_clean) * 100).round(1)\n\nfig = px.treemap(\n    top_sources,\n    path=['Source'],\n    values='Count',\n    title='Job Posting Sources Distribution',\n    color='Count',\n    color_continuous_scale='Blues'\n)\nfig.update_layout(template='plotly_white', height=500)\nfig.update_traces(textinfo='label+value+percent root', textfont_size=14)\nfig.write_image('figures/source_types.png', scale=2)\nfig.show()\n\nprint(\"\\nJob Posting Sources:\")\nfor i, row in top_sources.iterrows():\n    print(f\"  {i+1}. {row['Source']}: {row['Count']:,} ({row['Percentage']}%)\")\n\n\n                                                    \n\n\n\nJob Posting Sources:\n  1. Job Board: 38,924 (83.3%)\n  2. Company: 13,318 (28.5%)\n  3. Recruiter: 2,791 (6.0%)\n  4. Government: 862 (1.8%)\n  5. Education: 521 (1.1%)"
  },
  {
    "objectID": "eda.html#eda-conclusion",
    "href": "eda.html#eda-conclusion",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "EDA Conclusion",
    "text": "EDA Conclusion\nOur exploratory analysis of 46,700+ job postings reveals key insights for data analytics professionals:\nIndustry & Companies: Professional Services, administrative support, waste management and Finance dominate hiring, with Deloitte, Accenture, pricewaterhouse and Insight Global leading recruitment—consulting firms as the primary employers.\nSkills Demand: Each occupation has distinct skill requirements. Data Analysts prioritize SQL and Data Analysis, while Business Analysts focus on Communication and Project Management. Market Research emphasizes Marketing and Customer insights.\nCompensation: Median salaries range $95K-$99K across remote types, with remote positions slightly higher. Occupation type impacts salary more than remote status.\nJob Sources: 83% of postings appear on Job Boards, making platforms like LinkedIn and Indeed essential for job seekers. Direct company applications (28%) remain valuable."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The demand for data science, business analytics, and machine learning professionals has grown exponentially over the past decade. As organizations increasingly rely on data-driven decision making, understanding the current job market landscape is essential for career planning.\nThis project analyzes the 2024 job market using Lightcast data to answer key questions:\n\nWhat skills are most in-demand for Data Science, Business Analytics, and ML roles?\nWhich industries are hiring the most data professionals?\nHow do salaries vary by location, experience, and skill requirements?\nWhat is the career outlook for business analytics professionals?"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The demand for data science, business analytics, and machine learning professionals has grown exponentially over the past decade. As organizations increasingly rely on data-driven decision making, understanding the current job market landscape is essential for career planning.\nThis project analyzes the 2024 job market using Lightcast data to answer key questions:\n\nWhat skills are most in-demand for Data Science, Business Analytics, and ML roles?\nWhich industries are hiring the most data professionals?\nHow do salaries vary by location, experience, and skill requirements?\nWhat is the career outlook for business analytics professionals?"
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "2 Research Rationale",
    "text": "2 Research Rationale\nThe rise of artificial intelligence and automation has transformed the labor market. Data science and machine learning roles have emerged as some of the most sought-after positions across industries. Understanding which skills employers value, which regions offer the best opportunities, and how compensation varies can help job seekers make informed career decisions.\nThis analysis takes the perspective of a job seeker entering the data science field in 2024, examining real job posting data to identify trends and opportunities."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "3 Literature Review",
    "text": "3 Literature Review\nRecent research highlights the evolving nature of data science careers. According to industry reports, Python, SQL, and machine learning frameworks remain the most requested technical skills in job postings. The demand for cloud computing expertise (AWS, Azure, GCP) has also increased significantly.\nStudies show that salary disparities exist across geographic regions, with tech hubs like San Francisco, Seattle, and New York offering higher compensation. However, the rise of remote work has begun to equalize opportunities for candidates outside traditional tech centers.\nBusiness analytics roles increasingly require a blend of technical skills and domain expertise, with employers seeking candidates who can translate data insights into actionable business recommendations."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Preprocessing",
    "section": "",
    "text": "This notebook performs data cleaning on the Lightcast job postings dataset, preparing it for exploratory analysis and machine learning models.\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv('data/lightcast_job_postings.csv', \n                 low_memory=False,\n                 dtype={'ID': str}) \n\nprint(f\"Original dataset shape: {df.shape}\")\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)  \npd.set_option('display.width', None)\ndisplay(df.head())\n\n\nOriginal dataset shape: (72498, 131)\n\n\n\n\n\n\n\n\n\nID\nLAST_UPDATED_DATE\nLAST_UPDATED_TIMESTAMP\nDUPLICATES\nPOSTED\nEXPIRED\nDURATION\nSOURCE_TYPES\nSOURCES\nURL\nACTIVE_URLS\nACTIVE_SOURCES_INFO\nTITLE_RAW\nBODY\nMODELED_EXPIRED\nMODELED_DURATION\nCOMPANY\nCOMPANY_NAME\nCOMPANY_RAW\nCOMPANY_IS_STAFFING\nEDUCATION_LEVELS\nEDUCATION_LEVELS_NAME\nMIN_EDULEVELS\nMIN_EDULEVELS_NAME\nMAX_EDULEVELS\nMAX_EDULEVELS_NAME\nEMPLOYMENT_TYPE\nEMPLOYMENT_TYPE_NAME\nMIN_YEARS_EXPERIENCE\nMAX_YEARS_EXPERIENCE\nIS_INTERNSHIP\nSALARY\nREMOTE_TYPE\nREMOTE_TYPE_NAME\nORIGINAL_PAY_PERIOD\nSALARY_TO\nSALARY_FROM\nLOCATION\nCITY\nCITY_NAME\nCOUNTY\nCOUNTY_NAME\nMSA\nMSA_NAME\nSTATE\nSTATE_NAME\nCOUNTY_OUTGOING\nCOUNTY_NAME_OUTGOING\nCOUNTY_INCOMING\nCOUNTY_NAME_INCOMING\nMSA_OUTGOING\nMSA_NAME_OUTGOING\nMSA_INCOMING\nMSA_NAME_INCOMING\nNAICS2\nNAICS2_NAME\nNAICS3\nNAICS3_NAME\nNAICS4\nNAICS4_NAME\nNAICS5\nNAICS5_NAME\nNAICS6\nNAICS6_NAME\nTITLE\nTITLE_NAME\nTITLE_CLEAN\nSKILLS\nSKILLS_NAME\nSPECIALIZED_SKILLS\nSPECIALIZED_SKILLS_NAME\nCERTIFICATIONS\nCERTIFICATIONS_NAME\nCOMMON_SKILLS\nCOMMON_SKILLS_NAME\nSOFTWARE_SKILLS\nSOFTWARE_SKILLS_NAME\nONET\nONET_NAME\nONET_2019\nONET_2019_NAME\nCIP6\nCIP6_NAME\nCIP4\nCIP4_NAME\nCIP2\nCIP2_NAME\nSOC_2021_2\nSOC_2021_2_NAME\nSOC_2021_3\nSOC_2021_3_NAME\nSOC_2021_4\nSOC_2021_4_NAME\nSOC_2021_5\nSOC_2021_5_NAME\nLOT_CAREER_AREA\nLOT_CAREER_AREA_NAME\nLOT_OCCUPATION\nLOT_OCCUPATION_NAME\nLOT_SPECIALIZED_OCCUPATION\nLOT_SPECIALIZED_OCCUPATION_NAME\nLOT_OCCUPATION_GROUP\nLOT_OCCUPATION_GROUP_NAME\nLOT_V6_SPECIALIZED_OCCUPATION\nLOT_V6_SPECIALIZED_OCCUPATION_NAME\nLOT_V6_OCCUPATION\nLOT_V6_OCCUPATION_NAME\nLOT_V6_OCCUPATION_GROUP\nLOT_V6_OCCUPATION_GROUP_NAME\nLOT_V6_CAREER_AREA\nLOT_V6_CAREER_AREA_NAME\nSOC_2\nSOC_2_NAME\nSOC_3\nSOC_3_NAME\nSOC_4\nSOC_4_NAME\nSOC_5\nSOC_5_NAME\nLIGHTCAST_SECTORS\nLIGHTCAST_SECTORS_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nNAICS_2022_3\nNAICS_2022_3_NAME\nNAICS_2022_4\nNAICS_2022_4_NAME\nNAICS_2022_5\nNAICS_2022_5_NAME\nNAICS_2022_6\nNAICS_2022_6_NAME\n\n\n\n\n0\n1f57d95acf4dc67ed2819eb12f049f6a5c11782c\n9/6/2024\n2024-09-06 20:32:57.352 Z\n0.0\n6/2/2024\n6/8/2024\n6.0\n[\\n \"Company\"\\n]\n[\\n \"brassring.com\"\\n]\n[\\n \"https://sjobs.brassring.com/TGnewUI/Sear...\n[]\nNaN\nEnterprise Analyst (II-III)\n31-May-2024\\n\\nEnterprise Analyst (II-III)\\n\\n...\n6/8/2024\n6.0\n894731.0\nMurphy USA\nMurphy USA\nFalse\n[\\n 2\\n]\n[\\n \"Bachelor's degree\"\\n]\n2.0\nBachelor's degree\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n2.0\n2.0\nFalse\nNaN\n0.0\n[None]\nNaN\nNaN\nNaN\n{\\n \"lat\": 33.20763,\\n \"lon\": -92.6662674\\n}\nRWwgRG9yYWRvLCBBUg==\nEl Dorado, AR\n5139.0\nUnion, AR\n20980.0\nEl Dorado, AR\n5.0\nArkansas\n5139.0\nUnion, AR\n5139.0\nUnion, AR\n20980.0\nEl Dorado, AR\n20980.0\nEl Dorado, AR\n44.0\nRetail Trade\n441.0\nMotor Vehicle and Parts Dealers\n4413.0\nAutomotive Parts, Accessories, and Tire Retailers\n44133.0\nAutomotive Parts and Accessories Retailers\n441330.0\nAutomotive Parts and Accessories Retailers\nET29C073C03D1F86B4\nEnterprise Analysts\nenterprise analyst ii iii\n[\\n \"KS126DB6T061MHD7RTGQ\",\\n \"KS126706DPFD3...\n[\\n \"Merchandising\",\\n \"Mathematics\",\\n \"Pr...\n[\\n \"KS126DB6T061MHD7RTGQ\",\\n \"KS128006L3V0H...\n[\\n \"Merchandising\",\\n \"Predictive Modeling\"...\n[]\n[]\n[\\n \"KS126706DPFD3354M7YK\",\\n \"KS1280B68GD79...\n[\\n \"Mathematics\",\\n \"Presentations\",\\n \"Re...\n[\\n \"KS440W865GC4VRBW6LJP\",\\n \"KS13USA80NE38...\n[\\n \"SQL (Programming Language)\",\\n \"Power B...\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[\\n \"45.0601\",\\n \"27.0101\"\\n]\n[\\n \"Economics, General\",\\n \"Mathematics, Ge...\n[\\n \"45.06\",\\n \"27.01\"\\n]\n[\\n \"Economics\",\\n \"Mathematics\"\\n]\n[\\n \"45\",\\n \"27\"\\n]\n[\\n \"Social Sciences\",\\n \"Mathematics and St...\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231010.0\nBusiness Intelligence Analyst\n23101011.0\nGeneral ERP Analyst / Consultant\n2310.0\nBusiness Intelligence\n23101011.0\nGeneral ERP Analyst / Consultant\n231010.0\nBusiness Intelligence Analyst\n2310.0\nBusiness Intelligence\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n[\\n 7\\n]\n[\\n \"Artificial Intelligence\"\\n]\n44.0\nRetail Trade\n441.0\nMotor Vehicle and Parts Dealers\n4413.0\nAutomotive Parts, Accessories, and Tire Retailers\n44133.0\nAutomotive Parts and Accessories Retailers\n441330.0\nAutomotive Parts and Accessories Retailers\n\n\n1\n0cb072af26757b6c4ea9464472a50a443af681ac\n8/2/2024\n2024-08-02 17:08:58.838 Z\n0.0\n6/2/2024\n8/1/2024\nNaN\n[\\n \"Job Board\"\\n]\n[\\n \"maine.gov\"\\n]\n[\\n \"https://joblink.maine.gov/jobs/1085740\"\\n]\n[]\nNaN\nOracle Consultant - Reports (3592)\nOracle Consultant - Reports (3592)\\n\\nat SMX i...\n8/1/2024\nNaN\n133098.0\nSmx Corporation Limited\nSMX\nTrue\n[\\n 99\\n]\n[\\n \"No Education Listed\"\\n]\n99.0\nNo Education Listed\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n3.0\n3.0\nFalse\nNaN\n1.0\nRemote\nNaN\nNaN\nNaN\n{\\n \"lat\": 44.3106241,\\n \"lon\": -69.7794897\\n}\nQXVndXN0YSwgTUU=\nAugusta, ME\n23011.0\nKennebec, ME\n12300.0\nAugusta-Waterville, ME\n23.0\nMaine\n23011.0\nKennebec, ME\n23011.0\nKennebec, ME\n12300.0\nAugusta-Waterville, ME\n12300.0\nAugusta-Waterville, ME\n56.0\nAdministrative and Support and Waste Managemen...\n561.0\nAdministrative and Support Services\n5613.0\nEmployment Services\n56132.0\nTemporary Help Services\n561320.0\nTemporary Help Services\nET21DDA63780A7DC09\nOracle Consultants\noracle consultant reports\n[\\n \"KS122626T550SLQ7QZ1C\",\\n \"KS123YJ6KVWC9...\n[\\n \"Procurement\",\\n \"Financial Statements\",...\n[\\n \"KS122626T550SLQ7QZ1C\",\\n \"KS123YJ6KVWC9...\n[\\n \"Procurement\",\\n \"Financial Statements\",...\n[]\n[]\n[]\n[]\n[\\n \"BGSBF3F508F7F46312E3\",\\n \"ESEA839CED378...\n[\\n \"Oracle Business Intelligence (BI) / OBIA...\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231010.0\nBusiness Intelligence Analyst\n23101012.0\nOracle Consultant / Analyst\n2310.0\nBusiness Intelligence\n23101012.0\nOracle Consultant / Analyst\n231010.0\nBusiness Intelligence Analyst\n2310.0\nBusiness Intelligence\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\nNaN\nNaN\n56.0\nAdministrative and Support and Waste Managemen...\n561.0\nAdministrative and Support Services\n5613.0\nEmployment Services\n56132.0\nTemporary Help Services\n561320.0\nTemporary Help Services\n\n\n2\n85318b12b3331fa490d32ad014379df01855c557\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/7/2024\n35.0\n[\\n \"Job Board\"\\n]\n[\\n \"dejobs.org\"\\n]\n[\\n \"https://dejobs.org/dallas-tx/data-analys...\n[]\nNaN\nData Analyst\nTaking care of people is at the heart of every...\n6/10/2024\n8.0\n39063746.0\nSedgwick\nSedgwick\nFalse\n[\\n 2\\n]\n[\\n \"Bachelor's degree\"\\n]\n2.0\nBachelor's degree\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n5.0\nNaN\nFalse\nNaN\n0.0\n[None]\nNaN\nNaN\nNaN\n{\\n \"lat\": 32.7766642,\\n \"lon\": -96.7969879\\n}\nRGFsbGFzLCBUWA==\nDallas, TX\n48113.0\nDallas, TX\n19100.0\nDallas-Fort Worth-Arlington, TX\n48.0\nTexas\n48113.0\nDallas, TX\n48113.0\nDallas, TX\n19100.0\nDallas-Fort Worth-Arlington, TX\n19100.0\nDallas-Fort Worth-Arlington, TX\n52.0\nFinance and Insurance\n524.0\nInsurance Carriers and Related Activities\n5242.0\nAgencies, Brokerages, and Other Insurance Rela...\n52429.0\nOther Insurance Related Activities\n524291.0\nClaims Adjusting\nET3037E0C947A02404\nData Analysts\ndata analyst\n[\\n \"KS1218W78FGVPVP2KXPX\",\\n \"ESF3939CE1F80...\n[\\n \"Management\",\\n \"Exception Reporting\",\\n...\n[\\n \"ESF3939CE1F80C10C327\",\\n \"KS120GV6C72JM...\n[\\n \"Exception Reporting\",\\n \"Data Analysis\"...\n[\\n \"KS683TN76T77DQDVBZ1B\"\\n]\n[\\n \"Security Clearance\"\\n]\n[\\n \"KS1218W78FGVPVP2KXPX\",\\n \"BGS1ADAA36DB6...\n[\\n \"Management\",\\n \"Report Writing\",\\n \"In...\n[\\n \"KS126HY6YLTB9R7XJC4Z\"\\n]\n[\\n \"Microsoft Office\"\\n]\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231113.0\nData / Data Mining Analyst\n23111310.0\nData Analyst\n2311.0\nData Analysis and Mathematics\n23111310.0\nData Analyst\n231113.0\nData / Data Mining Analyst\n2311.0\nData Analysis and Mathematics\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\nNaN\nNaN\n52.0\nFinance and Insurance\n524.0\nInsurance Carriers and Related Activities\n5242.0\nAgencies, Brokerages, and Other Insurance Rela...\n52429.0\nOther Insurance Related Activities\n524291.0\nClaims Adjusting\n\n\n3\n1b5c3941e54a1889ef4f8ae55b401a550708a310\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/20/2024\n48.0\n[\\n \"Job Board\"\\n]\n[\\n \"disabledperson.com\",\\n \"dejobs.org\"\\n]\n[\\n \"https://www.disabledperson.com/jobs/5948...\n[]\nNaN\nSr. Lead Data Mgmt. Analyst - SAS Product Owner\nAbout this role:\\n\\nWells Fargo is looking for...\n6/12/2024\n10.0\n37615159.0\nWells Fargo\nWells Fargo\nFalse\n[\\n 99\\n]\n[\\n \"No Education Listed\"\\n]\n99.0\nNo Education Listed\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n3.0\nNaN\nFalse\nNaN\n0.0\n[None]\nNaN\nNaN\nNaN\n{\\n \"lat\": 33.4483771,\\n \"lon\": -112.0740373\\n}\nUGhvZW5peCwgQVo=\nPhoenix, AZ\n4013.0\nMaricopa, AZ\n38060.0\nPhoenix-Mesa-Chandler, AZ\n4.0\nArizona\n4013.0\nMaricopa, AZ\n4013.0\nMaricopa, AZ\n38060.0\nPhoenix-Mesa-Chandler, AZ\n38060.0\nPhoenix-Mesa-Chandler, AZ\n52.0\nFinance and Insurance\n522.0\nCredit Intermediation and Related Activities\n5221.0\nDepository Credit Intermediation\n52211.0\nCommercial Banking\n522110.0\nCommercial Banking\nET2114E0404BA30075\nManagement Analysts\nsr lead data mgmt analyst sas product owner\n[\\n \"KS123QX62QYTC4JF38H8\",\\n \"KS7G6NP6R6L1H...\n[\\n \"Exit Strategies\",\\n \"Reliability\",\\n \"...\n[\\n \"KS123QX62QYTC4JF38H8\",\\n \"KS441PQ64HT13...\n[\\n \"Exit Strategies\",\\n \"User Story\",\\n \"H...\n[]\n[]\n[\\n \"KS7G6NP6R6L1H1SKFTSY\",\\n \"KS1218W78FGVP...\n[\\n \"Reliability\",\\n \"Management\",\\n \"Strat...\n[\\n \"KS4409D76NW1S5LNCL18\",\\n \"ESC7869CF7378...\n[\\n \"SAS (Software)\",\\n \"Google Cloud Platfo...\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231113.0\nData / Data Mining Analyst\n23111310.0\nData Analyst\n2311.0\nData Analysis and Mathematics\n23111310.0\nData Analyst\n231113.0\nData / Data Mining Analyst\n2311.0\nData Analysis and Mathematics\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n[\\n 6\\n]\n[\\n \"Data Privacy/Protection\"\\n]\n52.0\nFinance and Insurance\n522.0\nCredit Intermediation and Related Activities\n5221.0\nDepository Credit Intermediation\n52211.0\nCommercial Banking\n522110.0\nCommercial Banking\n\n\n4\ncb5ca25f02bdf25c13edfede7931508bfd9e858f\n6/19/2024\n2024-06-19 07:00:00.000 Z\n0.0\n6/2/2024\n6/17/2024\n15.0\n[\\n \"FreeJobBoard\"\\n]\n[\\n \"craigslist.org\"\\n]\n[\\n \"https://modesto.craigslist.org/sls/77475...\n[]\nNaN\nComisiones de $1000 - $3000 por semana... Comi...\nComisiones de $1000 - $3000 por semana... Comi...\n6/17/2024\n15.0\n0.0\nUnclassified\nLH/GM\nFalse\n[\\n 99\\n]\n[\\n \"No Education Listed\"\\n]\n99.0\nNo Education Listed\nNaN\nNaN\n3.0\nPart-time / full-time\nNaN\nNaN\nFalse\n92500.0\n0.0\n[None]\nyear\n150000.0\n35000.0\n{\\n \"lat\": 37.6392595,\\n \"lon\": -120.9970014\\n}\nTW9kZXN0bywgQ0E=\nModesto, CA\n6099.0\nStanislaus, CA\n33700.0\nModesto, CA\n6.0\nCalifornia\n6099.0\nStanislaus, CA\n6099.0\nStanislaus, CA\n33700.0\nModesto, CA\n33700.0\nModesto, CA\n99.0\nUnclassified Industry\n999.0\nUnclassified Industry\n9999.0\nUnclassified Industry\n99999.0\nUnclassified Industry\n999999.0\nUnclassified Industry\nET0000000000000000\nUnclassified\ncomisiones de por semana comiensa rapido\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231010.0\nBusiness Intelligence Analyst\n23101012.0\nOracle Consultant / Analyst\n2310.0\nBusiness Intelligence\n23101012.0\nOracle Consultant / Analyst\n231010.0\nBusiness Intelligence Analyst\n2310.0\nBusiness Intelligence\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\nNaN\nNaN\n99.0\nUnclassified Industry\n999.0\nUnclassified Industry\n9999.0\nUnclassified Industry\n99999.0\nUnclassified Industry\n999999.0\nUnclassified Industry"
  },
  {
    "objectID": "data_cleaning.html#select-relevant-columns",
    "href": "data_cleaning.html#select-relevant-columns",
    "title": "Data Cleaning & Preprocessing",
    "section": "1. Select Relevant Columns",
    "text": "1. Select Relevant Columns\n\n\nCode\ncolumns_to_keep = [\n    'POSTED', 'EXPIRED','TITLE_NAME', 'COMPANY_NAME','SOURCE_TYPES', 'COMPANY_IS_STAFFING', \n    'SALARY', 'SALARY_FROM', 'SALARY_TO','STATE_NAME', 'CITY_NAME','REMOTE_TYPE_NAME', \n    'EMPLOYMENT_TYPE_NAME','MIN_YEARS_EXPERIENCE','MIN_EDULEVELS_NAME','SKILLS_NAME', 'SOFTWARE_SKILLS_NAME',\n    'LOT_V6_OCCUPATION_NAME','NAICS_2022_2_NAME'\n]\ncolumns_to_keep = [col for col in columns_to_keep if col in df.columns]\ndf = df[columns_to_keep].copy()\n\nprint(f\"Shape after column selection: {df.shape}\")\nprint(f\"\\nColumns kept ({len(df.columns)}):\")\nprint(df.columns.tolist())\n\n\nShape after column selection: (72498, 19)\n\nColumns kept (19):\n['POSTED', 'EXPIRED', 'TITLE_NAME', 'COMPANY_NAME', 'SOURCE_TYPES', 'COMPANY_IS_STAFFING', 'SALARY', 'SALARY_FROM', 'SALARY_TO', 'STATE_NAME', 'CITY_NAME', 'REMOTE_TYPE_NAME', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MIN_EDULEVELS_NAME', 'SKILLS_NAME', 'SOFTWARE_SKILLS_NAME', 'LOT_V6_OCCUPATION_NAME', 'NAICS_2022_2_NAME']"
  },
  {
    "objectID": "data_cleaning.html#create-duration-feature",
    "href": "data_cleaning.html#create-duration-feature",
    "title": "Data Cleaning & Preprocessing",
    "section": "2. Create Duration Feature",
    "text": "2. Create Duration Feature\nDuration = EXPIRED - POSTED (in days)\n\n\nCode\ndf = df.dropna(subset=['EXPIRED'])\ndf['POSTED'] = pd.to_datetime(df['POSTED'], errors='coerce')\ndf['EXPIRED'] = pd.to_datetime(df['EXPIRED'], errors='coerce')\n\ndf['DURATION'] = (df['EXPIRED'] - df['POSTED']).dt.days\n\nprint(\"Duration statistics:\")\nprint(df['DURATION'].describe())\n\ndf.drop(columns=['EXPIRED'], inplace=True)\n\nprint(f\"\\nShape: {df.shape}\")\n\n\nDuration statistics:\ncount    64654.000000\nmean        35.296811\nstd         23.961129\nmin          0.000000\n25%         14.000000\n50%         31.000000\n75%         60.000000\nmax        119.000000\nName: DURATION, dtype: float64\n\nShape: (64654, 19)"
  },
  {
    "objectID": "data_cleaning.html#explore-lot_v6_occupation_name-for-filtering",
    "href": "data_cleaning.html#explore-lot_v6_occupation_name-for-filtering",
    "title": "Data Cleaning & Preprocessing",
    "section": "3. Explore LOT_V6_OCCUPATION_NAME for Filtering",
    "text": "3. Explore LOT_V6_OCCUPATION_NAME for Filtering\nUsing LOT_V6_OCCUPATION_NAME provides more accurate job classification than TITLE_NAME.\n\n\nCode\nprint(\"LOT_V6_OCCUPATION_NAME value counts:\\n\")\nprint(df['LOT_V6_OCCUPATION_NAME'].value_counts())\n\n\nLOT_V6_OCCUPATION_NAME value counts:\n\nLOT_V6_OCCUPATION_NAME\nData / Data Mining Analyst                                              26809\nBusiness Intelligence Analyst                                           26550\nComputer Systems Engineer / Architect                                    7188\nBusiness / Management Analyst                                            3729\nClinical Analyst / Clinical Documentation and Improvement Specialist      228\nMarket Research Analyst                                                   132\nName: count, dtype: int64\n\n\n\n\nCode\noccupations_to_keep = [\n    'Data / Data Mining Analyst',\n    'Business Intelligence Analyst',\n    'Business / Management Analyst',\n    'Market Research Analyst'\n\n]\ndf = df[df['LOT_V6_OCCUPATION_NAME'].isin(occupations_to_keep)].copy()\n\nprint(f\"Shape after filtering: {df.shape}\")\nprint(f\"\\nOccupations kept:\")\nprint(df['LOT_V6_OCCUPATION_NAME'].value_counts())\n\n\nShape after filtering: (57220, 19)\n\nOccupations kept:\nLOT_V6_OCCUPATION_NAME\nData / Data Mining Analyst       26809\nBusiness Intelligence Analyst    26550\nBusiness / Management Analyst     3729\nMarket Research Analyst            132\nName: count, dtype: int64"
  },
  {
    "objectID": "data_cleaning.html#clean-string-formatting",
    "href": "data_cleaning.html#clean-string-formatting",
    "title": "Data Cleaning & Preprocessing",
    "section": "4. Clean String Formatting",
    "text": "4. Clean String Formatting\nRemove JSON-like formatting: [\\n  \"value\"\\n] → value1, value2\nAlso remove empty arrays [].\n\n\nCode\ndef clean_json_string(value):\n  \n    if pd.isna(value):\n        return np.nan\n    \n    if not isinstance(value, str):\n        return value\n    \n    value = value.strip()\n    \n    # Handle empty list or [None]\n    if value in ['[]', '[None]', '[\\n]', '', '[ ]']:\n        return np.nan\n    \n    # Check if it's a JSON-like array\n    if value.startswith('[') and value.endswith(']'):\n        # Remove brackets\n        cleaned = value[1:-1]\n        \n        # Remove \\n, extra spaces, quotes\n        cleaned = re.sub(r'\\\\n', '', cleaned)\n        cleaned = re.sub(r'\\n', '', cleaned)\n        cleaned = re.sub(r'\"', '', cleaned)\n        \n        # Split by comma and clean each item\n        items = [item.strip() for item in cleaned.split(',') if item.strip()]\n        \n        if not items:\n            return np.nan\n        \n        return ', '.join(items)\n    \n    return value.strip()\n\n\n# Columns to clean\njson_columns = [\n    'SKILLS_NAME', \n    'SOFTWARE_SKILLS_NAME',\n    'SOURCE_TYPES',\n    'REMOTE_TYPE_NAME',\n    'EMPLOYMENT_TYPE_NAME',\n    'MIN_EDULEVELS_NAME'\n]\n\n# Apply cleaning\nfor col in json_columns:\n    if col in df.columns:\n        print(f\"Cleaning: {col}\")\n        df[col] = df[col].apply(clean_json_string)\n\n\n\nCleaning: SKILLS_NAME\nCleaning: SOFTWARE_SKILLS_NAME\nCleaning: SOURCE_TYPES\nCleaning: REMOTE_TYPE_NAME\nCleaning: EMPLOYMENT_TYPE_NAME\nCleaning: MIN_EDULEVELS_NAME"
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Data Cleaning & Preprocessing",
    "section": "5. Handling Missing Values",
    "text": "5. Handling Missing Values\n\n5.1 Salary\n\n\nCode\n\nprint(\"Missing salary values BEFORE imputation:\")\nprint(f\"  SALARY: {df['SALARY'].isna().sum()}\")\nprint(f\"  SALARY_FROM: {df['SALARY_FROM'].isna().sum()}\")\nprint(f\"  SALARY_TO: {df['SALARY_TO'].isna().sum()}\")\n\nsalary_cols = ['SALARY', 'SALARY_FROM', 'SALARY_TO']\n\nfor col in salary_cols:\n    if col in df.columns:\n        median_by_occupation = df.groupby('LOT_V6_OCCUPATION_NAME')[col].transform('median')\n        \n        df[col] = df[col].fillna(median_by_occupation)\n\n\n\nfor col in salary_cols:\n    if col in df.columns:\n        remaining_na = df[col].isna().sum()\n        if remaining_na &gt; 0:\n            overall_median = df[col].median()\n            df[col] = df[col].fillna(overall_median)\n            print(f\"\\nFilled {remaining_na} remaining {col} NaN with overall median: {overall_median}\")\n\nprint(\"\\nFinal missing salary values:\")\nprint(f\"  SALARY: {df['SALARY'].isna().sum()}\")\nprint(f\"  SALARY_FROM: {df['SALARY_FROM'].isna().sum()}\")\nprint(f\"  SALARY_TO: {df['SALARY_TO'].isna().sum()}\")\n\n\nMissing salary values BEFORE imputation:\n  SALARY: 32833\n  SALARY_FROM: 31410\n  SALARY_TO: 31410\n\nFinal missing salary values:\n  SALARY: 0\n  SALARY_FROM: 0\n  SALARY_TO: 0\n\n\n\n\n5.2 SKILLS\n\n\nCode\nbefore = len(df)\n\ndf = df.dropna(subset=['SKILLS_NAME'])\n\nafter = len(df)\nprint(f\"Removed {before - after} rows with no skills data\")\nprint(f\"Remaining rows: {after}\")\n\n\nRemoved 447 rows with no skills data\nRemaining rows: 56773\n\n\nWe are dropping these rows since we are going to be analyze the skills required for these jobs.\n\n\n5.3 REMOTE_TYPE_NAME AND SOFTWARE_SKILLS_NAME\n\n\nCode\ncategorical_fills = {\n    'REMOTE_TYPE_NAME': 'Not Specified',\n    'SOFTWARE_SKILLS_NAME': 'Not Listed'\n}\n\nfor col, fill_value in categorical_fills.items():\n    if col in df.columns:\n        df[col] = df[col].fillna(fill_value)\n\n\n\n\n5.4 MIN_YEARS_EXPERIENCE\n\n\nCode\n#  indicator for missing experience\ndf['EXPERIENCE_SPECIFIED'] = df['MIN_YEARS_EXPERIENCE'].notna().astype(int)\n\n\ndf['MIN_YEARS_EXPERIENCE'] = df['MIN_YEARS_EXPERIENCE'].fillna(0)\n\n\nWe are not dropping thes rows, instead we are using another column to specify whether the experience was given or not, this method helps us to preserve our data since there are a lot of data points where minimum experience is not specified."
  },
  {
    "objectID": "data_cleaning.html#table-order",
    "href": "data_cleaning.html#table-order",
    "title": "Data Cleaning & Preprocessing",
    "section": "6. Table Order",
    "text": "6. Table Order\n\n\nCode\ncolumn_order = [\n    'TITLE_NAME', 'COMPANY_NAME', 'POSTED', 'DURATION',\n    'SALARY', 'SALARY_FROM', 'SALARY_TO',\n    'STATE_NAME', 'CITY_NAME',\n    'REMOTE_TYPE_NAME', 'EMPLOYMENT_TYPE_NAME',\n    'MIN_YEARS_EXPERIENCE','EXPERIENCE_SPECIFIED', 'MIN_EDULEVELS_NAME',\n    'SKILLS_NAME', 'SOFTWARE_SKILLS_NAME',\n    'SOURCE_TYPES', 'LOT_V6_OCCUPATION_NAME', 'NAICS_2022_2_NAME'\n]\ndf = df[column_order]"
  },
  {
    "objectID": "data_cleaning.html#cleaned-dataset-summary",
    "href": "data_cleaning.html#cleaned-dataset-summary",
    "title": "Data Cleaning & Preprocessing",
    "section": "7. Cleaned Dataset summary",
    "text": "7. Cleaned Dataset summary\n\n\nCode\ndf.isna().sum()\n\n\nTITLE_NAME                0\nCOMPANY_NAME              0\nPOSTED                    0\nDURATION                  0\nSALARY                    0\nSALARY_FROM               0\nSALARY_TO                 0\nSTATE_NAME                0\nCITY_NAME                 0\nREMOTE_TYPE_NAME          0\nEMPLOYMENT_TYPE_NAME      0\nMIN_YEARS_EXPERIENCE      0\nEXPERIENCE_SPECIFIED      0\nMIN_EDULEVELS_NAME        0\nSKILLS_NAME               0\nSOFTWARE_SKILLS_NAME      0\nSOURCE_TYPES              0\nLOT_V6_OCCUPATION_NAME    0\nNAICS_2022_2_NAME         0\ndtype: int64\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nPOSTED\nDURATION\nSALARY\nSALARY_FROM\nSALARY_TO\nMIN_YEARS_EXPERIENCE\nEXPERIENCE_SPECIFIED\n\n\n\n\ncount\n56773\n56773.000000\n56773.000000\n56773.000000\n56773.000000\n56773.000000\n56773.000000\n\n\nmean\n2024-07-10 09:06:03.912423168\n35.069329\n111014.995050\n87708.293872\n128664.903229\n3.407623\n0.671270\n\n\nmin\n2024-05-01 00:00:00\n0.000000\n15860.000000\n10230.000000\n11148.000000\n0.000000\n0.000000\n\n\n25%\n2024-06-03 00:00:00\n14.000000\n95300.000000\n75600.000000\n107225.000000\n0.000000\n0.000000\n\n\n50%\n2024-07-09 00:00:00\n30.000000\n105000.000000\n81770.000000\n120750.000000\n3.000000\n1.000000\n\n\n75%\n2024-08-16 00:00:00\n60.000000\n125900.000000\n97875.000000\n150000.000000\n5.000000\n1.000000\n\n\nmax\n2024-09-30 00:00:00\n119.000000\n500000.000000\n800000.000000\n950000.000000\n15.000000\n1.000000\n\n\nstd\nNaN\n23.855804\n29957.364868\n26816.028140\n42261.703578\n3.491146\n0.469756\n\n\n\n\n\n\n\n\n\nCode\ndisplay(df.head())\n\ndf.reset_index(drop=True, inplace=True)\ndf.to_csv('data/lightcast_cleaned.csv', index=False)\n\n\n\n\n\n\n\n\n\nTITLE_NAME\nCOMPANY_NAME\nPOSTED\nDURATION\nSALARY\nSALARY_FROM\nSALARY_TO\nSTATE_NAME\nCITY_NAME\nREMOTE_TYPE_NAME\nEMPLOYMENT_TYPE_NAME\nMIN_YEARS_EXPERIENCE\nEXPERIENCE_SPECIFIED\nMIN_EDULEVELS_NAME\nSKILLS_NAME\nSOFTWARE_SKILLS_NAME\nSOURCE_TYPES\nLOT_V6_OCCUPATION_NAME\nNAICS_2022_2_NAME\n\n\n\n\n0\nEnterprise Analysts\nMurphy USA\n2024-06-02\n6\n125900.0\n97875.0\n150000.0\nArkansas\nEl Dorado, AR\nNot Specified\nFull-time (&gt; 32 hours)\n2.0\n1\nBachelor's degree\nMerchandising, Mathematics, Presentations, Pre...\nSQL (Programming Language), Power BI\nCompany\nBusiness Intelligence Analyst\nRetail Trade\n\n\n1\nOracle Consultants\nSmx Corporation Limited\n2024-06-02\n60\n125900.0\n97875.0\n150000.0\nMaine\nAugusta, ME\nRemote\nFull-time (&gt; 32 hours)\n3.0\n1\nNo Education Listed\nProcurement, Financial Statements, Oracle Busi...\nOracle Business Intelligence (BI) / OBIA, Orac...\nJob Board\nBusiness Intelligence Analyst\nAdministrative and Support and Waste Managemen...\n\n\n2\nData Analysts\nSedgwick\n2024-06-02\n35\n95300.0\n75600.0\n107225.0\nTexas\nDallas, TX\nNot Specified\nFull-time (&gt; 32 hours)\n5.0\n1\nBachelor's degree\nManagement, Exception Reporting, Report Writin...\nMicrosoft Office\nJob Board\nData / Data Mining Analyst\nFinance and Insurance\n\n\n3\nManagement Analysts\nWells Fargo\n2024-06-02\n48\n95300.0\n75600.0\n107225.0\nArizona\nPhoenix, AZ\nNot Specified\nFull-time (&gt; 32 hours)\n3.0\n1\nNo Education Listed\nExit Strategies, Reliability, User Story, Mana...\nSAS (Software), Google Cloud Platform (GCP)\nJob Board\nData / Data Mining Analyst\nFinance and Insurance\n\n\n5\nLead Data Analysts\nLumen Technologies\n2024-06-02\n10\n110155.0\n94420.0\n125890.0\nArkansas\n[Unknown City], AR\nRemote\nFull-time (&gt; 32 hours)\n0.0\n0\nBachelor's degree\nPower BI, Presentations, Data Reporting, Qlik ...\nPower BI, Qlik Sense (Data Analytics Software)...\nJob Board\nData / Data Mining Analyst\nInformation"
  }
]