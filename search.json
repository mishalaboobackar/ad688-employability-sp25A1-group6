[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies supervised ML models to the employability dataset.\nWe evaluate baseline, linear, and ensemble methods, and compare performance."
  },
  {
    "objectID": "ml_methods.html#loading-the-dataset-and-initial-inspection",
    "href": "ml_methods.html#loading-the-dataset-and-initial-inspection",
    "title": "Machine Learning Methods",
    "section": "Loading the Dataset and Initial Inspection",
    "text": "Loading the Dataset and Initial Inspection\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n    f1_score,\n    mean_squared_error,\n    r2_score,\n)\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n\n\n\n\nCode\n# Step 1 — Load Dataset and Initial Inspection\nimport pandas as pd\ndf = pd.read_csv(\"data/data_science_job_posts_2025.csv\")\ndf.columns\n\n\nIndex(['job_title', 'seniority_level', 'status', 'company', 'location',\n       'post_date', 'headquarter', 'industry', 'ownership', 'company_size',\n       'revenue', 'salary', 'skills'],\n      dtype='object')\n\n\n\n\nCode\ndf.info()\ndf.isna().mean().sort_values(ascending=False)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 944 entries, 0 to 943\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   job_title        941 non-null    object\n 1   seniority_level  884 non-null    object\n 2   status           688 non-null    object\n 3   company          944 non-null    object\n 4   location         942 non-null    object\n 5   post_date        944 non-null    object\n 6   headquarter      944 non-null    object\n 7   industry         944 non-null    object\n 8   ownership        897 non-null    object\n 9   company_size     944 non-null    object\n 10  revenue          929 non-null    object\n 11  salary           944 non-null    object\n 12  skills           944 non-null    object\ndtypes: object(13)\nmemory usage: 96.0+ KB\n\n\nstatus             0.271186\nseniority_level    0.063559\nownership          0.049788\nrevenue            0.015890\njob_title          0.003178\nlocation           0.002119\ncompany            0.000000\nheadquarter        0.000000\npost_date          0.000000\nindustry           0.000000\ncompany_size       0.000000\nsalary             0.000000\nskills             0.000000\ndtype: float64\n\n\n\n\nCode\ndf.head(10)\n\n\n\n\nCode\n# Step 2 – Create binary skill features (python, sql, spark, etc.)\n\nskill_keywords = [\n    \"python\", \"sql\", \"spark\", \"r\", \"scala\", \"docker\",\n    \"tableau\", \"powerbi\",\n    \"tensorflow\", \"pytorch\", \"machine learning\", \"deep learning\",\n    \"aws\", \"azure\", \"gcp\"\n]\n\nfor kw in skill_keywords:\n    col = \"skill_\" + kw.lower().replace(\" \", \"_\")\n    df[col] = df[\"skills\"].str.contains(kw, case=False, na=False).astype(int)\n\n\n\n\n\nCode\n# Check how many postings mention each skill\n\ndf.filter(like=\"skill_\").sum()\n\n\n\n\nCode\n# Step 3 — Create Binary Target: Is This an ML Role?\n\nml_keywords = [\n    \"machine learning\", \"deep learning\", \"neural network\",\n    \"tensorflow\", \"pytorch\", \"ai\", \"ml\"\n]\n\npattern = \"|\".join(ml_keywords)\n\ndf[\"is_ml_role\"] = df[\"skills\"].str.contains(pattern, case=False, na=False).astype(int)\n\ndf[\"is_ml_role\"].value_counts()\n\n\nis_ml_role\n1    603\n0    341\nName: count, dtype: int64\n\n\n\n\nCode\n# Step 4 – Clean salary column\n\nimport re\n\ndef parse_salary_range(s):\n    if pd.isna(s):\n        return (None, None)\n    \n    # Remove € and commas\n    s = s.replace(\"€\", \"\").replace(\",\", \"\").strip()\n    \n    # Match patterns like \"100000 – 200000\"\n    match = re.findall(r\"(\\d+)\", s)\n    \n    if len(match) == 2:\n        low, high = map(int, match)\n        return low, high\n    else:\n        return (None, None)\n\n# Apply to dataset\ndf[\"salary_min\"], df[\"salary_max\"] = zip(*df[\"salary\"].apply(parse_salary_range))\n\n# Average salary\ndf[\"salary_avg\"] = df[[\"salary_min\", \"salary_max\"]].mean(axis=1)\n\ndf[[\"salary_min\", \"salary_max\", \"salary_avg\"]].head()\n\n\n\n\n\n\n\n\n\nsalary_min\nsalary_max\nsalary_avg\n\n\n\n\n0\n100472.0\n200938.0\n150705.0\n\n\n1\nNaN\nNaN\nNaN\n\n\n2\n94987.0\n159559.0\n127273.0\n\n\n3\n112797.0\n194402.0\n153599.5\n\n\n4\n114172.0\n228337.0\n171254.5\n\n\n\n\n\n\n\n\n\nCode\n# Check missing salary values\ndf[[\"salary_min\", \"salary_max\", \"salary_avg\"]].isna().sum()\n\n\nThe dataset contained missing values across several fields such as seniority_level, status, salary, revenue, and ownership. Missing salary values were particularly common because many job postings did not disclose salary ranges. After transforming the salary column into numeric values (min, max, avg), these missing entries became explicit as NaN\n\n\nCode\ndf[\"seniority_level\"].value_counts(dropna=False)\ndf[\"status\"].value_counts(dropna=False)\ndf[\"industry\"].value_counts().head(20)"
  },
  {
    "objectID": "ml_methods.html#step-5b-encode-status-remote-hybrid-on-site",
    "href": "ml_methods.html#step-5b-encode-status-remote-hybrid-on-site",
    "title": "Machine Learning Methods",
    "section": "STEP 5B – Encode status (remote / hybrid / on-site)",
    "text": "STEP 5B – Encode status (remote / hybrid / on-site)\n\n\nCode\ndf[\"status\"].value_counts(dropna=False)\n\n\n\n\nCode\n# Replace missing with \"unknown\"\ndf[\"status\"] = df[\"status\"].fillna(\"unknown\").str.lower()\n\n# One-hot encode (drop_first=True to avoid multicollinearity)\nstatus_dummies = pd.get_dummies(df[\"status\"], prefix=\"status\", drop_first=True)\n\ndf = pd.concat([df, status_dummies], axis=1)\n\ndf[status_dummies.columns].head()\n\n\n\n\n\n\n\n\n\nstatus_on-site\nstatus_remote\nstatus_unknown\n\n\n\n\n0\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\n\n\n2\nTrue\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\n\n\n4\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\n\nCode\ndf[status_dummies.columns].sum()"
  },
  {
    "objectID": "ml_methods.html#step-5c-encode-industry",
    "href": "ml_methods.html#step-5c-encode-industry",
    "title": "Machine Learning Methods",
    "section": "STEP 5C — Encode Industry",
    "text": "STEP 5C — Encode Industry\n\n\nCode\n# Step 5C – One-hot encode industry\n\ndf[\"industry\"] = df[\"industry\"].str.lower()\n\n# One-hot encode (drop_first=True to avoid multicollinearity)\nindustry_dummies = pd.get_dummies(df[\"industry\"], prefix=\"industry\", drop_first=True)\n\ndf = pd.concat([df, industry_dummies], axis=1)\n\ndf[industry_dummies.columns].head()\n\n\n\n\n\n\n\n\n\nindustry_energy\nindustry_finance\nindustry_healthcare\nindustry_logistics\nindustry_manufacturing\nindustry_retail\nindustry_technology\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\nCode\nindustry_dummies.sum()"
  },
  {
    "objectID": "ml_methods.html#step-6-build-feature-matrix-x-for-clustering-and-regression",
    "href": "ml_methods.html#step-6-build-feature-matrix-x-for-clustering-and-regression",
    "title": "Machine Learning Methods",
    "section": "Step 6 – Build feature matrix X for clustering and regression",
    "text": "Step 6 – Build feature matrix X for clustering and regression\n\n\nCode\n## Step 6 – Build feature matrix X for clustering and regression\n\n# 1) Skill features\nskill_cols = [c for c in df.columns if c.startswith(\"skill_\")]\n\n# 2) Seniority dummies\nseniority_cols = [\n    c for c in df.columns\n    if c.startswith(\"seniority_\") and c != \"seniority_level\"\n]\n\n# 3) Status dummies\nstatus_cols = [c for c in df.columns if c.startswith(\"status_\")]\n\n# 4) Industry dummies\nindustry_cols = [c for c in df.columns if c.startswith(\"industry_\")]\n\n# Combine all features\nfeature_cols = skill_cols + seniority_cols + status_cols + industry_cols\n\nX = df[feature_cols]\n\nX.dtypes.head(), X.shape\n\n\n(skill_python    int64\n skill_sql       int64\n skill_spark     int64\n skill_r         int64\n skill_scala     int64\n dtype: object,\n (944, 29))"
  },
  {
    "objectID": "ml_methods.html#step-7-unsupervised-learning-k-means-clustering",
    "href": "ml_methods.html#step-7-unsupervised-learning-k-means-clustering",
    "title": "Machine Learning Methods",
    "section": "Step 7 – Unsupervised Learning: K-Means Clustering",
    "text": "Step 7 – Unsupervised Learning: K-Means Clustering\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled.shape\n\n\n(944, 29)\n\n\n\n\nCode\n#Elbow plot to determine optimal k\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ninertias = []\nK = range(2, 11)\n\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    km.fit(X_scaled)\n    inertias.append(km.inertia_)\n\nplt.figure(figsize=(6, 4))\nplt.plot(K, inertias, marker=\"o\")\nplt.xlabel(\"Number of clusters k\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Plot for K-Means\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe elbow plot shows that inertia decreases as k increases. While there is no sharp elbow, the reduction in inertia begins to slow around k = 4-5, suggesting these are reasonable choices for the number of clusters.\nAlthough the inertia curve suggested both 4 and 5 as reasonable options, the 5-cluster solution produced a very small cluster (7 observations), which limits interpretability. Therefore, we selected k = 4, ensuring all clusters are large, stable, and meaningful for analysis.\n\n\nCode\n\nk = 4\n\nkmeans = KMeans(n_clusters=k, random_state=42)\ndf[\"cluster\"] = kmeans.fit_predict(X_scaled)\n\ndf[\"cluster\"].value_counts()\n\n\ncluster\n1    481\n2    231\n0    152\n3     80\nName: count, dtype: int64\n\n\n\n\nCode\n# Inspect Cluster Centers (Interpretation)\ncluster_centers = pd.DataFrame(\n    kmeans.cluster_centers_,\n    columns=feature_cols\n)\ncluster_centers\n\n\n\n\n\n\n\n\n\nskill_python\nskill_sql\nskill_spark\nskill_r\nskill_scala\nskill_docker\nskill_tableau\nskill_powerbi\nskill_tensorflow\nskill_pytorch\n...\nstatus_on-site\nstatus_remote\nstatus_unknown\nindustry_energy\nindustry_finance\nindustry_healthcare\nindustry_logistics\nindustry_manufacturing\nindustry_retail\nindustry_technology\n\n\n\n\n0\n0.435763\n-0.107711\n0.246219\n0.600192\n-0.061745\n0.886853\n-0.113783\n-0.001042\n1.722443\n1.848682\n...\n-0.195402\n-0.139250\n0.189118\n-0.054745\n0.126303\n0.014766\n0.238615\n-0.086433\n-0.260650\n0.098612\n\n\n1\n0.382195\n0.399086\n-0.116272\n0.496535\n-0.256462\n-0.129941\n0.208305\n0.106971\n-0.410961\n-0.408324\n...\n0.008715\n-0.051076\n0.166290\n-0.002123\n0.050504\n0.056594\n-0.065233\n0.034732\n-0.181744\n0.044687\n\n\n2\n-1.311981\n-0.886285\n-0.453453\n-1.636700\n-0.314567\n-0.246321\n-0.321550\n-0.164935\n-0.460228\n-0.431196\n...\n-0.114144\n0.302699\n-0.376295\n0.041098\n-0.064414\n-0.050605\n0.001412\n0.014486\n-0.255236\n0.218863\n\n\n3\n0.662451\n0.364296\n1.540613\n0.600192\n2.567605\n-0.192495\n-0.107771\n-0.164935\n0.527171\n0.187628\n...\n0.648460\n-0.302372\n-0.272591\n-0.001891\n-0.357635\n-0.222201\n-0.065233\n-0.086433\n2.324966\n-1.088010\n\n\n\n\n4 rows × 29 columns\n\n\n\n\n\nCode\n# Display cluster centers as a heatmap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncluster_centers = pd.DataFrame(\n    kmeans.cluster_centers_,\n    columns=feature_cols,\n    index=[f\"Cluster {i}\" for i in range(k)]\n)\n\n# Create heatmap\nplt.figure(figsize=(14, 6))\nsns.heatmap(cluster_centers, annot=False, cmap=\"coolwarm\", center=0, linewidths=0.5)\nplt.title(\"K-Means Cluster Centers Heatmap\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Features\")\nplt.ylabel(\"Cluster\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n# Also display the dataframe\ndisplay(cluster_centers.round(3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskill_python\nskill_sql\nskill_spark\nskill_r\nskill_scala\nskill_docker\nskill_tableau\nskill_powerbi\nskill_tensorflow\nskill_pytorch\n...\nstatus_on-site\nstatus_remote\nstatus_unknown\nindustry_energy\nindustry_finance\nindustry_healthcare\nindustry_logistics\nindustry_manufacturing\nindustry_retail\nindustry_technology\n\n\n\n\nCluster 0\n0.436\n-0.108\n0.246\n0.600\n-0.062\n0.887\n-0.114\n-0.001\n1.722\n1.849\n...\n-0.195\n-0.139\n0.189\n-0.055\n0.126\n0.015\n0.239\n-0.086\n-0.261\n0.099\n\n\nCluster 1\n0.382\n0.399\n-0.116\n0.497\n-0.256\n-0.130\n0.208\n0.107\n-0.411\n-0.408\n...\n0.009\n-0.051\n0.166\n-0.002\n0.051\n0.057\n-0.065\n0.035\n-0.182\n0.045\n\n\nCluster 2\n-1.312\n-0.886\n-0.453\n-1.637\n-0.315\n-0.246\n-0.322\n-0.165\n-0.460\n-0.431\n...\n-0.114\n0.303\n-0.376\n0.041\n-0.064\n-0.051\n0.001\n0.014\n-0.255\n0.219\n\n\nCluster 3\n0.662\n0.364\n1.541\n0.600\n2.568\n-0.192\n-0.108\n-0.165\n0.527\n0.188\n...\n0.648\n-0.302\n-0.273\n-0.002\n-0.358\n-0.222\n-0.065\n-0.086\n2.325\n-1.088\n\n\n\n\n4 rows × 29 columns\n\n\n\nCluster 0 (n=152): Represents Deep Learning / AI Engineers, characterized by the highest TensorFlow and PyTorch scores, strong Docker usage, and elevated R proficiency. These roles emphasize neural network frameworks and containerization, appearing more often in logistics and less in retail. They tend toward unknown or hybrid work arrangements rather than strictly on-site positions.\nCluster 1 (n=481): Represents the largest cluster of general Data Analysts/Scientists with balanced skill profiles. These roles show moderate Python, SQL, R, and Tableau usage—typical of traditional analytics positions. They span multiple industries and seniority levels, indicating a broad category of data professionals without deep specialization in ML or big data engineering.\nCluster 2 (n=231): Consists of entry-level or non-technical data roles. These postings show the lowest scores across nearly all skills—Python, SQL, R, Tableau, and ML frameworks are all below average. They have the highest proportion of remote work and technology industry representation. The absence of clear technical requirements suggests these are general data-related, support, or junior positions rather than specialized roles.\nCluster 3 (n=80): Represents Big Data Engineers with the highest Spark and Scala scores by a significant margin. These specialized roles require distributed computing expertise and appear predominantly in the retail industry (very high positive value) while being underrepresented in technology and finance sectors. They are mostly on-site positions with senior-level requirements.\nThe purpose of K-means clustering is to uncover natural job families based on shared skill profiles, seniority, and industry characteristics. By grouping similar postings together without using salary, we can identify distinct job types and then compare their compensation and skill requirements in subsequent analysis.\n\n\nCode\ndf[\"cluster\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Number of Job Postings per Cluster\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# PCA for 2D visualization of clusters\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create scatter plot colored by cluster\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['cluster'], cmap='viridis', alpha=0.6)\nplt.colorbar(scatter, label='Cluster')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\nplt.title('K-Means Clusters Visualized with PCA')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total variance explained by 2 components: {sum(pca.explained_variance_ratio_):.1%}\")\n\n\n\n\n\n\n\n\n\nTotal variance explained by 2 components: 23.8%"
  },
  {
    "objectID": "ml_methods.html#supervised-learning-salary-prediction-using-lightcast-dataset",
    "href": "ml_methods.html#supervised-learning-salary-prediction-using-lightcast-dataset",
    "title": "Machine Learning Methods",
    "section": "Supervised Learning: Salary Prediction using Lightcast Dataset",
    "text": "Supervised Learning: Salary Prediction using Lightcast Dataset\nNow we switch to the lightcast_cleaned.csv dataset for supervised regression modeling.\nTarget Variable: SALARY (continuous)\nFeatures Selected: - Continuous Features: DURATION, MIN_YEARS_EXPERIENCE - Binary Feature: EXPERIENCE_SPECIFIED (indicator for whether experience was listed) - Categorical Features: REMOTE_TYPE_NAME, LOT_V6_OCCUPATION_NAME\nNote: We do NOT use SALARY_FROM or SALARY_TO as features because they are directly related to the target variable (SALARY is derived from them), which would cause data leakage.\nWe will: 1. Load and prepare the data 2. Check for multicollinearity using VIF 3. Encode categorical variables using OneHotEncoder 4. Train/Test split (70/30) 5. Build Polynomial Regression model with diagnostic plots 6. Build Random Forest Regression model with feature importance 7. Compare model performance\n\n\nCode\n# Load Lightcast Dataset and Select Features\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the lightcast dataset\ndf_lc = pd.read_csv('data/lightcast_cleaned.csv')\n\n\n\ncontinuous_features = ['DURATION', 'MIN_YEARS_EXPERIENCE']\nbinary_features = ['EXPERIENCE_SPECIFIED']\ncategorical_features = ['REMOTE_TYPE_NAME', 'LOT_V6_OCCUPATION_NAME']\ntarget = 'SALARY'\n\n# Create working dataframe\ndf_ml = df_lc[continuous_features + binary_features + categorical_features + [target]].copy()\n\nprint(\"Dataset Shape:\", df_ml.shape)\nprint(\"\\nNull values check:\")\nprint(df_ml.isnull().sum())\nprint(\"\\nFeature Statistics:\")\ndisplay(df_ml.describe())\n\n\nDataset Shape: (47513, 6)\n\nNull values check:\nDURATION                  0\nMIN_YEARS_EXPERIENCE      0\nEXPERIENCE_SPECIFIED      0\nREMOTE_TYPE_NAME          0\nLOT_V6_OCCUPATION_NAME    0\nSALARY                    0\ndtype: int64\n\nFeature Statistics:\n\n\n\n\n\n\n\n\n\nDURATION\nMIN_YEARS_EXPERIENCE\nEXPERIENCE_SPECIFIED\nSALARY\n\n\n\n\ncount\n47513.000000\n47513.000000\n47513.000000\n47513.000000\n\n\nmean\n35.637426\n3.494580\n0.692400\n111674.426936\n\n\nstd\n24.027774\n3.464785\n0.461505\n30031.592449\n\n\nmin\n0.000000\n0.000000\n0.000000\n20583.000000\n\n\n25%\n14.000000\n0.000000\n0.000000\n96008.000000\n\n\n50%\n31.000000\n3.000000\n1.000000\n105000.000000\n\n\n75%\n60.000000\n5.000000\n1.000000\n125900.000000\n\n\nmax\n119.000000\n15.000000\n1.000000\n500000.000000\n\n\n\n\n\n\n\n\n\nCode\n# One-Hot Encoding for Categorical Variables\n\n# One-hot encode categorical features\nremote_dummies = pd.get_dummies(df_ml['REMOTE_TYPE_NAME'], prefix='remote', drop_first=True)\noccupation_dummies = pd.get_dummies(df_ml['LOT_V6_OCCUPATION_NAME'], prefix='occupation', drop_first=True)\n\nprint(\"Categorical Features:\")\nprint(f\"  REMOTE_TYPE_NAME categories: {df_ml['REMOTE_TYPE_NAME'].nunique()}\")\nprint(f\"  LOT_V6_OCCUPATION_NAME categories: {df_ml['LOT_V6_OCCUPATION_NAME'].nunique()}\")\n\nprint(\"\\nOne-Hot Encoded Remote Type columns:\")\nprint(remote_dummies.columns.tolist())\nprint(\"\\nOne-Hot Encoded Occupation columns:\")\nprint(occupation_dummies.columns.tolist())\n\n# Combine all features: continuous + binary + categorical (one-hot encoded)\nX_continuous = df_ml[continuous_features].values\nX_binary = df_ml[binary_features].values\nX_categorical = pd.concat([remote_dummies, occupation_dummies], axis=1).values\n\n# Full feature matrix\nX = np.hstack([X_continuous, X_binary, X_categorical])\ny = df_ml[target].values\n\n# Feature names for later use\nfeature_names = continuous_features + binary_features + remote_dummies.columns.tolist() + occupation_dummies.columns.tolist()\n\nprint(f\"\\nTotal features: {len(feature_names)}\")\nprint(f\"Feature names: {feature_names}\")\nprint(f\"\\nX shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n\n\nCategorical Features:\n  REMOTE_TYPE_NAME categories: 4\n  LOT_V6_OCCUPATION_NAME categories: 4\n\nOne-Hot Encoded Remote Type columns:\n['remote_Not Remote', 'remote_Not Specified', 'remote_Remote']\n\nOne-Hot Encoded Occupation columns:\n['occupation_Business Intelligence Analyst', 'occupation_Data / Data Mining Analyst', 'occupation_Market Research Analyst']\n\nTotal features: 9\nFeature names: ['DURATION', 'MIN_YEARS_EXPERIENCE', 'EXPERIENCE_SPECIFIED', 'remote_Not Remote', 'remote_Not Specified', 'remote_Remote', 'occupation_Business Intelligence Analyst', 'occupation_Data / Data Mining Analyst', 'occupation_Market Research Analyst']\n\nX shape: (47513, 9)\ny shape: (47513,)\n\n\n\n\nCode\n# Train/Test Split (70/30)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.3, \n    random_state=42\n)\n\nprint(f\"Training set size: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Test set size: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"\\nTraining features shape: {X_train.shape}\")\nprint(f\"Test features shape: {X_test.shape}\")\n\n\nTraining set size: 33259 (70.0%)\nTest set size: 14254 (30.0%)\n\nTraining features shape: (33259, 9)\nTest features shape: (14254, 9)\n\n\n\n\nCode\n# Polynomial Regression Model (Degree 2)\n\n# Create polynomial features (degree=2)\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)\n\nprint(f\"Original features: {X_train.shape[1]}\")\nprint(f\"Polynomial features (degree=2): {X_train_poly.shape[1]}\")\n\n# Scale features for better convergence\nscaler = StandardScaler()\nX_train_poly_scaled = scaler.fit_transform(X_train_poly)\nX_test_poly_scaled = scaler.transform(X_test_poly)\n\n# Fit Linear Regression on polynomial features\npoly_reg = LinearRegression()\npoly_reg.fit(X_train_poly_scaled, y_train)\n\n# Predictions\ny_train_pred_poly = poly_reg.predict(X_train_poly_scaled)\ny_test_pred_poly = poly_reg.predict(X_test_poly_scaled)\n\n# Metrics\ntrain_r2_poly = r2_score(y_train, y_train_pred_poly)\ntest_r2_poly = r2_score(y_test, y_test_pred_poly)\ntrain_rmse_poly = np.sqrt(mean_squared_error(y_train, y_train_pred_poly))\ntest_rmse_poly = np.sqrt(mean_squared_error(y_test, y_test_pred_poly))\ntest_mae_poly = mean_absolute_error(y_test, y_test_pred_poly)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"POLYNOMIAL REGRESSION (Degree=2) RESULTS\")\nprint(\"=\"*50)\nprint(f\"Training R²:  {train_r2_poly:.4f}\")\nprint(f\"Test R²:      {test_r2_poly:.4f}\")\nprint(f\"Training RMSE: ${train_rmse_poly:,.2f}\")\nprint(f\"Test RMSE:     ${test_rmse_poly:,.2f}\")\nprint(f\"Test MAE:      ${test_mae_poly:,.2f}\")\n\n\nOriginal features: 9\nPolynomial features (degree=2): 54\n\n==================================================\nPOLYNOMIAL REGRESSION (Degree=2) RESULTS\n==================================================\nTraining R²:  0.2878\nTest R²:      0.2941\nTraining RMSE: $25,248.76\nTest RMSE:     $25,452.27\nTest MAE:      $15,082.37\n\n==================================================\nPOLYNOMIAL REGRESSION (Degree=2) RESULTS\n==================================================\nTraining R²:  0.2878\nTest R²:      0.2941\nTraining RMSE: $25,248.76\nTest RMSE:     $25,452.27\nTest MAE:      $15,082.37\n\n\n\n\nCode\n# Statsmodels OLS Summary for Polynomial Regression\n# Using original features (not polynomial) for interpretable summary\n\nX_train_sm = sm.add_constant(X_train)\nX_test_sm = sm.add_constant(X_test)\n\n# Fit OLS model\nols_model = sm.OLS(y_train, X_train_sm).fit()\n\n# Print summary\nprint(\"=\"*70)\nprint(\"OLS REGRESSION SUMMARY (Original Features)\")\nprint(\"=\"*70)\nprint(ols_model.summary())\n\n# Coefficient interpretation\nprint(\"\\n\" + \"=\"*50)\nprint(\"COEFFICIENT INTERPRETATION\")\nprint(\"=\"*50)\ncoef_df = pd.DataFrame({\n    'Feature': ['Intercept'] + feature_names,\n    'Coefficient': ols_model.params,\n    'Std Error': ols_model.bse,\n    't-value': ols_model.tvalues,\n    'p-value': ols_model.pvalues\n})\ncoef_df['Significant'] = coef_df['p-value'].apply(lambda x: '***' if x &lt; 0.001 else ('**' if x &lt; 0.01 else ('*' if x &lt; 0.05 else '')))\nprint(coef_df.to_string(index=False))\n\n\n======================================================================\nOLS REGRESSION SUMMARY (Original Features)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.270\nModel:                            OLS   Adj. R-squared:                  0.270\nMethod:                 Least Squares   F-statistic:                     1369.\nDate:                Mon, 08 Dec 2025   Prob (F-statistic):               0.00\nTime:                        04:05:22   Log-Likelihood:            -3.8472e+05\nNo. Observations:               33259   AIC:                         7.695e+05\nDf Residuals:                   33249   BIC:                         7.696e+05\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        9.45e+04   1003.165     94.202      0.000    9.25e+04    9.65e+04\nx1            35.0960      5.840      6.010      0.000      23.650      46.542\nx2          2824.0426     56.886     49.644      0.000    2712.544    2935.541\nx3         -9442.4694    416.259    -22.684      0.000   -1.03e+04   -8626.587\nx4           543.4413   1445.539      0.376      0.707   -2289.867    3376.750\nx5          2996.1671    808.504      3.706      0.000    1411.471    4580.863\nx6          3802.3762    863.543      4.403      0.000    2109.801    5494.952\nx7          2.222e+04    594.218     37.397      0.000    2.11e+04    2.34e+04\nx8         -1138.9932    586.574     -1.942      0.052   -2288.699      10.713\nx9         -3049.2895   2842.824     -1.073      0.283   -8621.325    2522.746\n==============================================================================\nOmnibus:                    11755.633   Durbin-Watson:                   2.006\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           130426.482\nSkew:                           1.380   Prob(JB):                         0.00\nKurtosis:                      12.300   Cond. No.                         877.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n==================================================\nCOEFFICIENT INTERPRETATION\n==================================================\n                                 Feature  Coefficient   Std Error    t-value       p-value Significant\n                               Intercept 94500.226843 1003.165117  94.202066  0.000000e+00         ***\n                                DURATION    35.096029    5.839908   6.009689  1.878110e-09         ***\n                    MIN_YEARS_EXPERIENCE  2824.042627   56.886011  49.643886  0.000000e+00         ***\n                    EXPERIENCE_SPECIFIED -9442.469423  416.259097 -22.684115 4.648020e-113         ***\n                       remote_Not Remote   543.441278 1445.539477   0.375944  7.069612e-01            \n                    remote_Not Specified  2996.167085  808.503848   3.705817  2.110521e-04         ***\n                           remote_Remote  3802.376196  863.543379   4.403225  1.069856e-05         ***\noccupation_Business Intelligence Analyst 22222.111449  594.218240  37.397222 7.234862e-300         ***\n   occupation_Data / Data Mining Analyst -1138.993175  586.574238  -1.941772  5.217320e-02            \n      occupation_Market Research Analyst -3049.289490 2842.824196  -1.072627  2.834464e-01"
  },
  {
    "objectID": "ml_methods.html#diagnostic-plots-for-polynomial-regression",
    "href": "ml_methods.html#diagnostic-plots-for-polynomial-regression",
    "title": "Machine Learning Methods",
    "section": "Diagnostic Plots for Polynomial Regression",
    "text": "Diagnostic Plots for Polynomial Regression\n\n\nCode\n# Diagnostic Plots for Polynomial Regression\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 1. Actual vs Predicted Plot\nax1 = axes[0, 0]\nax1.scatter(y_test, y_test_pred_poly, alpha=0.3, edgecolors='none', s=20)\nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\nax1.set_xlabel('Actual Salary ($)', fontsize=12)\nax1.set_ylabel('Predicted Salary ($)', fontsize=12)\nax1.set_title('Actual vs Predicted Salary (Polynomial Regression)', fontsize=14)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Residuals vs Predicted Plot\nresiduals_poly = y_test - y_test_pred_poly\nax2 = axes[0, 1]\nax2.scatter(y_test_pred_poly, residuals_poly, alpha=0.3, edgecolors='none', s=20)\nax2.axhline(y=0, color='r', linestyle='--', lw=2)\nax2.set_xlabel('Predicted Salary ($)', fontsize=12)\nax2.set_ylabel('Residuals ($)', fontsize=12)\nax2.set_title('Residuals vs Predicted Values', fontsize=14)\nax2.grid(True, alpha=0.3)\n\n# 3. Residual Distribution (Histogram)\nax3 = axes[1, 0]\nax3.hist(residuals_poly, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\nax3.axvline(x=0, color='r', linestyle='--', lw=2)\nax3.set_xlabel('Residuals ($)', fontsize=12)\nax3.set_ylabel('Frequency', fontsize=12)\nax3.set_title('Distribution of Residuals', fontsize=14)\nax3.grid(True, alpha=0.3)\n\n# 4. Q-Q Plot for Normality\nfrom scipy import stats\nax4 = axes[1, 1]\nstats.probplot(residuals_poly, dist=\"norm\", plot=ax4)\nax4.set_title('Q-Q Plot (Normality Check)', fontsize=14)\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('figures/poly_regression_diagnostics.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Print residual statistics\nprint(\"\\nResidual Statistics:\")\nprint(f\"Mean Residual: ${np.mean(residuals_poly):,.2f}\")\nprint(f\"Std Residual:  ${np.std(residuals_poly):,.2f}\")\nprint(f\"Min Residual:  ${np.min(residuals_poly):,.2f}\")\nprint(f\"Max Residual:  ${np.max(residuals_poly):,.2f}\")\n\n\n\n\n\n\n\n\n\n\nResidual Statistics:\nMean Residual: $90.74\nStd Residual:  $25,452.11\nMin Residual:  $-105,048.16\nMax Residual:  $375,292.37"
  },
  {
    "objectID": "ml_methods.html#random-forest-regression-model",
    "href": "ml_methods.html#random-forest-regression-model",
    "title": "Machine Learning Methods",
    "section": "Random Forest Regression Model",
    "text": "Random Forest Regression Model\n\n\nCode\n# Random Forest Regression Model\n\n# Build Random Forest with hyperparameter tuning\nrf_model = RandomForestRegressor(\n    n_estimators=200,        # Number of trees\n    max_depth=15,            # Maximum depth of trees\n    min_samples_split=10,    # Minimum samples to split a node\n    min_samples_leaf=5,      # Minimum samples in leaf node\n    max_features='sqrt',     # Number of features to consider for best split\n    random_state=42,\n    n_jobs=-1                # Use all CPU cores\n)\n\n# Fit the model (using original features, not polynomial)\nrf_model.fit(X_train, y_train)\n\n# Predictions\ny_train_pred_rf = rf_model.predict(X_train)\ny_test_pred_rf = rf_model.predict(X_test)\n\n# Metrics\ntrain_r2_rf = r2_score(y_train, y_train_pred_rf)\ntest_r2_rf = r2_score(y_test, y_test_pred_rf)\ntrain_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\ntest_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\ntest_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n\nprint(\"=\"*50)\nprint(\"RANDOM FOREST REGRESSION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Number of Trees: {rf_model.n_estimators}\")\nprint(f\"Max Depth: {rf_model.max_depth}\")\nprint(f\"\\nTraining R²:  {train_r2_rf:.4f}\")\nprint(f\"Test R²:      {test_r2_rf:.4f}\")\nprint(f\"Training RMSE: ${train_rmse_rf:,.2f}\")\nprint(f\"Test RMSE:     ${test_rmse_rf:,.2f}\")\nprint(f\"Test MAE:      ${test_mae_rf:,.2f}\")\n\n\n==================================================\nRANDOM FOREST REGRESSION RESULTS\n==================================================\nNumber of Trees: 200\nMax Depth: 15\n\nTraining R²:  0.4100\nTest R²:      0.3541\nTraining RMSE: $22,980.07\nTest RMSE:     $24,347.07\nTest MAE:      $14,509.18\n\n\n\n\nCode\n# Feature Importance Plot for Random Forest\n\n# Get feature importances\nimportances = rf_model.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Create feature importance dataframe\nimportance_df = pd.DataFrame({\n    'Feature': [feature_names[i] for i in indices],\n    'Importance': importances[indices]\n})\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 8))\ncolors = plt.cm.viridis(np.linspace(0.2, 0.8, len(feature_names)))\n\nbars = ax.barh(range(len(feature_names)), importance_df['Importance'], color=colors)\nax.set_yticks(range(len(feature_names)))\nax.set_yticklabels(importance_df['Feature'])\nax.invert_yaxis()  # Top feature at top\nax.set_xlabel('Feature Importance', fontsize=12)\nax.set_title('Random Forest Feature Importance', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3, axis='x')\n\n# Add value labels\nfor i, (bar, imp) in enumerate(zip(bars, importance_df['Importance'])):\n    ax.text(imp + 0.005, i, f'{imp:.3f}', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.savefig('figures/rf_feature_importance.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nFeature Importance Ranking:\")\nfor i, row in importance_df.iterrows():\n    print(f\"  {i+1}. {row['Feature']}: {row['Importance']:.4f}\")\n\n\n\n\n\n\n\n\n\n\nFeature Importance Ranking:\n  1. occupation_Business Intelligence Analyst: 0.2990\n  2. MIN_YEARS_EXPERIENCE: 0.2632\n  3. DURATION: 0.2339\n  4. occupation_Data / Data Mining Analyst: 0.1703\n  5. EXPERIENCE_SPECIFIED: 0.0135\n  6. remote_Remote: 0.0087\n  7. remote_Not Specified: 0.0081\n  8. remote_Not Remote: 0.0024\n  9. occupation_Market Research Analyst: 0.0008"
  },
  {
    "objectID": "ml_methods.html#model-comparison-polynomial-regression-vs-random-forest",
    "href": "ml_methods.html#model-comparison-polynomial-regression-vs-random-forest",
    "title": "Machine Learning Methods",
    "section": "Model Comparison: Polynomial Regression vs Random Forest",
    "text": "Model Comparison: Polynomial Regression vs Random Forest\n\n\nCode\n# Model Comparison\n\n# Create comparison dataframe\ncomparison_df = pd.DataFrame({\n    'Metric': ['Training R²', 'Test R²', 'Training RMSE ($)', 'Test RMSE ($)', 'Test MAE ($)'],\n    'Polynomial Regression': [\n        f\"{train_r2_poly:.4f}\",\n        f\"{test_r2_poly:.4f}\",\n        f\"${train_rmse_poly:,.2f}\",\n        f\"${test_rmse_poly:,.2f}\",\n        f\"${test_mae_poly:,.2f}\"\n    ],\n    'Random Forest': [\n        f\"{train_r2_rf:.4f}\",\n        f\"{test_r2_rf:.4f}\",\n        f\"${train_rmse_rf:,.2f}\",\n        f\"${test_rmse_rf:,.2f}\",\n        f\"${test_mae_rf:,.2f}\"\n    ]\n})\n\nprint(\"=\"*70)\nprint(\"MODEL COMPARISON: POLYNOMIAL REGRESSION vs RANDOM FOREST\")\nprint(\"=\"*70)\nprint(comparison_df.to_string(index=False))\n\n# Visual comparison\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# 1. R² Comparison\nax1 = axes[0]\nmodels = ['Polynomial\\nRegression', 'Random\\nForest']\ntrain_r2 = [train_r2_poly, train_r2_rf]\ntest_r2 = [test_r2_poly, test_r2_rf]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nbars1 = ax1.bar(x - width/2, train_r2, width, label='Training R²', color='steelblue')\nbars2 = ax1.bar(x + width/2, test_r2, width, label='Test R²', color='coral')\n\nax1.set_ylabel('R² Score', fontsize=12)\nax1.set_title('R² Comparison', fontsize=14, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(models)\nax1.legend()\nax1.set_ylim(0, 1)\nax1.grid(True, alpha=0.3, axis='y')\n\nfor bar in bars1 + bars2:\n    height = bar.get_height()\n    ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n\n# 2. RMSE Comparison\nax2 = axes[1]\ntrain_rmse = [train_rmse_poly, train_rmse_rf]\ntest_rmse = [test_rmse_poly, test_rmse_rf]\n\nbars3 = ax2.bar(x - width/2, train_rmse, width, label='Training RMSE', color='steelblue')\nbars4 = ax2.bar(x + width/2, test_rmse, width, label='Test RMSE', color='coral')\n\nax2.set_ylabel('RMSE ($)', fontsize=12)\nax2.set_title('RMSE Comparison', fontsize=14, fontweight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(models)\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\n\nfor bar in bars3 + bars4:\n    height = bar.get_height()\n    ax2.annotate(f'${height:,.0f}', xy=(bar.get_x() + bar.get_width()/2, height),\n                 xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n\n# 3. Actual vs Predicted for both models\nax3 = axes[2]\nax3.scatter(y_test, y_test_pred_poly, alpha=0.3, label='Polynomial Reg', color='steelblue', s=15)\nax3.scatter(y_test, y_test_pred_rf, alpha=0.3, label='Random Forest', color='coral', s=15)\nax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect')\nax3.set_xlabel('Actual Salary ($)', fontsize=12)\nax3.set_ylabel('Predicted Salary ($)', fontsize=12)\nax3.set_title('Actual vs Predicted (Both Models)', fontsize=14, fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('figures/model_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n\n======================================================================\nMODEL COMPARISON: POLYNOMIAL REGRESSION vs RANDOM FOREST\n======================================================================\n           Metric Polynomial Regression Random Forest\n      Training R²                0.2878        0.4100\n          Test R²                0.2941        0.3541\nTraining RMSE ($)            $25,248.76    $22,980.07\n    Test RMSE ($)            $25,452.27    $24,347.07\n     Test MAE ($)            $15,082.37    $14,509.18"
  },
  {
    "objectID": "ml_methods.html#model-conclusions",
    "href": "ml_methods.html#model-conclusions",
    "title": "Machine Learning Methods",
    "section": "Model Conclusions",
    "text": "Model Conclusions\n\nKey Findings:\nPolynomial Regression: - Creates polynomial features from the original features, resulting in many interaction and squared terms - Provides interpretable coefficients through the OLS summary - Can capture non-linear relationships through polynomial terms - May overfit with too high a degree\nRandom Forest: - Ensemble method that builds multiple decision trees - Handles non-linear relationships naturally - Provides feature importance rankings - More robust to outliers and doesn’t require feature scaling\nFeature Importance Insights: - DURATION and MIN_YEARS_EXPERIENCE are the key continuous predictors - EXPERIENCE_SPECIFIED indicates whether experience requirements were listed - Occupation type and remote work status also influence salary levels\nNote on Feature Selection: We intentionally did NOT use SALARY_FROM or SALARY_TO as features because they are directly related to the target variable SALARY (which is typically the midpoint of these values). Using them would cause data leakage and artificially inflate model performance.\nRecommendation: Based on the R² and RMSE metrics, the model with lower test RMSE and higher test R² generalizes better to unseen data. Random Forest typically handles complex interactions better, while Polynomial Regression provides more interpretable results for stakeholders."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "",
    "text": "Code\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook\"\nResearch Question: What does the job market look like for Business Analytics, Data Science, and ML professionals in 2024?\nWhat we’re Looking into: 1. Who’s hiring? → Top industries (barplot) and companies (treemap) 2. What roles exist? → Job titles within our occupation categories 3. What do they say? → Word cloud from job descriptions 4. What skills do they want? → Radar charts for each occupation + software skills barplot 5. What drives salary? → Salary by remote work type 6. Where are jobs posted? → Source types analysis\nEach insight builds toward our ML modeling decisions.\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport re\n\npd.set_option('display.max_columns', None)\n\ndf = pd.read_csv('data/lightcast_cleaned.csv')\ndf['POSTED'] = pd.to_datetime(df['POSTED'])\n\n\n\nprint(f\"Dataset: {len(df):,} job postings\")\nprint(f\"Date range: {df['POSTED'].min().strftime('%b %Y')} - {df['POSTED'].max().strftime('%b %Y')}\")\nprint(f\"Occupations: {df['LOT_V6_OCCUPATION_NAME'].nunique()}\")\n\n\nDataset: 47,513 job postings\nDate range: May 2024 - Sep 2024\nOccupations: 4"
  },
  {
    "objectID": "eda.html#whos-hiring",
    "href": "eda.html#whos-hiring",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Who’s Hiring?",
    "text": "Who’s Hiring?\n\n\nCode\n# Top 5 Industries - Horizontal Bar Plot\nindustry_counts = df['NAICS_2022_2_NAME'].value_counts().head(5)\n\n# Shorten long industry names\nindustry_short = {\n    'Professional, Scientific, and Technical Services': 'Professional Services',\n    'Administrative and Support and Waste Management and Remediation Services': 'Admin & Support Services',\n    'Management of Companies and Enterprises': 'Management & Enterprises',\n    'Finance and Insurance': 'Finance & Insurance',\n    'Information': 'Information',\n    'Health Care and Social Assistance': 'Healthcare & Social',\n    'Manufacturing': 'Manufacturing',\n    'Retail Trade': 'Retail Trade',\n    'Educational Services': 'Educational Services',\n    'Wholesale Trade': 'Wholesale Trade'\n}\nshort_names = [industry_short.get(name, name[:30]) for name in industry_counts.index]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    y=short_names,\n    x=industry_counts.values,\n    orientation='h',\n    marker_color=px.colors.qualitative.Set2[:5],\n    text=industry_counts.values,\n    textposition='outside'\n))\n\nfig.update_layout(\n    title='Top 5 Industries Hiring Data Professionals',\n    xaxis_title='Number of Job Postings',\n    yaxis={'categoryorder': 'total ascending'},\n    template='plotly_white',\n    height=400,\n    margin=dict(l=180, r=100),\n    xaxis=dict(range=[0, industry_counts.values.max() * 1.15])\n)\nfig.write_image('figures/top_industries.png', scale=2)\nfig.show()\n\n# Top 10 Companies - Vertical Bar Plot with colors\ncompany_counts = df['COMPANY_NAME'].value_counts().head(10).reset_index()\ncompany_counts.columns = ['Company', 'Postings']\n\nfig2 = go.Figure()\nfig2.add_trace(go.Bar(\n    x=company_counts['Company'],\n    y=company_counts['Postings'],\n    marker_color=px.colors.qualitative.Plotly[:10],\n    text=company_counts['Postings'],\n    textposition='outside'\n))\n\nfig2.update_layout(\n    title='Top 10 Companies Hiring Data Professionals',\n    xaxis_title='Company',\n    yaxis_title='Number of Job Postings',\n    template='plotly_white',\n    height=550,\n    xaxis_tickangle=-45,\n    margin=dict(b=150, t=80),\n    yaxis=dict(range=[0, company_counts['Postings'].max() * 1.15])\n)\nfig2.write_image('figures/top_companies.png', scale=2)\nfig2.show()\n\nprint(\"\\nTop 10 Companies by Job Postings:\")\nfor i, row in company_counts.iterrows():\n    pct = row['Postings'] / len(df) * 100\n    print(f\"  {i+1}. {row['Company']}: {row['Postings']:,} postings ({pct:.1f}%)\")\n\n\n                                                    \n\n\n                                                    \n\n\n\nTop 10 Companies by Job Postings:\n  1. Deloitte: 2,271 postings (4.8%)\n  2. Accenture: 1,316 postings (2.8%)\n  3. PricewaterhouseCoopers: 697 postings (1.5%)\n  4. Insight Global: 362 postings (0.8%)\n  5. Cardinal Health: 346 postings (0.7%)\n  6. Chewy: 319 postings (0.7%)\n  7. Smx Corporation Limited: 317 postings (0.7%)\n  8. Oracle: 311 postings (0.7%)\n  9. Robert Half: 309 postings (0.7%)\n  10. Randstad: 285 postings (0.6%)"
  },
  {
    "objectID": "eda.html#what-roles-can-you-go-for",
    "href": "eda.html#what-roles-can-you-go-for",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "What Roles can you Go for?",
    "text": "What Roles can you Go for?\n\n\nCode\n# Top 5 Job Titles per Occupation Category\noccupations = df['LOT_V6_OCCUPATION_NAME'].unique()\n\n# Shorten occupation names for subplot titles\nocc_short_titles = {\n    'Data / Data Mining Analyst': 'Data/Data Mining',\n    'Business Intelligence Analyst': 'Business Intelligence', \n    'Business / Management Analyst': 'Business/Management',\n    'Market Research Analyst': 'Market Research'\n}\n\ncolors = {'Data / Data Mining Analyst': '#1f77b4', \n          'Business Intelligence Analyst': '#ff7f0e', \n          'Business / Management Analyst': '#2ca02c', \n          'Market Research Analyst': '#d62728'}\n\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=[occ_short_titles.get(occ, occ) for occ in occupations],\n    horizontal_spacing=0.35,\n    vertical_spacing=0.30\n)\n\nfor idx, occ in enumerate(occupations):\n    row = idx // 2 + 1\n    col = idx % 2 + 1\n    \n    df_occ = df[df['LOT_V6_OCCUPATION_NAME'] == occ]\n    df_occ = df_occ[~df_occ['TITLE_NAME'].str.contains('Unclassified', case=False, na=False)]\n    top_titles = df_occ['TITLE_NAME'].value_counts().head(5)\n    \n    # Shorten titles\n    truncated_titles = [t[:20] + '...' if len(t) &gt; 20 else t for t in top_titles.index]\n    \n    fig.add_trace(\n        go.Bar(\n            y=truncated_titles,\n            x=top_titles.values,\n            orientation='h',\n            marker_color=colors.get(occ, '#636EFA'),\n            text=top_titles.values,\n            textposition='outside',\n            name=occ,\n            showlegend=False\n        ),\n        row=row, col=col\n    )\n    \n    # Set individual x-axis range based on each subplot's max value\n    subplot_max = top_titles.values.max()\n    fig.update_xaxes(range=[0, subplot_max * 1.25], title_text=\"Count\", row=row, col=col)\n    fig.update_yaxes(categoryorder='total ascending', tickfont=dict(size=9), row=row, col=col)\n\nfig.update_layout(\n    title_text='Top 5 Job Titles per Occupation Category',\n    height=700,\n    template='plotly_white',\n    margin=dict(l=160, r=60, t=80, b=40)\n)\nfig.write_image('figures/job_titles_by_occupation.png', scale=2)\nfig.show()\n\nprint(\"\\nPostings per Occupation:\")\nfor occ, count in df['LOT_V6_OCCUPATION_NAME'].value_counts().items():\n    print(f\"  • {occ}: {count:,} ({count/len(df)*100:.1f}%)\")\n\n\n                                                    \n\n\n\nPostings per Occupation:\n  • Data / Data Mining Analyst: 22,672 (47.7%)\n  • Business Intelligence Analyst: 21,640 (45.5%)\n  • Business / Management Analyst: 3,083 (6.5%)\n  • Market Research Analyst: 118 (0.2%)"
  },
  {
    "objectID": "eda.html#wordcloud-of-job-descriptions",
    "href": "eda.html#wordcloud-of-job-descriptions",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Wordcloud of Job Descriptions",
    "text": "Wordcloud of Job Descriptions\n\n\nCode\n# Using a sample of 3000 rows to prevent crashes\n\ncustom_stopwords = {\n    'the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this',\n    'with', 'are', 'be', 'as', 'at', 'from', 'or', 'an', 'will', 'your', 'you',\n    'we', 'our', 'have', 'has', 'it', 'their', 'all', 'can', 'been', 'would',\n    'who', 'more', 'if', 'about', 'which', 'when', 'what', 'into', 'also',\n    'may', 'other', 'its', 'than', 'should', 'such', 'any', 'these', 'only',\n    'new', 'well', 'them', 'they', 'but', 'not', 'do', 'up', 'out', 'so',\n    'job', 'position', 'apply', 'applicant', 'employer', 'employment',\n    'equal', 'opportunity', 'eeo', 'affirmative', 'action', 'disability',\n    'race', 'color', 'religion', 'sex', 'national', 'origin', 'age',\n    'status', 'protected', 'discrimination', 'including', 'without', 'regard',\n    'com', 'www', 'http', 'https', 'click', 'here', 'learn', 'please', 'contact',\n    'must', 'work', 'working', 'experience', 'years', 'year', 'required',\n    'requirements', 'skills', 'ability', 'strong', 'excellent', 'good',\n    'team', 'company', 'business', 'including', 'within', 'across', 'using'\n}\n\ndf_sample = df.sample(n=min(3000, len(df)), random_state=42)\nbody_text = ' '.join(df_sample['BODY'].dropna().astype(str).tolist())\n\nbody_text = re.sub(r'[^a-zA-Z\\s]', ' ', body_text.lower())\nbody_text = re.sub(r'\\s+', ' ', body_text)\n\nwordcloud = WordCloud(\n    width=1200, \n    height=600,\n    background_color='white',\n    stopwords=custom_stopwords,\n    max_words=100,\n    colormap='viridis',\n    collocations=False,\n    random_state=42\n).generate(body_text)\n\nplt.figure(figsize=(14, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud: Common Terms in Job Descriptions', fontsize=18, fontweight='bold', pad=15)\nplt.tight_layout()\nplt.savefig('figures/job_description_wordcloud.png', dpi=200, bbox_inches='tight')\nplt.show()\n\nprint(f\"Word cloud generated from {len(df_sample):,} sampled job postings\")\n\n\n\n\n\n\n\n\n\nWord cloud generated from 3,000 sampled job postings"
  },
  {
    "objectID": "eda.html#skills-to-look-for",
    "href": "eda.html#skills-to-look-for",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Skills to Look For",
    "text": "Skills to Look For\n\n\nCode\ndef extract_skills(skills_series):\n    all_skills = []\n    for skills in skills_series.dropna():\n        if isinstance(skills, str) and skills not in ['Not Listed', '']:\n            all_skills.extend([s.strip() for s in skills.split(',')])\n    return Counter(all_skills)\n\n# Shorten skill names for better display\ndef shorten_skill(skill):\n    replacements = {\n        'SQL (Programming Language)': 'SQL',\n        'Microsoft Excel': 'Excel',\n        'Microsoft Power BI': 'Power BI',\n        'Computer Science': 'Computer Sci',\n        'Problem Solving': 'Problem Solving',\n        'Data Visualization': 'Data Viz',\n        'Business Intelligence': 'Business Intel',\n        'Project Management': 'Project Mgmt',\n        'Data Management': 'Data Mgmt',\n        'Business Development': 'Business Dev',\n        'Customer Service': 'Customer Svc',\n        'Marketing Strategy': 'Mktg Strategy',\n        'Market Research': 'Market Research',\n        'Statistical Analysis': 'Statistics'\n    }\n    return replacements.get(skill, skill[:14] + '..' if len(skill) &gt; 14 else skill)\n\nocc_short = {\n    'Business Intelligence Analyst': 'Business Intelligence',\n    'Data / Data Mining Analyst': 'Data/Data Mining',\n    'Business / Management Analyst': 'Business/Management',\n    'Market Research Analyst': 'Market Research'\n}\n\ncolors_list = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\noccupations = df['LOT_V6_OCCUPATION_NAME'].unique().tolist()\n\n# Get top 8 skills for EACH occupation separately\nskill_data = {}\nfor occ in occupations:\n    df_occ = df[df['LOT_V6_OCCUPATION_NAME'] == occ]\n    occ_skills = extract_skills(df_occ['SKILLS_NAME'])\n    top_8 = occ_skills.most_common(8)\n    total = len(df_occ)\n    skill_data[occ] = {\n        'skills': [s[0] for s in top_8],\n        'values': [(s[1] / total * 100) for s in top_8]\n    }\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{'type': 'polar'}, {'type': 'polar'}],\n           [{'type': 'polar'}, {'type': 'polar'}]],\n    subplot_titles=[occ_short.get(occ, occ) for occ in occupations],\n    vertical_spacing=0.18,\n    horizontal_spacing=0.12\n)\n\nfor idx, occ in enumerate(occupations):\n    row = idx // 2 + 1\n    col = idx % 2 + 1\n    \n    skills = skill_data[occ]['skills']\n    values = skill_data[occ]['values']\n    \n    # Shorten labels and close the polygon\n    short_labels = [shorten_skill(s) for s in skills]\n    values_closed = values + [values[0]]\n    labels_closed = short_labels + [short_labels[0]]\n    \n    fig.add_trace(\n        go.Scatterpolar(\n            r=values_closed,\n            theta=labels_closed,\n            fill='toself',\n            name=occ_short.get(occ, occ),\n            line_color=colors_list[idx],\n            fillcolor=colors_list[idx],\n            opacity=0.5,\n            showlegend=False\n        ),\n        row=row, col=col\n    )\n\n# Update polar subplots\nfig.update_polars(\n    radialaxis=dict(\n        visible=True,\n        tickfont=dict(size=9),\n        range=[0, 85]\n    ),\n    angularaxis=dict(\n        tickfont=dict(size=10),\n        rotation=90,\n        direction='clockwise'\n    )\n)\n\nfig.update_layout(\n    title=dict(\n        text='Top 8 Skills by Occupation (Each with Own Top Skills)',\n        font=dict(size=16),\n        y=0.98\n    ),\n    height=750,\n    template='plotly_white',\n    margin=dict(t=80, b=30, l=60, r=60)\n)\n\n# Adjust subplot title positions\nfor annotation in fig['layout']['annotations']:\n    annotation['font'] = dict(size=12, color='#333')\n    annotation['y'] = annotation['y'] + 0.02\n\nfig.write_image('figures/skills_radar_by_occupation.png', scale=2)\nfig.show()\n\n# Print the top skills for each occupation\nprint(\"\\nTop 8 Skills by Occupation:\")\nfor occ in occupations:\n    print(f\"\\n{occ_short.get(occ, occ)}:\")\n    for skill, val in zip(skill_data[occ]['skills'], skill_data[occ]['values']):\n        print(f\"  • {skill}: {val:.1f}%\")\n\n# Software skills - Horizontal Bar Chart\nsoftware_counts = extract_skills(df['SOFTWARE_SKILLS_NAME'])\ntop_software = pd.DataFrame(software_counts.most_common(12), columns=['Software', 'Count'])\ntop_software['Percentage'] = (top_software['Count'] / len(df) * 100).round(1)\n\nfig2 = go.Figure()\nfig2.add_trace(go.Bar(\n    y=top_software['Software'],\n    x=top_software['Percentage'],\n    orientation='h',\n    marker_color='mediumpurple',\n    text=[f\"{p}%\" for p in top_software['Percentage']],\n    textposition='outside'\n))\n\nfig2.update_layout(\n    title='Top 12 Software/Technical Skills (% of Postings)',\n    xaxis_title='% of Job Postings',\n    yaxis={'categoryorder': 'total ascending'},\n    template='plotly_white',\n    height=450,\n    margin=dict(r=80)\n)\nfig2.write_image('figures/software_skills.png', scale=2)\nfig2.show()\n\n\n                                                    \n\n\n\nTop 8 Skills by Occupation:\n\nBusiness Intelligence:\n  • Communication: 42.9%\n  • SAP Applications: 36.5%\n  • Management: 35.1%\n  • Business Process: 29.0%\n  • Business Requirements: 27.5%\n  • Problem Solving: 25.0%\n  • Finance: 24.5%\n  • Consulting: 22.7%\n\nData/Data Mining:\n  • Data Analysis: 76.4%\n  • SQL (Programming Language): 51.7%\n  • Communication: 45.1%\n  • Management: 33.7%\n  • Python (Programming Language): 31.5%\n  • Tableau (Business Intelligence Software): 30.7%\n  • Microsoft Excel: 28.3%\n  • Dashboard: 27.5%\n\nBusiness/Management:\n  • Communication: 51.4%\n  • Management: 38.1%\n  • Operations: 35.6%\n  • Leadership: 34.9%\n  • Microsoft Excel: 31.2%\n  • Problem Solving: 30.1%\n  • Presentations: 29.5%\n  • Project Management: 28.6%\n\nMarket Research:\n  • Customer Relationship Management: 83.1%\n  • Business Process: 45.8%\n  • Communication: 41.5%\n  • Business Requirements: 41.5%\n  • Salesforce: 40.7%\n  • Project Management: 40.7%\n  • Sales: 39.8%\n  • Problem Solving: 38.1%"
  },
  {
    "objectID": "eda.html#how-does-remote-work-type-affect-salary",
    "href": "eda.html#how-does-remote-work-type-affect-salary",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "How does Remote Work Type affect Salary?",
    "text": "How does Remote Work Type affect Salary?\n\n\nCode\n# Salary Analysis - Violin Plot by Remote Work Type and Occupation\ndf_remote = df[df['REMOTE_TYPE_NAME'] != 'Not Specified'].copy()\n\nfig = px.violin(\n    df_remote,\n    x='REMOTE_TYPE_NAME',\n    y='SALARY',\n    color='LOT_V6_OCCUPATION_NAME',\n    box=True,\n    title='Salary Distribution by Remote Work Type and Occupation',\n    labels={\n        'REMOTE_TYPE_NAME': 'Remote Work Type',\n        'SALARY': 'Annual Salary ($)',\n        'LOT_V6_OCCUPATION_NAME': 'Occupation'\n    }\n)\nfig.update_layout(\n    template='plotly_white',\n    height=500,\n    legend=dict(orientation='h', yanchor='bottom', y=-0.3)\n)\nfig.write_image('figures/salary_by_remote.png', scale=2)\nfig.show()\n\nprint(\"\\nSalary Statistics by Remote Type:\")\nsalary_stats = df_remote.groupby('REMOTE_TYPE_NAME')['SALARY'].agg(['count', 'median', 'mean', 'std']).round(0)\nsalary_stats.columns = ['Count', 'Median', 'Mean', 'Std Dev']\nsalary_stats['Median'] = salary_stats['Median'].apply(lambda x: f\"${x:,.0f}\")\nsalary_stats['Mean'] = salary_stats['Mean'].apply(lambda x: f\"${x:,.0f}\")\nsalary_stats['Std Dev'] = salary_stats['Std Dev'].apply(lambda x: f\"${x:,.0f}\")\nprint(salary_stats)\n\n\n                                                    \n\n\n\nSalary Statistics by Remote Type:\n                  Count   Median      Mean  Std Dev\nREMOTE_TYPE_NAME                                   \nHybrid Remote      1482  $96,008  $105,086  $28,885\nNot Remote          688  $98,800  $104,989  $27,900\nRemote             7950  $99,700  $109,444  $29,127"
  },
  {
    "objectID": "eda.html#where-can-you-apply-for-these-jobs",
    "href": "eda.html#where-can-you-apply-for-these-jobs",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "Where can you apply for these Jobs?",
    "text": "Where can you apply for these Jobs?\n\n\nCode\ndef extract_sources(source_series):\n    all_sources = []\n    for sources in source_series.dropna():\n        if isinstance(sources, str):\n            all_sources.extend([s.strip() for s in sources.split(',') if s.strip()])\n    return Counter(all_sources)\n\nsource_counts = extract_sources(df['SOURCE_TYPES'])\n\ncleaned_counts = {}\nfor source, count in source_counts.items():\n    if source == 'NONE' or source == '':\n        continue\n    elif source == 'Job intermediary':\n        cleaned_counts['Recruiter'] = cleaned_counts.get('Recruiter', 0) + count\n    elif source =='FreeJobBoard':\n        cleaned_counts['Job Board'] = cleaned_counts.get('Job Board', 0) + count\n    else:\n        cleaned_counts[source] = cleaned_counts.get(source, 0) + count\n\ntop_sources = pd.DataFrame(list(cleaned_counts.items()), columns=['Source', 'Count'])\ntop_sources = top_sources.sort_values('Count', ascending=False).head(10).reset_index(drop=True)\n\n# Calculate percentage based on total source mentions (not total jobs)\ntotal_source_mentions = top_sources['Count'].sum()\ntop_sources['Percentage'] = (top_sources['Count'] / total_source_mentions * 100).round(1)\n\nfig = px.treemap(\n    top_sources,\n    path=['Source'],\n    values='Count',\n    title='Job Posting Sources Distribution',\n    color='Count',\n    color_continuous_scale='Blues'\n)\nfig.update_layout(template='plotly_white', height=500)\nfig.update_traces(textinfo='label+value+percent root', textfont_size=14)\nfig.write_image('figures/source_types.png', scale=2)\nfig.show()\n\nprint(f\"\\nJob Posting Sources (Total mentions: {total_source_mentions:,}):\")\nfor i, row in top_sources.iterrows():\n    print(f\"  {i+1}. {row['Source']}: {row['Count']:,} ({row['Percentage']}%)\")\n\n\n                                                    \n\n\n\nJob Posting Sources (Total mentions: 57,491):\n  1. Job Board: 39,645 (69.0%)\n  2. Company: 13,668 (23.8%)\n  3. Recruiter: 2,792 (4.9%)\n  4. Government: 865 (1.5%)\n  5. Education: 521 (0.9%)"
  },
  {
    "objectID": "eda.html#eda-conclusion",
    "href": "eda.html#eda-conclusion",
    "title": "Exploratory Data Analysis: Data & Analytics Job Market",
    "section": "EDA Conclusion",
    "text": "EDA Conclusion\nOur exploratory analysis of 47,000+ job postings reveals key insights for data analytics professionals:\nIndustry & Companies: Industry & Companies: Professional Services leads with 17,600+ postings, followed by Admin & Support Services, Finance & Insurance, Manufacturing, and Information. Deloitte dominates hiring (2,271 postings, 4.8%), followed by Accenture, PwC, and Insight Global—consulting firms are the primary employers.\nJob Roles: Data/Data Mining Analysts (47.7%) and Business Intelligence Analysts (45.5%) make up over 93% of postings. Business/Management Analysts account for 6.5%, while Market Research Analysts represent only 0.2%.\nSkills Demand: Each occupation has distinct skill requirements:\n\nData Analysts: Data Analysis (76%), SQL (52%), Python (32%), Tableau (31%)\nBusiness Intelligence: Communication (43%), SAP (37%), Management (35%)\nBusiness/Management: Communication (51%), Management (38%), Operations (36%)\nMarket Research: CRM (83%), Business Process (46%), Salesforce (41%)\n\nOnline VS Offline : Remote Jobs pay much more higher than hybrid and online jobs for all four occupations. This could tell us that the trend COVID-19 instilled among companies of hiring workers online is slowly dying and infact occupations like Market research analyst have zero online job mode.\nJob Sources: Job postings often appear on multiple platforms - Job Boards lead with 39,645 postings (83.4% of jobs), Company websites list 13,668 (28.8%), and Recruiters post 2,792 (5.9%)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The demand for data science, business analytics, and machine learning professionals has grown exponentially over the past decade. As organizations increasingly rely on data-driven decision making, understanding the current job market landscape is essential for career planning.\nThis project analyzes the 2024 job market using Lightcast data to answer key questions:\n\nWhat skills are most in-demand for Data Science, Business Analytics, and ML roles?\nWhich industries are hiring the most data professionals?\nHow do salaries vary by location, experience, and skill requirements?\nWhat is the career outlook for business analytics professionals?"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The demand for data science, business analytics, and machine learning professionals has grown exponentially over the past decade. As organizations increasingly rely on data-driven decision making, understanding the current job market landscape is essential for career planning.\nThis project analyzes the 2024 job market using Lightcast data to answer key questions:\n\nWhat skills are most in-demand for Data Science, Business Analytics, and ML roles?\nWhich industries are hiring the most data professionals?\nHow do salaries vary by location, experience, and skill requirements?\nWhat is the career outlook for business analytics professionals?"
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "2 Research Rationale",
    "text": "2 Research Rationale\nThe rise of artificial intelligence and automation has transformed the labor market. Data science and machine learning roles have emerged as some of the most sought-after positions across industries. Understanding which skills employers value, which regions offer the best opportunities, and how compensation varies can help job seekers make informed career decisions.\nThis analysis takes the perspective of a job seeker entering the data science field in 2024, examining real job posting data to identify trends and opportunities."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "3 Literature Review",
    "text": "3 Literature Review\nRecent research highlights the evolving nature of data science careers. According to industry reports, Python, SQL, and machine learning frameworks remain the most requested technical skills in job postings. The demand for cloud computing expertise (AWS, Azure, GCP) has also increased significantly.\nStudies show that salary disparities exist across geographic regions, with tech hubs like San Francisco, Seattle, and New York offering higher compensation. However, the rise of remote work has begun to equalize opportunities for candidates outside traditional tech centers.\nBusiness analytics roles increasingly require a blend of technical skills and domain expertise, with employers seeking candidates who can translate data insights into actionable business recommendations."
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "Program: Applied Business Analytics, Boston University\nThis analysis compares our current skill levels against the market demand identified in our Exploratory Data Analysi of job postings from the lightcast dataset. The goal is to identify skill gaps and prioritize learning areas for career readiness.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\n\nmarket_demand = {\n    'Data Analysis': 76,\n    'SQL': 52,\n    'Python': 32,\n    'Tableau': 31,\n    'Communication': 43,\n    'Excel': 28,\n    'Statistics': 25,\n    'Power BI': 22,\n    'Machine Learning': 18,\n    'R': 15,\n    'Project Management': 20,\n    'Business Intelligence': 35\n}\n\n\nmishal_skills = {\n    'Data Analysis': 55,\n    'SQL': 30,\n    'Python': 70,\n    'Tableau': 70,\n    'Communication': 70,\n    'Excel': 55,\n    'Statistics': 50,\n    'Power BI': 75,\n    'Machine Learning': 40,\n    'R': 25,\n    'Project Management': 60,\n    'Business Intelligence': 60\n}\n\nalmas_skills = {\n    'Data Analysis': 70,\n    'SQL': 50,\n    'Python': 60,\n    'Tableau': 0,\n    'Communication': 60,\n    'Excel': 80,\n    'Statistics': 60,\n    'Power BI': 70,\n    'Machine Learning': 50,\n    'R': 55,\n    'Project Management': 20,\n    'Business Intelligence': 60\n}\n\nskills_df = pd.DataFrame({\n    'Skill': list(market_demand.keys()),\n    'Market_Demand': list(market_demand.values()),\n    'Mishal': [mishal_skills[s] for s in market_demand.keys()],\n    'Almas': [almas_skills[s] for s in market_demand.keys()]\n})\n\nskills_df['Mishal_Gap'] = skills_df['Mishal'] - skills_df['Market_Demand']\nskills_df['Almas_Gap'] = skills_df['Almas'] - skills_df['Market_Demand']\n\nprint(\"📊 Skill Gap Analysis Summary\")\nprint(\"-\" * 50)\nprint(skills_df.to_string(index=False))\n\n\n📊 Skill Gap Analysis Summary\n--------------------------------------------------\n                Skill  Market_Demand  Mishal  Almas  Mishal_Gap  Almas_Gap\n        Data Analysis             76      55     70         -21         -6\n                  SQL             52      30     50         -22         -2\n               Python             32      70     60          38         28\n              Tableau             31      70      0          39        -31\n        Communication             43      70     60          27         17\n                Excel             28      55     80          27         52\n           Statistics             25      50     60          25         35\n             Power BI             22      75     70          53         48\n     Machine Learning             18      40     50          22         32\n                    R             15      25     55          10         40\n   Project Management             20      60     20          40          0\nBusiness Intelligence             35      60     60          25         25"
  },
  {
    "objectID": "skill_gap_analysis.html#personal-skills-assessment-mishal-almas",
    "href": "skill_gap_analysis.html#personal-skills-assessment-mishal-almas",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "Program: Applied Business Analytics, Boston University\nThis analysis compares our current skill levels against the market demand identified in our Exploratory Data Analysi of job postings from the lightcast dataset. The goal is to identify skill gaps and prioritize learning areas for career readiness.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\n\nmarket_demand = {\n    'Data Analysis': 76,\n    'SQL': 52,\n    'Python': 32,\n    'Tableau': 31,\n    'Communication': 43,\n    'Excel': 28,\n    'Statistics': 25,\n    'Power BI': 22,\n    'Machine Learning': 18,\n    'R': 15,\n    'Project Management': 20,\n    'Business Intelligence': 35\n}\n\n\nmishal_skills = {\n    'Data Analysis': 55,\n    'SQL': 30,\n    'Python': 70,\n    'Tableau': 70,\n    'Communication': 70,\n    'Excel': 55,\n    'Statistics': 50,\n    'Power BI': 75,\n    'Machine Learning': 40,\n    'R': 25,\n    'Project Management': 60,\n    'Business Intelligence': 60\n}\n\nalmas_skills = {\n    'Data Analysis': 70,\n    'SQL': 50,\n    'Python': 60,\n    'Tableau': 0,\n    'Communication': 60,\n    'Excel': 80,\n    'Statistics': 60,\n    'Power BI': 70,\n    'Machine Learning': 50,\n    'R': 55,\n    'Project Management': 20,\n    'Business Intelligence': 60\n}\n\nskills_df = pd.DataFrame({\n    'Skill': list(market_demand.keys()),\n    'Market_Demand': list(market_demand.values()),\n    'Mishal': [mishal_skills[s] for s in market_demand.keys()],\n    'Almas': [almas_skills[s] for s in market_demand.keys()]\n})\n\nskills_df['Mishal_Gap'] = skills_df['Mishal'] - skills_df['Market_Demand']\nskills_df['Almas_Gap'] = skills_df['Almas'] - skills_df['Market_Demand']\n\nprint(\"📊 Skill Gap Analysis Summary\")\nprint(\"-\" * 50)\nprint(skills_df.to_string(index=False))\n\n\n📊 Skill Gap Analysis Summary\n--------------------------------------------------\n                Skill  Market_Demand  Mishal  Almas  Mishal_Gap  Almas_Gap\n        Data Analysis             76      55     70         -21         -6\n                  SQL             52      30     50         -22         -2\n               Python             32      70     60          38         28\n              Tableau             31      70      0          39        -31\n        Communication             43      70     60          27         17\n                Excel             28      55     80          27         52\n           Statistics             25      50     60          25         35\n             Power BI             22      75     70          53         48\n     Machine Learning             18      40     50          22         32\n                    R             15      25     55          10         40\n   Project Management             20      60     20          40          0\nBusiness Intelligence             35      60     60          25         25"
  },
  {
    "objectID": "skill_gap_analysis.html#visualization-1-skill-comparison-radar-chart",
    "href": "skill_gap_analysis.html#visualization-1-skill-comparison-radar-chart",
    "title": "Skill Gap Analysis",
    "section": "Visualization 1: Skill Comparison Radar Chart",
    "text": "Visualization 1: Skill Comparison Radar Chart\nThis radar chart compares our individual skill levels against market demand, helping visualize strengths and areas for improvement.\n\n\nCode\n\n\nskills = skills_df['Skill'].tolist()\nskills_closed = skills + [skills[0]]  \n\nmarket_values = skills_df['Market_Demand'].tolist() + [skills_df['Market_Demand'].tolist()[0]]\nmishal_values = skills_df['Mishal'].tolist() + [skills_df['Mishal'].tolist()[0]]\nalmas_values = skills_df['Almas'].tolist() + [skills_df['Almas'].tolist()[0]]\n\nfig1 = go.Figure()\n\nfig1.add_trace(go.Scatterpolar(\n    r=market_values,\n    theta=skills_closed,\n    fill='toself',\n    name='📈 Market Demand',\n    line=dict(color='#636EFA', width=3),\n    fillcolor='rgba(99, 110, 250, 0.15)',\n    opacity=0.9\n))\n\nfig1.add_trace(go.Scatterpolar(\n    r=mishal_values,\n    theta=skills_closed,\n    fill='toself',\n    name='👩‍💻 Mishal',\n    line=dict(color='#EF553B', width=2.5),\n    fillcolor='rgba(239, 85, 59, 0.2)',\n    opacity=0.85\n))\n\nfig1.add_trace(go.Scatterpolar(\n    r=almas_values,\n    theta=skills_closed,\n    fill='toself',\n    name='👨‍💻 Almas',\n    line=dict(color='#00CC96', width=2.5),\n    fillcolor='rgba(0, 204, 150, 0.2)',\n    opacity=0.85\n))\n\nfig1.update_layout(\n    polar=dict(\n        radialaxis=dict(\n            visible=True,\n            range=[0, 100],\n            tickfont=dict(size=10, color='#666'),\n            gridcolor='rgba(0,0,0,0.1)',\n            linecolor='rgba(0,0,0,0.1)'\n        ),\n        angularaxis=dict(\n            tickfont=dict(size=11, color='#333', family='Arial'),\n            gridcolor='rgba(0,0,0,0.08)',\n            linecolor='rgba(0,0,0,0.1)',\n            rotation=90,\n            direction='clockwise'\n        ),\n        bgcolor='rgba(250,250,250,0.5)'\n    ),\n    title=dict(\n        text='&lt;b&gt;Skill Proficiency vs Market Demand&lt;/b&gt;&lt;br&gt;&lt;sup&gt;Mishal & Almas | BU Applied Business Analytics&lt;/sup&gt;',\n        font=dict(size=18, color='#2c3e50', family='Arial Black'),\n        x=0.5,\n        y=0.95\n    ),\n    legend=dict(\n        orientation='h',\n        yanchor='bottom',\n        y=-0.15,\n        xanchor='center',\n        x=0.5,\n        font=dict(size=12),\n        bgcolor='rgba(255,255,255,0.8)',\n        bordercolor='rgba(0,0,0,0.1)',\n        borderwidth=1\n    ),\n    template='plotly_white',\n    height=650,\n    width=800,\n    margin=dict(t=100, b=80, l=80, r=80)\n)\n\nfig1.write_image('figures/skill_gap_radar.png', scale=2)\nfig1.show()\nprint(\"📊 Skill Gap Analysis Summary\")\nprint(\"-\" * 50)\nprint(skills_df.to_string(index=False))\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n📊 Skill Gap Analysis Summary\n--------------------------------------------------\n                Skill  Market_Demand  Mishal  Almas  Mishal_Gap  Almas_Gap\n        Data Analysis             76      55     70         -21         -6\n                  SQL             52      30     50         -22         -2\n               Python             32      70     60          38         28\n              Tableau             31      70      0          39        -31\n        Communication             43      70     60          27         17\n                Excel             28      55     80          27         52\n           Statistics             25      50     60          25         35\n             Power BI             22      75     70          53         48\n     Machine Learning             18      40     50          22         32\n                    R             15      25     55          10         40\n   Project Management             20      60     20          40          0\nBusiness Intelligence             35      60     60          25         25"
  },
  {
    "objectID": "skill_gap_analysis.html#visualization-2-skill-gap-heatmap-table",
    "href": "skill_gap_analysis.html#visualization-2-skill-gap-heatmap-table",
    "title": "Skill Gap Analysis",
    "section": "Visualization 2: Skill Gap Heatmap Table",
    "text": "Visualization 2: Skill Gap Heatmap Table\nThis heatmap provides a detailed breakdown of each skill, showing market demand alongside our proficiency levels and highlighting specific gaps to address.\n\n\nCode\n\nskills_sorted = skills_df.sort_values('Market_Demand', ascending=False).reset_index(drop=True)\n\ndef get_status(gap):\n    if gap &gt;= 10:\n        return '✅ Strong'\n    elif gap &gt;= -10:\n        return '⚡ On Track'\n    else:\n        return '🎯 Focus Area'\n\nskills_sorted['Mishal_Status'] = skills_sorted['Mishal_Gap'].apply(get_status)\nskills_sorted['Almas_Status'] = skills_sorted['Almas_Gap'].apply(get_status)\n\nfig2 = go.Figure()\n\nheaders = ['&lt;b&gt;Skill&lt;/b&gt;', '&lt;b&gt;Market&lt;br&gt;Demand&lt;/b&gt;', \n           '&lt;b&gt;Mishal&lt;br&gt;Level&lt;/b&gt;', '&lt;b&gt;Mishal&lt;br&gt;Status&lt;/b&gt;',\n           '&lt;b&gt;Almas&lt;br&gt;Level&lt;/b&gt;', '&lt;b&gt;Almas&lt;br&gt;Status&lt;/b&gt;']\n\ndef get_cell_color(status):\n    if '✅' in status:\n        return 'rgba(0, 204, 150, 0.3)'\n    elif '⚡' in status:\n        return 'rgba(255, 193, 7, 0.3)'\n    else:\n        return 'rgba(239, 85, 59, 0.25)'\n\nmishal_colors = [get_cell_color(s) for s in skills_sorted['Mishal_Status']]\nalmas_colors = [get_cell_color(s) for s in skills_sorted['Almas_Status']]\n\n# Create cell fill colors (column-wise)\ncell_colors = [\n    ['rgba(248, 249, 250, 0.8)'] * len(skills_sorted),  # Skill\n    ['rgba(99, 110, 250, 0.15)'] * len(skills_sorted),   # Market Demand\n    ['rgba(239, 85, 59, 0.1)'] * len(skills_sorted),     # Mishal Level\n    mishal_colors,                                         # Mishal Status\n    ['rgba(0, 204, 150, 0.1)'] * len(skills_sorted),     # Almas Level\n    almas_colors                                           # Almas Status\n]\n\nfig2.add_trace(go.Table(\n    header=dict(\n        values=headers,\n        fill_color='#2c3e50',\n        font=dict(color='white', size=13, family='Arial'),\n        align='center',\n        height=45,\n        line=dict(color='white', width=2)\n    ),\n    cells=dict(\n        values=[\n            skills_sorted['Skill'],\n            [f\"{v}%\" for v in skills_sorted['Market_Demand']],\n            [f\"{v}%\" for v in skills_sorted['Mishal']],\n            skills_sorted['Mishal_Status'],\n            [f\"{v}%\" for v in skills_sorted['Almas']],\n            skills_sorted['Almas_Status']\n        ],\n        fill_color=cell_colors,\n        font=dict(color='#2c3e50', size=12, family='Arial'),\n        align=['left', 'center', 'center', 'center', 'center', 'center'],\n        height=35,\n        line=dict(color='rgba(0,0,0,0.1)', width=1)\n    )\n))\n\nfig2.update_layout(\n    title=dict(\n        text='&lt;b&gt;Skill Gap Analysis Dashboard&lt;/b&gt;&lt;br&gt;&lt;sup&gt;🎯 Focus Area | ⚡ On Track | ✅ Strong&lt;/sup&gt;',\n        font=dict(size=18, color='#2c3e50', family='Arial Black'),\n        x=0.5,\n        y=0.98\n    ),\n    height=550,\n    width=900,\n    margin=dict(t=80, b=20, l=20, r=20),\n    paper_bgcolor='white'\n)\n\nfig2.write_image('figures/skill_gap_table.png', scale=2)\nfig2.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"📋 SKILL GAP ANALYSIS INSIGHTS\")\nprint(\"=\" * 70)\n\nprint(\"\\n👩‍💻 MISHAL'S PROFILE:\")\nprint(\"-\" * 40)\nstrong = skills_sorted[skills_sorted['Mishal_Status'].str.contains('Strong')]['Skill'].tolist()\nfocus = skills_sorted[skills_sorted['Mishal_Status'].str.contains('Focus')]['Skill'].tolist()\nprint(f\"   Strengths: {', '.join(strong) if strong else 'Building towards excellence'}\")\nprint(f\"  🎯 Focus Areas: {', '.join(focus) if focus else 'Well-rounded profile!'}\")\n\nprint(\"\\n👨‍💻 ALMAS'S PROFILE:\")\nprint(\"-\" * 40)\nstrong = skills_sorted[skills_sorted['Almas_Status'].str.contains('Strong')]['Skill'].tolist()\nfocus = skills_sorted[skills_sorted['Almas_Status'].str.contains('Focus')]['Skill'].tolist()\nprint(f\"  ✅ Strengths: {', '.join(strong) if strong else 'Building towards excellence'}\")\nprint(f\"  🎯 Focus Areas: {', '.join(focus) if focus else 'Well-rounded profile!'}\")\n\nprint(\"\\n💡 KEY RECOMMENDATIONS:\")\nprint(\"-\" * 40)\nprint(\"  1. SQL is critical - both need improvement (Mishal: 30%, Almas: 50%)\")\nprint(\"  2. Almas should prioritize Tableau (currently 0%, market demand 31%)\")\nprint(\"  3. Mishal should strengthen Data Analysis fundamentals (55% vs 76%)\")\nprint(\"  4. Leverage complementary strengths: Mishal's Python + Almas's Excel/R\")\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n======================================================================\n📋 SKILL GAP ANALYSIS INSIGHTS\n======================================================================\n\n👩‍💻 MISHAL'S PROFILE:\n----------------------------------------\n   Strengths: Communication, Business Intelligence, Python, Tableau, Excel, Statistics, Power BI, Project Management, Machine Learning, R\n  🎯 Focus Areas: Data Analysis, SQL\n\n👨‍💻 ALMAS'S PROFILE:\n----------------------------------------\n  ✅ Strengths: Communication, Business Intelligence, Python, Excel, Statistics, Power BI, Machine Learning, R\n  🎯 Focus Areas: Tableau\n\n💡 KEY RECOMMENDATIONS:\n----------------------------------------\n  1. SQL is critical - both need improvement (Mishal: 30%, Almas: 50%)\n  2. Almas should prioritize Tableau (currently 0%, market demand 31%)\n  3. Mishal should strengthen Data Analysis fundamentals (55% vs 76%)\n  4. Leverage complementary strengths: Mishal's Python + Almas's Excel/R"
  },
  {
    "objectID": "skill_gap_analysis.html#conclusion",
    "href": "skill_gap_analysis.html#conclusion",
    "title": "Skill Gap Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nBased on our analysis of 47,000+ job postings and our personal skill assessments:\nMishal’s Profile: - Strengths: Power BI (75%), Python (70%), Tableau (70%), Communication (70%) — exceeds market demand in visualization and programming - Focus Areas: Data Analysis (55% vs 76% demand), SQL (30% vs 52% demand), R (25% vs 15% demand) - Strong visualization toolkit but needs to strengthen core data analysis and database querying skills\nAlmas’s Profile:\n- Strengths: Excel (80%), Data Analysis (70%), Power BI (70%) — solid foundation in spreadsheet and BI tools - Focus Areas: Tableau (0% - not yet learned), Project Management (20% vs 20% demand), SQL (50% vs 52% demand) - Excellent Excel skills; should prioritize learning Tableau to complete the visualization skillset\nTeam Synergy: - Mishal brings stronger Python programming and visualization expertise - Almas brings superior Excel, R, and Data Analysis capabilities - Together, we try to help each other out in bringing good results for the project.\n\nAnalysis conducted as part of AD688 CLOUD ANALYTICS FOR BUSINESS - Applied Business Analytics at Boston University"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Preprocessing",
    "section": "",
    "text": "This notebook performs data cleaning on the Lightcast job postings dataset, preparing it for exploratory analysis and machine learning models.\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv('data/lightcast_job_postings.csv', \n                 low_memory=False,\n                 dtype={'ID': str}) \n\nprint(f\"Original dataset shape: {df.shape}\")\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)  \npd.set_option('display.width', None)\ndisplay(df.head())\n\n\nOriginal dataset shape: (72498, 131)\n\n\n\n\n\n\n\n\n\nID\nLAST_UPDATED_DATE\nLAST_UPDATED_TIMESTAMP\nDUPLICATES\nPOSTED\nEXPIRED\nDURATION\nSOURCE_TYPES\nSOURCES\nURL\nACTIVE_URLS\nACTIVE_SOURCES_INFO\nTITLE_RAW\nBODY\nMODELED_EXPIRED\nMODELED_DURATION\nCOMPANY\nCOMPANY_NAME\nCOMPANY_RAW\nCOMPANY_IS_STAFFING\nEDUCATION_LEVELS\nEDUCATION_LEVELS_NAME\nMIN_EDULEVELS\nMIN_EDULEVELS_NAME\nMAX_EDULEVELS\nMAX_EDULEVELS_NAME\nEMPLOYMENT_TYPE\nEMPLOYMENT_TYPE_NAME\nMIN_YEARS_EXPERIENCE\nMAX_YEARS_EXPERIENCE\nIS_INTERNSHIP\nSALARY\nREMOTE_TYPE\nREMOTE_TYPE_NAME\nORIGINAL_PAY_PERIOD\nSALARY_TO\nSALARY_FROM\nLOCATION\nCITY\nCITY_NAME\nCOUNTY\nCOUNTY_NAME\nMSA\nMSA_NAME\nSTATE\nSTATE_NAME\nCOUNTY_OUTGOING\nCOUNTY_NAME_OUTGOING\nCOUNTY_INCOMING\nCOUNTY_NAME_INCOMING\nMSA_OUTGOING\nMSA_NAME_OUTGOING\nMSA_INCOMING\nMSA_NAME_INCOMING\nNAICS2\nNAICS2_NAME\nNAICS3\nNAICS3_NAME\nNAICS4\nNAICS4_NAME\nNAICS5\nNAICS5_NAME\nNAICS6\nNAICS6_NAME\nTITLE\nTITLE_NAME\nTITLE_CLEAN\nSKILLS\nSKILLS_NAME\nSPECIALIZED_SKILLS\nSPECIALIZED_SKILLS_NAME\nCERTIFICATIONS\nCERTIFICATIONS_NAME\nCOMMON_SKILLS\nCOMMON_SKILLS_NAME\nSOFTWARE_SKILLS\nSOFTWARE_SKILLS_NAME\nONET\nONET_NAME\nONET_2019\nONET_2019_NAME\nCIP6\nCIP6_NAME\nCIP4\nCIP4_NAME\nCIP2\nCIP2_NAME\nSOC_2021_2\nSOC_2021_2_NAME\nSOC_2021_3\nSOC_2021_3_NAME\nSOC_2021_4\nSOC_2021_4_NAME\nSOC_2021_5\nSOC_2021_5_NAME\nLOT_CAREER_AREA\nLOT_CAREER_AREA_NAME\nLOT_OCCUPATION\nLOT_OCCUPATION_NAME\nLOT_SPECIALIZED_OCCUPATION\nLOT_SPECIALIZED_OCCUPATION_NAME\nLOT_OCCUPATION_GROUP\nLOT_OCCUPATION_GROUP_NAME\nLOT_V6_SPECIALIZED_OCCUPATION\nLOT_V6_SPECIALIZED_OCCUPATION_NAME\nLOT_V6_OCCUPATION\nLOT_V6_OCCUPATION_NAME\nLOT_V6_OCCUPATION_GROUP\nLOT_V6_OCCUPATION_GROUP_NAME\nLOT_V6_CAREER_AREA\nLOT_V6_CAREER_AREA_NAME\nSOC_2\nSOC_2_NAME\nSOC_3\nSOC_3_NAME\nSOC_4\nSOC_4_NAME\nSOC_5\nSOC_5_NAME\nLIGHTCAST_SECTORS\nLIGHTCAST_SECTORS_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nNAICS_2022_3\nNAICS_2022_3_NAME\nNAICS_2022_4\nNAICS_2022_4_NAME\nNAICS_2022_5\nNAICS_2022_5_NAME\nNAICS_2022_6\nNAICS_2022_6_NAME\n\n\n\n\n0\n1f57d95acf4dc67ed2819eb12f049f6a5c11782c\n9/6/2024\n2024-09-06 20:32:57.352 Z\n0.0\n6/2/2024\n6/8/2024\n6.0\n[\\n \"Company\"\\n]\n[\\n \"brassring.com\"\\n]\n[\\n \"https://sjobs.brassring.com/TGnewUI/Sear...\n[]\nNaN\nEnterprise Analyst (II-III)\n31-May-2024\\n\\nEnterprise Analyst (II-III)\\n\\n...\n6/8/2024\n6.0\n894731.0\nMurphy USA\nMurphy USA\nFalse\n[\\n 2\\n]\n[\\n \"Bachelor's degree\"\\n]\n2.0\nBachelor's degree\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n2.0\n2.0\nFalse\nNaN\n0.0\n[None]\nNaN\nNaN\nNaN\n{\\n \"lat\": 33.20763,\\n \"lon\": -92.6662674\\n}\nRWwgRG9yYWRvLCBBUg==\nEl Dorado, AR\n5139.0\nUnion, AR\n20980.0\nEl Dorado, AR\n5.0\nArkansas\n5139.0\nUnion, AR\n5139.0\nUnion, AR\n20980.0\nEl Dorado, AR\n20980.0\nEl Dorado, AR\n44.0\nRetail Trade\n441.0\nMotor Vehicle and Parts Dealers\n4413.0\nAutomotive Parts, Accessories, and Tire Retailers\n44133.0\nAutomotive Parts and Accessories Retailers\n441330.0\nAutomotive Parts and Accessories Retailers\nET29C073C03D1F86B4\nEnterprise Analysts\nenterprise analyst ii iii\n[\\n \"KS126DB6T061MHD7RTGQ\",\\n \"KS126706DPFD3...\n[\\n \"Merchandising\",\\n \"Mathematics\",\\n \"Pr...\n[\\n \"KS126DB6T061MHD7RTGQ\",\\n \"KS128006L3V0H...\n[\\n \"Merchandising\",\\n \"Predictive Modeling\"...\n[]\n[]\n[\\n \"KS126706DPFD3354M7YK\",\\n \"KS1280B68GD79...\n[\\n \"Mathematics\",\\n \"Presentations\",\\n \"Re...\n[\\n \"KS440W865GC4VRBW6LJP\",\\n \"KS13USA80NE38...\n[\\n \"SQL (Programming Language)\",\\n \"Power B...\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[\\n \"45.0601\",\\n \"27.0101\"\\n]\n[\\n \"Economics, General\",\\n \"Mathematics, Ge...\n[\\n \"45.06\",\\n \"27.01\"\\n]\n[\\n \"Economics\",\\n \"Mathematics\"\\n]\n[\\n \"45\",\\n \"27\"\\n]\n[\\n \"Social Sciences\",\\n \"Mathematics and St...\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231010.0\nBusiness Intelligence Analyst\n23101011.0\nGeneral ERP Analyst / Consultant\n2310.0\nBusiness Intelligence\n23101011.0\nGeneral ERP Analyst / Consultant\n231010.0\nBusiness Intelligence Analyst\n2310.0\nBusiness Intelligence\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n[\\n 7\\n]\n[\\n \"Artificial Intelligence\"\\n]\n44.0\nRetail Trade\n441.0\nMotor Vehicle and Parts Dealers\n4413.0\nAutomotive Parts, Accessories, and Tire Retailers\n44133.0\nAutomotive Parts and Accessories Retailers\n441330.0\nAutomotive Parts and Accessories Retailers\n\n\n1\n0cb072af26757b6c4ea9464472a50a443af681ac\n8/2/2024\n2024-08-02 17:08:58.838 Z\n0.0\n6/2/2024\n8/1/2024\nNaN\n[\\n \"Job Board\"\\n]\n[\\n \"maine.gov\"\\n]\n[\\n \"https://joblink.maine.gov/jobs/1085740\"\\n]\n[]\nNaN\nOracle Consultant - Reports (3592)\nOracle Consultant - Reports (3592)\\n\\nat SMX i...\n8/1/2024\nNaN\n133098.0\nSmx Corporation Limited\nSMX\nTrue\n[\\n 99\\n]\n[\\n \"No Education Listed\"\\n]\n99.0\nNo Education Listed\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n3.0\n3.0\nFalse\nNaN\n1.0\nRemote\nNaN\nNaN\nNaN\n{\\n \"lat\": 44.3106241,\\n \"lon\": -69.7794897\\n}\nQXVndXN0YSwgTUU=\nAugusta, ME\n23011.0\nKennebec, ME\n12300.0\nAugusta-Waterville, ME\n23.0\nMaine\n23011.0\nKennebec, ME\n23011.0\nKennebec, ME\n12300.0\nAugusta-Waterville, ME\n12300.0\nAugusta-Waterville, ME\n56.0\nAdministrative and Support and Waste Managemen...\n561.0\nAdministrative and Support Services\n5613.0\nEmployment Services\n56132.0\nTemporary Help Services\n561320.0\nTemporary Help Services\nET21DDA63780A7DC09\nOracle Consultants\noracle consultant reports\n[\\n \"KS122626T550SLQ7QZ1C\",\\n \"KS123YJ6KVWC9...\n[\\n \"Procurement\",\\n \"Financial Statements\",...\n[\\n \"KS122626T550SLQ7QZ1C\",\\n \"KS123YJ6KVWC9...\n[\\n \"Procurement\",\\n \"Financial Statements\",...\n[]\n[]\n[]\n[]\n[\\n \"BGSBF3F508F7F46312E3\",\\n \"ESEA839CED378...\n[\\n \"Oracle Business Intelligence (BI) / OBIA...\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231010.0\nBusiness Intelligence Analyst\n23101012.0\nOracle Consultant / Analyst\n2310.0\nBusiness Intelligence\n23101012.0\nOracle Consultant / Analyst\n231010.0\nBusiness Intelligence Analyst\n2310.0\nBusiness Intelligence\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\nNaN\nNaN\n56.0\nAdministrative and Support and Waste Managemen...\n561.0\nAdministrative and Support Services\n5613.0\nEmployment Services\n56132.0\nTemporary Help Services\n561320.0\nTemporary Help Services\n\n\n2\n85318b12b3331fa490d32ad014379df01855c557\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/7/2024\n35.0\n[\\n \"Job Board\"\\n]\n[\\n \"dejobs.org\"\\n]\n[\\n \"https://dejobs.org/dallas-tx/data-analys...\n[]\nNaN\nData Analyst\nTaking care of people is at the heart of every...\n6/10/2024\n8.0\n39063746.0\nSedgwick\nSedgwick\nFalse\n[\\n 2\\n]\n[\\n \"Bachelor's degree\"\\n]\n2.0\nBachelor's degree\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n5.0\nNaN\nFalse\nNaN\n0.0\n[None]\nNaN\nNaN\nNaN\n{\\n \"lat\": 32.7766642,\\n \"lon\": -96.7969879\\n}\nRGFsbGFzLCBUWA==\nDallas, TX\n48113.0\nDallas, TX\n19100.0\nDallas-Fort Worth-Arlington, TX\n48.0\nTexas\n48113.0\nDallas, TX\n48113.0\nDallas, TX\n19100.0\nDallas-Fort Worth-Arlington, TX\n19100.0\nDallas-Fort Worth-Arlington, TX\n52.0\nFinance and Insurance\n524.0\nInsurance Carriers and Related Activities\n5242.0\nAgencies, Brokerages, and Other Insurance Rela...\n52429.0\nOther Insurance Related Activities\n524291.0\nClaims Adjusting\nET3037E0C947A02404\nData Analysts\ndata analyst\n[\\n \"KS1218W78FGVPVP2KXPX\",\\n \"ESF3939CE1F80...\n[\\n \"Management\",\\n \"Exception Reporting\",\\n...\n[\\n \"ESF3939CE1F80C10C327\",\\n \"KS120GV6C72JM...\n[\\n \"Exception Reporting\",\\n \"Data Analysis\"...\n[\\n \"KS683TN76T77DQDVBZ1B\"\\n]\n[\\n \"Security Clearance\"\\n]\n[\\n \"KS1218W78FGVPVP2KXPX\",\\n \"BGS1ADAA36DB6...\n[\\n \"Management\",\\n \"Report Writing\",\\n \"In...\n[\\n \"KS126HY6YLTB9R7XJC4Z\"\\n]\n[\\n \"Microsoft Office\"\\n]\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231113.0\nData / Data Mining Analyst\n23111310.0\nData Analyst\n2311.0\nData Analysis and Mathematics\n23111310.0\nData Analyst\n231113.0\nData / Data Mining Analyst\n2311.0\nData Analysis and Mathematics\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\nNaN\nNaN\n52.0\nFinance and Insurance\n524.0\nInsurance Carriers and Related Activities\n5242.0\nAgencies, Brokerages, and Other Insurance Rela...\n52429.0\nOther Insurance Related Activities\n524291.0\nClaims Adjusting\n\n\n3\n1b5c3941e54a1889ef4f8ae55b401a550708a310\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/20/2024\n48.0\n[\\n \"Job Board\"\\n]\n[\\n \"disabledperson.com\",\\n \"dejobs.org\"\\n]\n[\\n \"https://www.disabledperson.com/jobs/5948...\n[]\nNaN\nSr. Lead Data Mgmt. Analyst - SAS Product Owner\nAbout this role:\\n\\nWells Fargo is looking for...\n6/12/2024\n10.0\n37615159.0\nWells Fargo\nWells Fargo\nFalse\n[\\n 99\\n]\n[\\n \"No Education Listed\"\\n]\n99.0\nNo Education Listed\nNaN\nNaN\n1.0\nFull-time (&gt; 32 hours)\n3.0\nNaN\nFalse\nNaN\n0.0\n[None]\nNaN\nNaN\nNaN\n{\\n \"lat\": 33.4483771,\\n \"lon\": -112.0740373\\n}\nUGhvZW5peCwgQVo=\nPhoenix, AZ\n4013.0\nMaricopa, AZ\n38060.0\nPhoenix-Mesa-Chandler, AZ\n4.0\nArizona\n4013.0\nMaricopa, AZ\n4013.0\nMaricopa, AZ\n38060.0\nPhoenix-Mesa-Chandler, AZ\n38060.0\nPhoenix-Mesa-Chandler, AZ\n52.0\nFinance and Insurance\n522.0\nCredit Intermediation and Related Activities\n5221.0\nDepository Credit Intermediation\n52211.0\nCommercial Banking\n522110.0\nCommercial Banking\nET2114E0404BA30075\nManagement Analysts\nsr lead data mgmt analyst sas product owner\n[\\n \"KS123QX62QYTC4JF38H8\",\\n \"KS7G6NP6R6L1H...\n[\\n \"Exit Strategies\",\\n \"Reliability\",\\n \"...\n[\\n \"KS123QX62QYTC4JF38H8\",\\n \"KS441PQ64HT13...\n[\\n \"Exit Strategies\",\\n \"User Story\",\\n \"H...\n[]\n[]\n[\\n \"KS7G6NP6R6L1H1SKFTSY\",\\n \"KS1218W78FGVP...\n[\\n \"Reliability\",\\n \"Management\",\\n \"Strat...\n[\\n \"KS4409D76NW1S5LNCL18\",\\n \"ESC7869CF7378...\n[\\n \"SAS (Software)\",\\n \"Google Cloud Platfo...\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231113.0\nData / Data Mining Analyst\n23111310.0\nData Analyst\n2311.0\nData Analysis and Mathematics\n23111310.0\nData Analyst\n231113.0\nData / Data Mining Analyst\n2311.0\nData Analysis and Mathematics\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n[\\n 6\\n]\n[\\n \"Data Privacy/Protection\"\\n]\n52.0\nFinance and Insurance\n522.0\nCredit Intermediation and Related Activities\n5221.0\nDepository Credit Intermediation\n52211.0\nCommercial Banking\n522110.0\nCommercial Banking\n\n\n4\ncb5ca25f02bdf25c13edfede7931508bfd9e858f\n6/19/2024\n2024-06-19 07:00:00.000 Z\n0.0\n6/2/2024\n6/17/2024\n15.0\n[\\n \"FreeJobBoard\"\\n]\n[\\n \"craigslist.org\"\\n]\n[\\n \"https://modesto.craigslist.org/sls/77475...\n[]\nNaN\nComisiones de $1000 - $3000 por semana... Comi...\nComisiones de $1000 - $3000 por semana... Comi...\n6/17/2024\n15.0\n0.0\nUnclassified\nLH/GM\nFalse\n[\\n 99\\n]\n[\\n \"No Education Listed\"\\n]\n99.0\nNo Education Listed\nNaN\nNaN\n3.0\nPart-time / full-time\nNaN\nNaN\nFalse\n92500.0\n0.0\n[None]\nyear\n150000.0\n35000.0\n{\\n \"lat\": 37.6392595,\\n \"lon\": -120.9970014\\n}\nTW9kZXN0bywgQ0E=\nModesto, CA\n6099.0\nStanislaus, CA\n33700.0\nModesto, CA\n6.0\nCalifornia\n6099.0\nStanislaus, CA\n6099.0\nStanislaus, CA\n33700.0\nModesto, CA\n33700.0\nModesto, CA\n99.0\nUnclassified Industry\n999.0\nUnclassified Industry\n9999.0\nUnclassified Industry\n99999.0\nUnclassified Industry\n999999.0\nUnclassified Industry\nET0000000000000000\nUnclassified\ncomisiones de por semana comiensa rapido\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n15-2051.01\nBusiness Intelligence Analysts\n15-2051.01\nBusiness Intelligence Analysts\n[]\n[]\n[]\n[]\n[]\n[]\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\n23.0\nInformation Technology and Computer Science\n231010.0\nBusiness Intelligence Analyst\n23101012.0\nOracle Consultant / Analyst\n2310.0\nBusiness Intelligence\n23101012.0\nOracle Consultant / Analyst\n231010.0\nBusiness Intelligence Analyst\n2310.0\nBusiness Intelligence\n23.0\nInformation Technology and Computer Science\n15-0000\nComputer and Mathematical Occupations\n15-2000\nMathematical Science Occupations\n15-2050\nData Scientists\n15-2051\nData Scientists\nNaN\nNaN\n99.0\nUnclassified Industry\n999.0\nUnclassified Industry\n9999.0\nUnclassified Industry\n99999.0\nUnclassified Industry\n999999.0\nUnclassified Industry"
  },
  {
    "objectID": "data_cleaning.html#select-relevant-columns",
    "href": "data_cleaning.html#select-relevant-columns",
    "title": "Data Cleaning & Preprocessing",
    "section": "1. Select Relevant Columns",
    "text": "1. Select Relevant Columns\n\n\nCode\ncolumns_to_keep = [\n    'POSTED', 'EXPIRED','TITLE_NAME', 'BODY', 'COMPANY_NAME','SOURCE_TYPES', 'COMPANY_IS_STAFFING', \n    'SALARY', 'SALARY_FROM', 'SALARY_TO','STATE_NAME', 'CITY_NAME','REMOTE_TYPE_NAME', \n    'EMPLOYMENT_TYPE_NAME','MIN_YEARS_EXPERIENCE','MIN_EDULEVELS_NAME','SKILLS_NAME', 'SOFTWARE_SKILLS_NAME',\n    'LOT_V6_OCCUPATION_NAME','NAICS_2022_2_NAME'\n]\ncolumns_to_keep = [col for col in columns_to_keep if col in df.columns]\ndf = df[columns_to_keep].copy()\n\nprint(f\"Shape after column selection: {df.shape}\")\nprint(f\"\\nColumns kept ({len(df.columns)}):\")\nprint(df.columns.tolist())\n\n\nShape after column selection: (72498, 20)\n\nColumns kept (20):\n['POSTED', 'EXPIRED', 'TITLE_NAME', 'BODY', 'COMPANY_NAME', 'SOURCE_TYPES', 'COMPANY_IS_STAFFING', 'SALARY', 'SALARY_FROM', 'SALARY_TO', 'STATE_NAME', 'CITY_NAME', 'REMOTE_TYPE_NAME', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MIN_EDULEVELS_NAME', 'SKILLS_NAME', 'SOFTWARE_SKILLS_NAME', 'LOT_V6_OCCUPATION_NAME', 'NAICS_2022_2_NAME']"
  },
  {
    "objectID": "data_cleaning.html#create-duration-feature",
    "href": "data_cleaning.html#create-duration-feature",
    "title": "Data Cleaning & Preprocessing",
    "section": "2. Create Duration Feature",
    "text": "2. Create Duration Feature\nDuration = EXPIRED - POSTED (in days)\n\n\nCode\ndf = df.dropna(subset=['EXPIRED'])\ndf['POSTED'] = pd.to_datetime(df['POSTED'], errors='coerce')\ndf['EXPIRED'] = pd.to_datetime(df['EXPIRED'], errors='coerce')\n\ndf['DURATION'] = (df['EXPIRED'] - df['POSTED']).dt.days\n\nprint(\"Duration statistics:\")\nprint(df['DURATION'].describe())\n\ndf.drop(columns=['EXPIRED'], inplace=True)\n\nprint(f\"\\nShape: {df.shape}\")\n\n\nDuration statistics:\ncount    64654.000000\nmean        35.296811\nstd         23.961129\nmin          0.000000\n25%         14.000000\n50%         31.000000\n75%         60.000000\nmax        119.000000\nName: DURATION, dtype: float64\n\nShape: (64654, 20)"
  },
  {
    "objectID": "data_cleaning.html#explore-lot_v6_occupation_name-for-filtering",
    "href": "data_cleaning.html#explore-lot_v6_occupation_name-for-filtering",
    "title": "Data Cleaning & Preprocessing",
    "section": "3. Explore LOT_V6_OCCUPATION_NAME for Filtering",
    "text": "3. Explore LOT_V6_OCCUPATION_NAME for Filtering\nUsing LOT_V6_OCCUPATION_NAME provides more accurate job classification than TITLE_NAME.\n\n\nCode\nprint(\"LOT_V6_OCCUPATION_NAME value counts:\\n\")\nprint(df['LOT_V6_OCCUPATION_NAME'].value_counts())\n\n\nLOT_V6_OCCUPATION_NAME value counts:\n\nLOT_V6_OCCUPATION_NAME\nData / Data Mining Analyst                                              26809\nBusiness Intelligence Analyst                                           26550\nComputer Systems Engineer / Architect                                    7188\nBusiness / Management Analyst                                            3729\nClinical Analyst / Clinical Documentation and Improvement Specialist      228\nMarket Research Analyst                                                   132\nName: count, dtype: int64\n\n\n\n\nCode\noccupations_to_keep = [\n    'Data / Data Mining Analyst',\n    'Business Intelligence Analyst',\n    'Business / Management Analyst',\n    'Market Research Analyst'\n\n]\ndf = df[df['LOT_V6_OCCUPATION_NAME'].isin(occupations_to_keep)].copy()\n\nprint(f\"Shape after filtering: {df.shape}\")\nprint(f\"\\nOccupations kept:\")\nprint(df['LOT_V6_OCCUPATION_NAME'].value_counts())\n\n\nShape after filtering: (57220, 20)\n\nOccupations kept:\nLOT_V6_OCCUPATION_NAME\nData / Data Mining Analyst       26809\nBusiness Intelligence Analyst    26550\nBusiness / Management Analyst     3729\nMarket Research Analyst            132\nName: count, dtype: int64"
  },
  {
    "objectID": "data_cleaning.html#clean-string-formatting",
    "href": "data_cleaning.html#clean-string-formatting",
    "title": "Data Cleaning & Preprocessing",
    "section": "4. Clean String Formatting",
    "text": "4. Clean String Formatting\nRemove JSON-like formatting: [\\n  \"value\"\\n] → value1, value2\nAlso remove empty arrays [].\n\n\nCode\ndef clean_json_string(value):\n  \n    if pd.isna(value):\n        return np.nan\n    \n    if not isinstance(value, str):\n        return value\n    \n    value = value.strip()\n    \n    # Handle empty list or [None]\n    if value in ['[]', '[None]', '[\\n]', '', '[ ]']:\n        return np.nan\n    \n    # Check if it's a JSON-like array\n    if value.startswith('[') and value.endswith(']'):\n        # Remove brackets\n        cleaned = value[1:-1]\n        \n        # Remove \\n, extra spaces, quotes\n        cleaned = re.sub(r'\\\\n', '', cleaned)\n        cleaned = re.sub(r'\\n', '', cleaned)\n        cleaned = re.sub(r'\"', '', cleaned)\n        \n        # Split by comma and clean each item\n        items = [item.strip() for item in cleaned.split(',') if item.strip()]\n        \n        if not items:\n            return np.nan\n        \n        return ', '.join(items)\n    \n    return value.strip()\n\n\n# Columns to clean\njson_columns = [\n    'SKILLS_NAME', \n    'SOFTWARE_SKILLS_NAME',\n    'SOURCE_TYPES',\n    'REMOTE_TYPE_NAME',\n    'EMPLOYMENT_TYPE_NAME',\n    'MIN_EDULEVELS_NAME'\n]\n\n# Apply cleaning\nfor col in json_columns:\n    if col in df.columns:\n        print(f\"Cleaning: {col}\")\n        df[col] = df[col].apply(clean_json_string)\n\n\n\nCleaning: SKILLS_NAME\nCleaning: SOFTWARE_SKILLS_NAME\nCleaning: SOFTWARE_SKILLS_NAME\nCleaning: SOURCE_TYPES\nCleaning: SOURCE_TYPES\nCleaning: REMOTE_TYPE_NAME\nCleaning: EMPLOYMENT_TYPE_NAME\nCleaning: MIN_EDULEVELS_NAME\nCleaning: REMOTE_TYPE_NAME\nCleaning: EMPLOYMENT_TYPE_NAME\nCleaning: MIN_EDULEVELS_NAME\n\n\n\n4.1 Clean BODY Column\nRemove newlines, HTML artifacts, and junk descriptions that are scraped webpage content rather than actual job descriptions.\n\n\nCode\n# Clean BODY column\ndef clean_body(text):\n    if pd.isna(text):\n        return np.nan\n    \n    if not isinstance(text, str):\n        return text\n    \n    # Remove \\n and excessive whitespace\n    cleaned = re.sub(r'\\n+', ' ', text)\n    cleaned = re.sub(r'\\s+', ' ', cleaned)\n    cleaned = cleaned.strip()\n    \n    return cleaned if cleaned else np.nan\n\n# Junk patterns to filter out (scraped webpage artifacts, not real job descriptions)\njunk_patterns = [\n    r'^Find jobs\\s*Search\\s*Enter any',  # Scraped job board UI\n    r'^Search for jobs',\n    r'^Loading\\.\\.\\.',\n    r'^Please wait',\n    r'^Click here to',\n    r'^Apply now',\n]\n\n# Apply cleaning\nprint(f\"BODY column before cleaning: {df['BODY'].notna().sum()} non-null values\")\n\ndf['BODY'] = df['BODY'].apply(clean_body)\n\n# Remove junk descriptions\nfor pattern in junk_patterns:\n    mask = df['BODY'].str.contains(pattern, case=False, na=False, regex=True)\n    junk_count = mask.sum()\n    if junk_count &gt; 0:\n        print(f\"Removing {junk_count} rows matching junk pattern: {pattern[:40]}...\")\n        df.loc[mask, 'BODY'] = np.nan\n\n# Remove very short descriptions (likely incomplete)\nshort_mask = df['BODY'].str.len() &lt; 50\nshort_count = short_mask.sum()\nif short_count &gt; 0:\n    print(f\"Setting {short_count} very short descriptions (&lt;50 chars) to NaN\")\n    df.loc[short_mask, 'BODY'] = np.nan\n\nprint(f\"\\nBODY column after cleaning: {df['BODY'].notna().sum()} non-null values\")\nprint(f\"\\nSample cleaned BODY (first 200 chars):\")\nsample = df['BODY'].dropna().iloc[0]\nprint(sample[:200] + \"...\" if len(str(sample)) &gt; 200 else sample)\n\n\nBODY column before cleaning: 57220 non-null values\nRemoving 207 rows matching junk pattern: ^Find jobs\\s*Search\\s*Enter any...\nRemoving 4 rows matching junk pattern: ^Loading\\.\\.\\....\nRemoving 207 rows matching junk pattern: ^Find jobs\\s*Search\\s*Enter any...\nRemoving 4 rows matching junk pattern: ^Loading\\.\\.\\....\nSetting 29 very short descriptions (&lt;50 chars) to NaN\n\nBODY column after cleaning: 56980 non-null values\n\nSample cleaned BODY (first 200 chars):\n31-May-2024 Enterprise Analyst (II-III) Merchandising El Dorado Arkansas Job Posting GENERAL DESCRIPTION OF POSITION Performs business analysis using various techniques, e.g. statistical analysis, exp...\nSetting 29 very short descriptions (&lt;50 chars) to NaN\n\nBODY column after cleaning: 56980 non-null values\n\nSample cleaned BODY (first 200 chars):\n31-May-2024 Enterprise Analyst (II-III) Merchandising El Dorado Arkansas Job Posting GENERAL DESCRIPTION OF POSITION Performs business analysis using various techniques, e.g. statistical analysis, exp..."
  },
  {
    "objectID": "data_cleaning.html#excluding-unknown-values",
    "href": "data_cleaning.html#excluding-unknown-values",
    "title": "Data Cleaning & Preprocessing",
    "section": "5. Excluding Unknown Values",
    "text": "5. Excluding Unknown Values\n\n\nCode\ndf = df[~df['NAICS_2022_2_NAME'].str.contains('Unclassified', na=False)].copy()\ndf = df[~df['COMPANY_NAME'].str.contains('Unclassified', case=False, na=False)].copy()\ndf = df[~df['TITLE_NAME'].str.contains('Unclassified', case=False, na=False)].copy()"
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Data Cleaning & Preprocessing",
    "section": "6. Handling Missing Values",
    "text": "6. Handling Missing Values\n\n6.1 Salary\n\n\nCode\n\nprint(\"Missing salary values BEFORE imputation:\")\nprint(f\"  SALARY: {df['SALARY'].isna().sum()}\")\nprint(f\"  SALARY_FROM: {df['SALARY_FROM'].isna().sum()}\")\nprint(f\"  SALARY_TO: {df['SALARY_TO'].isna().sum()}\")\n\nsalary_cols = ['SALARY', 'SALARY_FROM', 'SALARY_TO']\n\nfor col in salary_cols:\n    if col in df.columns:\n        median_by_occupation = df.groupby('LOT_V6_OCCUPATION_NAME')[col].transform('median')\n        \n        df[col] = df[col].fillna(median_by_occupation)\n\n\n\nfor col in salary_cols:\n    if col in df.columns:\n        remaining_na = df[col].isna().sum()\n        if remaining_na &gt; 0:\n            overall_median = df[col].median()\n            df[col] = df[col].fillna(overall_median)\n            print(f\"\\nFilled {remaining_na} remaining {col} NaN with overall median: {overall_median}\")\n\nprint(\"\\nFinal missing salary values:\")\nprint(f\"  SALARY: {df['SALARY'].isna().sum()}\")\nprint(f\"  SALARY_FROM: {df['SALARY_FROM'].isna().sum()}\")\nprint(f\"  SALARY_TO: {df['SALARY_TO'].isna().sum()}\")\n\n\nMissing salary values BEFORE imputation:\n  SALARY: 26716\n  SALARY_FROM: 25621\n  SALARY_TO: 25621\n\nFinal missing salary values:\n  SALARY: 0\n  SALARY_FROM: 0\n  SALARY_TO: 0\n\n\n\n\n6.2 SKILLS\n\n\nCode\nbefore = len(df)\n\ndf = df.dropna(subset=['SKILLS_NAME'])\n\nafter = len(df)\nprint(f\"Removed {before - after} rows with no skills data\")\nprint(f\"Remaining rows: {after}\")\n\n\nRemoved 197 rows with no skills data\nRemaining rows: 47513\n\n\nWe are dropping these rows since we are going to be analyze the skills required for these jobs.\n\n\n6.3 REMOTE_TYPE_NAME AND SOFTWARE_SKILLS_NAME\n\n\nCode\ncategorical_fills = {\n    'REMOTE_TYPE_NAME': 'Not Specified',\n    'SOFTWARE_SKILLS_NAME': 'Not Listed'\n}\n\nfor col, fill_value in categorical_fills.items():\n    if col in df.columns:\n        df[col] = df[col].fillna(fill_value)\n\n\n\n\n6.4 MIN_YEARS_EXPERIENCE\n\n\nCode\n#  indicator for missing experience\ndf['EXPERIENCE_SPECIFIED'] = df['MIN_YEARS_EXPERIENCE'].notna().astype(int)\n\n\ndf['MIN_YEARS_EXPERIENCE'] = df['MIN_YEARS_EXPERIENCE'].fillna(0)\n\n\nWe are not dropping thes rows, instead we are using another column to specify whether the experience was given or not, this method helps us to preserve our data since there are a lot of data points where minimum experience is not specified."
  },
  {
    "objectID": "data_cleaning.html#table-order",
    "href": "data_cleaning.html#table-order",
    "title": "Data Cleaning & Preprocessing",
    "section": "7. Table Order",
    "text": "7. Table Order\n\n\nCode\ncolumn_order = [\n    'TITLE_NAME', 'COMPANY_NAME', 'BODY', 'POSTED', 'DURATION',\n    'SALARY', 'SALARY_FROM', 'SALARY_TO',\n    'STATE_NAME', 'CITY_NAME',\n    'REMOTE_TYPE_NAME', 'EMPLOYMENT_TYPE_NAME',\n    'MIN_YEARS_EXPERIENCE','EXPERIENCE_SPECIFIED', 'MIN_EDULEVELS_NAME',\n    'SKILLS_NAME', 'SOFTWARE_SKILLS_NAME',\n    'SOURCE_TYPES', 'LOT_V6_OCCUPATION_NAME', 'NAICS_2022_2_NAME'\n]\ndf = df[column_order]"
  },
  {
    "objectID": "data_cleaning.html#cleaned-dataset-summary",
    "href": "data_cleaning.html#cleaned-dataset-summary",
    "title": "Data Cleaning & Preprocessing",
    "section": "8. Cleaned Dataset summary",
    "text": "8. Cleaned Dataset summary\n\n\nCode\ndf.shape\n\n\n(47513, 20)\n\n\n\n\nCode\ndf.isna().sum()\n\n\nTITLE_NAME                  0\nCOMPANY_NAME                0\nBODY                      156\nPOSTED                      0\nDURATION                    0\nSALARY                      0\nSALARY_FROM                 0\nSALARY_TO                   0\nSTATE_NAME                  0\nCITY_NAME                   0\nREMOTE_TYPE_NAME            0\nEMPLOYMENT_TYPE_NAME        0\nMIN_YEARS_EXPERIENCE        0\nEXPERIENCE_SPECIFIED        0\nMIN_EDULEVELS_NAME          0\nSKILLS_NAME                 0\nSOFTWARE_SKILLS_NAME        0\nSOURCE_TYPES                0\nLOT_V6_OCCUPATION_NAME      0\nNAICS_2022_2_NAME           0\ndtype: int64\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nPOSTED\nDURATION\nSALARY\nSALARY_FROM\nSALARY_TO\nMIN_YEARS_EXPERIENCE\nEXPERIENCE_SPECIFIED\n\n\n\n\ncount\n47513\n47513.000000\n47513.000000\n47513.000000\n47513.000000\n47513.000000\n47513.000000\n\n\nmean\n2024-07-10 14:42:13.251952128\n35.637426\n111674.426936\n87664.599562\n132083.203544\n3.494580\n0.692400\n\n\nmin\n2024-05-01 00:00:00\n0.000000\n20583.000000\n10230.000000\n11148.000000\n0.000000\n0.000000\n\n\n25%\n2024-06-04 00:00:00\n14.000000\n96008.000000\n76960.000000\n109100.000000\n0.000000\n0.000000\n\n\n50%\n2024-07-09 00:00:00\n31.000000\n105000.000000\n81000.000000\n122000.000000\n3.000000\n1.000000\n\n\n75%\n2024-08-16 00:00:00\n60.000000\n125900.000000\n96000.000000\n156800.000000\n5.000000\n1.000000\n\n\nmax\n2024-09-30 00:00:00\n119.000000\n500000.000000\n800000.000000\n950000.000000\n15.000000\n1.000000\n\n\nstd\nNaN\n24.027774\n30031.592449\n25934.636196\n43204.011452\n3.464785\n0.461505\n\n\n\n\n\n\n\n\n\nCode\ndisplay(df.head())\n\ndf.reset_index(drop=True, inplace=True)\ndf.to_csv('data/lightcast_cleaned.csv', index=False)\n\n\n\n\n\n\n\n\n\nTITLE_NAME\nCOMPANY_NAME\nBODY\nPOSTED\nDURATION\nSALARY\nSALARY_FROM\nSALARY_TO\nSTATE_NAME\nCITY_NAME\nREMOTE_TYPE_NAME\nEMPLOYMENT_TYPE_NAME\nMIN_YEARS_EXPERIENCE\nEXPERIENCE_SPECIFIED\nMIN_EDULEVELS_NAME\nSKILLS_NAME\nSOFTWARE_SKILLS_NAME\nSOURCE_TYPES\nLOT_V6_OCCUPATION_NAME\nNAICS_2022_2_NAME\n\n\n\n\n0\nEnterprise Analysts\nMurphy USA\n31-May-2024 Enterprise Analyst (II-III) Mercha...\n2024-06-02\n6\n125900.0\n96000.0\n156800.0\nArkansas\nEl Dorado, AR\nNot Specified\nFull-time (&gt; 32 hours)\n2.0\n1\nBachelor's degree\nMerchandising, Mathematics, Presentations, Pre...\nSQL (Programming Language), Power BI\nCompany\nBusiness Intelligence Analyst\nRetail Trade\n\n\n1\nOracle Consultants\nSmx Corporation Limited\nOracle Consultant - Reports (3592) at SMX in A...\n2024-06-02\n60\n125900.0\n96000.0\n156800.0\nMaine\nAugusta, ME\nRemote\nFull-time (&gt; 32 hours)\n3.0\n1\nNo Education Listed\nProcurement, Financial Statements, Oracle Busi...\nOracle Business Intelligence (BI) / OBIA, Orac...\nJob Board\nBusiness Intelligence Analyst\nAdministrative and Support and Waste Managemen...\n\n\n2\nData Analysts\nSedgwick\nTaking care of people is at the heart of every...\n2024-06-02\n35\n96008.0\n76960.0\n109100.0\nTexas\nDallas, TX\nNot Specified\nFull-time (&gt; 32 hours)\n5.0\n1\nBachelor's degree\nManagement, Exception Reporting, Report Writin...\nMicrosoft Office\nJob Board\nData / Data Mining Analyst\nFinance and Insurance\n\n\n3\nManagement Analysts\nWells Fargo\nAbout this role: Wells Fargo is looking for a ...\n2024-06-02\n48\n96008.0\n76960.0\n109100.0\nArizona\nPhoenix, AZ\nNot Specified\nFull-time (&gt; 32 hours)\n3.0\n1\nNo Education Listed\nExit Strategies, Reliability, User Story, Mana...\nSAS (Software), Google Cloud Platform (GCP)\nJob Board\nData / Data Mining Analyst\nFinance and Insurance\n\n\n5\nLead Data Analysts\nLumen Technologies\nAbout Lumen Lumen connects the world. We are i...\n2024-06-02\n10\n110155.0\n94420.0\n125890.0\nArkansas\n[Unknown City], AR\nRemote\nFull-time (&gt; 32 hours)\n0.0\n0\nBachelor's degree\nPower BI, Presentations, Data Reporting, Qlik ...\nPower BI, Qlik Sense (Data Analytics Software)...\nJob Board\nData / Data Mining Analyst\nInformation"
  }
]